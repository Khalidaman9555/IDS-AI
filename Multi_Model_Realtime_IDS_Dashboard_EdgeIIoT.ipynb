{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15Qa4FVLvHQVDj3jRmSPFJdg5Ak2qiRhB",
      "authorship_tag": "ABX9TyPBOKn8LViF+NU7uT0q5d65",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khalidaman9555/IDS-AI/blob/main/Multi_Model_Realtime_IDS_Dashboard_EdgeIIoT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "52_tdX-Pw1o6",
        "outputId": "123d9de5-6756-4abd-d548-eb6e84629c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit server starting in background...\n",
            "\n",
            "📊 Access your dashboard at: https://8501-m-s-19dsq8pbxxhz3-b.europe-west4-1.prod.colab.dev\n",
            "If the link above doesn't work or shows an error, please check the Colab cell output for any Streamlit errors.\n",
            "Ensure your Google Drive is mounted and all file paths are correct.\n"
          ]
        }
      ],
      "source": [
        "# ... (Outer Colab setup code: ngrok, pip installs etc. remain the same) ...\n",
        "\n",
        "# 3. Define the Streamlit application content and write it to app.py\n",
        "import os\n",
        "app_content = \"\"\"\n",
        "import streamlit as st\n",
        "\n",
        "# MUST BE THE VERY FIRST STREAMLIT COMMAND\n",
        "st.set_page_config(layout=\"wide\", page_title=\"Enhanced IDS Dashboard\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "import altair as alt\n",
        "from collections import defaultdict, deque\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler # For LSTM, original scaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import joblib # For loading scikit-learn models\n",
        "import os # Added for os.path.exists\n",
        "\n",
        "# Conditional import for TensorFlow\n",
        "try:\n",
        "    from tensorflow.keras.models import load_model\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "    # This st.sidebar.warning will only be called when the UI tries to render the sidebar later\n",
        "    # However, for robustness, messages from initial loading could be collected and displayed\n",
        "    # after set_page_config and basic UI elements are set up.\n",
        "    # For now, this should be okay as set_page_config is first.\n",
        "\n",
        "# --- Configuration ---\n",
        "# Paths to your trained models and scaler\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/\" # Adjust if your Drive is mounted differently\n",
        "RESULTS_PATH = os.path.join(BASE_PATH, \"results\")\n",
        "DATA_PATH = os.path.join(BASE_PATH, \"datasets/ML-EdgeIIoT-dataset.csv\")\n",
        "\n",
        "# Scikit-learn models and common scaler\n",
        "SKLEARN_MODEL_FILES = {\n",
        "    \"RandomForest\": os.path.join(RESULTS_PATH, \"randomforest_model.joblib\"),\n",
        "    \"DecisionTree\": os.path.join(RESULTS_PATH, \"decisiontree_model.joblib\"),\n",
        "    \"LogisticRegression\": os.path.join(RESULTS_PATH, \"logisticregression_model.joblib\"),\n",
        "    \"XGBoost\": os.path.join(RESULTS_PATH, \"xgboost_model.joblib\"),\n",
        "    \"SVM\": os.path.join(RESULTS_PATH, \"svm_model.joblib\")\n",
        "}\n",
        "COMMON_SCALER_PATH = os.path.join(RESULTS_PATH, \"common_scaler.joblib\")\n",
        "\n",
        "# LSTM Model\n",
        "LSTM_MODEL_PATH = os.path.join(RESULTS_PATH, \"lstm_model.h5\")\n",
        "\n",
        "# Features: CRITICAL - This list MUST match the features your models were trained on.\n",
        "FEATURE_COLUMNS = [\n",
        "    'arp.hw.size', 'http.content_length', 'http.response', 'http.tls_port',\n",
        "    'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst',\n",
        "    'tcp.connection.syn', 'tcp.connection.synack', 'tcp.dstport',\n",
        "    'tcp.flags.ack', 'tcp.len', 'udp.stream', 'udp.time_delta', 'dns.qry.qu',\n",
        "    'dns.qry.type', 'dns.retransmission', 'dns.retransmit_request',\n",
        "    'dns.retransmit_request_in', 'mqtt.conflag.cleansess', 'mqtt.hdrflags',\n",
        "    'mqtt.len', 'mqtt.msg_decoded_as', 'mbtcp.len', 'mbtcp.trans_id',\n",
        "    'mbtcp.unit_id'\n",
        "]\n",
        "LSTM_SEQUENCE_LENGTH = 1\n",
        "\n",
        "# --- Session State Initialization ---\n",
        "if 'data' not in st.session_state:\n",
        "    st.session_state.data = {\n",
        "        'events': deque(maxlen=500),\n",
        "        'predictions_log': deque(maxlen=500),\n",
        "        'attack_stats': defaultdict(int),\n",
        "        'model_metrics': {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0},\n",
        "        'common_scaler_sklearn': None,\n",
        "        'lstm_scaler': MinMaxScaler(),\n",
        "        'data_iterator': None,\n",
        "        'selected_model_name': None,\n",
        "        'loaded_models': {}\n",
        "    }\n",
        "if 'page_loaded' not in st.session_state:\n",
        "    st.session_state.page_loaded = False\n",
        "\n",
        "# --- Model and Scaler Loading ---\n",
        "@st.cache_resource # Use cache_resource for model objects\n",
        "def load_sklearn_models_and_scaler():\n",
        "    models = {}\n",
        "    # Temporary lists to hold messages, display them after set_page_config\n",
        "    load_messages = []\n",
        "    for name, path in SKLEARN_MODEL_FILES.items():\n",
        "        if os.path.exists(path):\n",
        "            try:\n",
        "                models[name] = joblib.load(path)\n",
        "            except Exception as e:\n",
        "                load_messages.append(f\"Error: Failed to load {name} from {path}: {e}\")\n",
        "        else:\n",
        "            load_messages.append(f\"Warning: Scikit-learn model not found: {path}\")\n",
        "\n",
        "    scaler = None\n",
        "    if os.path.exists(COMMON_SCALER_PATH):\n",
        "        try:\n",
        "            scaler = joblib.load(COMMON_SCALER_PATH)\n",
        "            load_messages.append(\"Success: Common Scaler (for scikit-learn) loaded.\")\n",
        "        except Exception as e:\n",
        "            load_messages.append(f\"Error: Failed to load Common Scaler from {COMMON_SCALER_PATH}: {e}\")\n",
        "    else:\n",
        "        load_messages.append(f\"Warning: Common Scaler not found: {COMMON_SCALER_PATH}\")\n",
        "    return models, scaler, load_messages\n",
        "\n",
        "@st.cache_resource\n",
        "def load_lstm_model_keras():\n",
        "    load_messages = []\n",
        "    if not TENSORFLOW_AVAILABLE:\n",
        "        load_messages.append(\"Warning: TensorFlow/Keras not found. LSTM model will be unavailable.\")\n",
        "        return None, load_messages\n",
        "\n",
        "    model = None\n",
        "    if os.path.exists(LSTM_MODEL_PATH):\n",
        "        try:\n",
        "            model = load_model(LSTM_MODEL_PATH)\n",
        "            load_messages.append(\"Success: LSTM Model loaded.\")\n",
        "        except Exception as e:\n",
        "            load_messages.append(f\"Error: LSTM Model loading failed from {LSTM_MODEL_PATH}: {e}\")\n",
        "    else:\n",
        "        load_messages.append(f\"Warning: LSTM Model not found: {LSTM_MODEL_PATH}\")\n",
        "    return model, load_messages\n",
        "\n",
        "# Initial loading messages will be displayed in the sidebar later\n",
        "_initial_load_messages = []\n",
        "\n",
        "if not st.session_state.data['loaded_models']: # Load only once\n",
        "    sklearn_models_loaded, common_scaler_loaded, skl_msgs = load_sklearn_models_and_scaler()\n",
        "    _initial_load_messages.extend(skl_msgs)\n",
        "    if common_scaler_loaded:\n",
        "        st.session_state.data['common_scaler_sklearn'] = common_scaler_loaded\n",
        "    for name, model in sklearn_models_loaded.items():\n",
        "        st.session_state.data['loaded_models'][name] = model\n",
        "\n",
        "    if TENSORFLOW_AVAILABLE:\n",
        "        lstm_model_loaded, lstm_msgs = load_lstm_model_keras()\n",
        "        _initial_load_messages.extend(lstm_msgs)\n",
        "        if lstm_model_loaded:\n",
        "            st.session_state.data['loaded_models']['LSTM'] = lstm_model_loaded\n",
        "\n",
        "# --- Data Pipeline ---\n",
        "class EdgeIIoTDataIterator:\n",
        "    def __init__(self, file_path, feature_columns):\n",
        "        self.feature_columns = feature_columns\n",
        "        self.df_processed = pd.DataFrame()\n",
        "        self.current_idx = 0\n",
        "        self.load_messages = [] # Messages specific to data iterator\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            self.load_messages.append(f\"Error: Dataset CSV not found at {file_path}.\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.df = pd.read_csv(file_path, low_memory=False)\n",
        "            self.load_messages.append(f\"Info: Raw CSV loaded: {len(self.df)} rows, {len(self.df.columns)} columns.\")\n",
        "\n",
        "            for col in self.feature_columns:\n",
        "                if col not in self.df.columns:\n",
        "                    self.load_messages.append(f\"Warning: Feature '{col}' not in CSV! Filling with 0.\")\n",
        "                    self.df[col] = 0\n",
        "                else:\n",
        "                    self.df[col] = pd.to_numeric(self.df[col], errors='coerce').fillna(0)\n",
        "\n",
        "            if 'Attack_label' in self.df.columns:\n",
        "                self.df['is_attack'] = self.df['Attack_label'].apply(lambda x: 0 if str(x) == '0' else 1)\n",
        "            else:\n",
        "                self.load_messages.append(\"Warning: 'Attack_label' not in CSV. Assuming normal.\")\n",
        "                self.df['is_attack'] = 0\n",
        "\n",
        "            self.df_processed = self.df[self.feature_columns + ['is_attack']].copy()\n",
        "            self.load_messages.append(f\"Success: Dataset processed. Features: {len(self.feature_columns)}.\")\n",
        "        except Exception as e:\n",
        "            self.load_messages.append(f\"Error: Error loading/processing dataset: {str(e)}\")\n",
        "\n",
        "    def get_load_messages(self):\n",
        "        return self.load_messages\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.df_processed.empty or self.current_idx >= len(self.df_processed):\n",
        "            self.current_idx = 0\n",
        "            if self.df_processed.empty:\n",
        "                # This message will be handled by the main loop checking iterator status\n",
        "                raise StopIteration\n",
        "\n",
        "        row = self.df_processed.iloc[self.current_idx]\n",
        "        self.current_idx += 1\n",
        "        sample_features = {col: float(row[col]) for col in self.feature_columns}\n",
        "        return {\n",
        "            'timestamp': datetime.now().strftime('%H:%M:%S.%f')[:-3],\n",
        "            'is_attack': int(row['is_attack']),\n",
        "            **sample_features\n",
        "        }\n",
        "\n",
        "_data_iterator_messages = []\n",
        "if st.session_state.data.get('data_iterator') is None and DATA_PATH:\n",
        "    iterator_instance = EdgeIIoTDataIterator(DATA_PATH, FEATURE_COLUMNS)\n",
        "    st.session_state.data['data_iterator'] = iterator_instance\n",
        "    _data_iterator_messages = iterator_instance.get_load_messages()\n",
        "\n",
        "# --- Preprocessing and Prediction --- (Definition remains the same)\n",
        "def preprocess_and_predict(sample_features_dict, model_name, feature_columns):\n",
        "    features_2d = np.array([[sample_features_dict[col] for col in feature_columns]])\n",
        "    current_model = st.session_state.data['loaded_models'].get(model_name)\n",
        "\n",
        "    if current_model is None: return 0, 0.0, f\"Model '{model_name}' not loaded\"\n",
        "\n",
        "    try:\n",
        "        if model_name == \"LSTM\":\n",
        "            scaler_lstm = st.session_state.data['lstm_scaler']\n",
        "            if not hasattr(scaler_lstm, 'scale_'):\n",
        "                 scaler_lstm.fit(features_2d)\n",
        "            model_input = scaler_lstm.transform(features_2d)\n",
        "            model_input = model_input.reshape(1, LSTM_SEQUENCE_LENGTH, len(feature_columns))\n",
        "            status_msg = \"LSTM Preprocessed\"\n",
        "            prediction_prob = current_model.predict(model_input, verbose=0)[0][0]\n",
        "            predicted_class = int(prediction_prob > 0.5)\n",
        "        else: # Scikit-learn\n",
        "            scaler_sklearn = st.session_state.data.get('common_scaler_sklearn')\n",
        "            if scaler_sklearn is None: return 0, 0.0, \"Common scaler not loaded\"\n",
        "            model_input = scaler_sklearn.transform(features_2d)\n",
        "            status_msg = \"Scikit-learn Preprocessed\"\n",
        "            predicted_class = current_model.predict(model_input)[0]\n",
        "            prediction_prob = current_model.predict_proba(model_input)[0][1] if hasattr(current_model, \"predict_proba\") else (1.0 if predicted_class == 1 else 0.0)\n",
        "\n",
        "        return int(predicted_class), float(prediction_prob), status_msg\n",
        "    except Exception as e:\n",
        "        return 0, 0.0, f\"Prediction error ({model_name}): {str(e)}\"\n",
        "\n",
        "# --- Dashboard UI ---\n",
        "# Title is now after set_page_config\n",
        "st.title(\"🛡️ Edge-IIoT Intrusion Detection System\")\n",
        "\n",
        "# Display initial loading messages in the sidebar\n",
        "st.sidebar.header(\"🔄 Loading Status\")\n",
        "for msg in _initial_load_messages:\n",
        "    if \"Error:\" in msg: st.sidebar.error(msg)\n",
        "    elif \"Warning:\" in msg: st.sidebar.warning(msg)\n",
        "    elif \"Success:\" in msg: st.sidebar.success(msg)\n",
        "    else: st.sidebar.info(msg)\n",
        "\n",
        "for msg in _data_iterator_messages: # Display data iterator messages\n",
        "    if \"Error:\" in msg: st.sidebar.error(msg)\n",
        "    elif \"Warning:\" in msg: st.sidebar.warning(msg)\n",
        "    elif \"Success:\" in msg: st.sidebar.success(msg)\n",
        "    else: st.sidebar.info(msg)\n",
        "\n",
        "\n",
        "st.sidebar.header(\"⚙️ Controls & Settings\")\n",
        "available_model_names = [name for name, model in st.session_state.data['loaded_models'].items() if model is not None]\n",
        "if not available_model_names:\n",
        "    st.sidebar.error(\"No models loaded! Dashboard cannot operate.\")\n",
        "    st.stop()\n",
        "\n",
        "active_model_name = st.sidebar.selectbox(\"🧠 Select Model:\", available_model_names, index=0)\n",
        "st.sidebar.info(f\"Active Model: **{active_model_name}**\")\n",
        "st.session_state.data['selected_model_name'] = active_model_name\n",
        "\n",
        "update_interval_ms = st.sidebar.slider(\"⏱️ Update Interval (ms)\", 100, 2000, 750)\n",
        "history_length = st.sidebar.slider(\"📊 History Length (samples)\", 20, 200, 50)\n",
        "\n",
        "if st.session_state.data['events'].maxlen != history_length:\n",
        "    st.session_state.data['events'] = deque(st.session_state.data['events'], maxlen=history_length)\n",
        "    st.session_state.data['predictions_log'] = deque(st.session_state.data['predictions_log'], maxlen=history_length)\n",
        "\n",
        "col1, col2 = st.columns([2, 1])\n",
        "with col1:\n",
        "    st.subheader(\"🚦 Real-time Detections\")\n",
        "    event_plot_placeholder = st.empty()\n",
        "    st.subheader(\"📈 Prediction Confidence\")\n",
        "    confidence_plot_placeholder = st.empty()\n",
        "with col2:\n",
        "    st.subheader(\"📊 Session Statistics\")\n",
        "    stats_placeholder = st.empty()\n",
        "    st.subheader(\"⚠️ Recent Alerts (Predicted Attacks)\")\n",
        "    alerts_placeholder = st.empty()\n",
        "    selected_model_obj = st.session_state.data['loaded_models'].get(active_model_name)\n",
        "    if active_model_name not in [\"LSTM\", \"SVM\"] and selected_model_obj and (hasattr(selected_model_obj, 'feature_importances_') or hasattr(selected_model_obj, 'coef_')):\n",
        "        st.subheader(f\"Feature Importance ({active_model_name})\")\n",
        "        fi_placeholder = st.empty()\n",
        "\n",
        "data_iterator = st.session_state.data.get('data_iterator')\n",
        "if data_iterator is None or (hasattr(data_iterator, 'df_processed') and data_iterator.df_processed.empty):\n",
        "    # This message is now more prominent if data loading failed\n",
        "    st.error(\"Data not available or failed to load. Dashboard cannot run. Check sidebar for loading status messages.\")\n",
        "    st.stop()\n",
        "\n",
        "if not st.session_state.page_loaded: st.session_state.page_loaded = True\n",
        "\n",
        "# Main display loop (content remains largely the same as before)\n",
        "while True:\n",
        "    try: sample = next(data_iterator)\n",
        "    except StopIteration: st.warning(\"Data source depleted.\"); break\n",
        "    except Exception as e: st.error(f\"Error fetching data: {e}\"); time.sleep(2); continue\n",
        "\n",
        "    sample_features = {k: sample[k] for k in FEATURE_COLUMNS}\n",
        "    # Use the globally selected active_model_name\n",
        "    predicted_class, prediction_prob, pred_status = preprocess_and_predict(sample_features, st.session_state.data['selected_model_name'], FEATURE_COLUMNS)\n",
        "\n",
        "\n",
        "    if \"error\" in pred_status.lower() or \"not loaded\" in pred_status.lower():\n",
        "        # Display persistent error in sidebar if prediction keeps failing for the selected model\n",
        "        # This check helps to avoid flooding the main UI if a model is consistently failing.\n",
        "        # More sophisticated error handling might be needed for continuous errors.\n",
        "        pass # Error already shown via initial load messages or will be caught if model is None\n",
        "\n",
        "    st.session_state.data['events'].append(sample)\n",
        "    st.session_state.data['predictions_log'].append({\n",
        "        'timestamp': sample['timestamp'], 'actual': sample['is_attack'],\n",
        "        'predicted': predicted_class, 'probability': prediction_prob, 'model': st.session_state.data['selected_model_name']\n",
        "    })\n",
        "\n",
        "    actual, pred = sample['is_attack'], predicted_class\n",
        "    metrics = st.session_state.data['model_metrics']\n",
        "    if actual == 1 and pred == 1: metrics['tp'] += 1\n",
        "    elif actual == 0 and pred == 1: metrics['fp'] += 1\n",
        "    elif actual == 0 and pred == 0: metrics['tn'] += 1\n",
        "    elif actual == 1 and pred == 0: metrics['fn'] += 1\n",
        "\n",
        "    events_df = pd.DataFrame(list(st.session_state.data['events']))\n",
        "    predictions_log_df = pd.DataFrame(list(st.session_state.data['predictions_log']))\n",
        "\n",
        "    if not predictions_log_df.empty:\n",
        "        chart_data = predictions_log_df.copy()\n",
        "        try:\n",
        "            chart_data['time_obj'] = pd.to_datetime(chart_data['timestamp'], format='%H:%M:%S.%f', errors='coerce')\n",
        "            chart_data.dropna(subset=['time_obj'], inplace=True)\n",
        "        except Exception: pass\n",
        "\n",
        "        if 'time_obj' in chart_data.columns and not chart_data.empty: # Check if chart_data is not empty after potential dropna\n",
        "            with event_plot_placeholder.container():\n",
        "                base_chart = alt.Chart(chart_data).encode(x='time_obj:T')\n",
        "                line_prob = base_chart.mark_line(opacity=0.8).encode(\n",
        "                    y=alt.Y('probability:Q', title='Attack Probability', scale=alt.Scale(domain=[0, 1])),\n",
        "                    tooltip=['timestamp:N', 'probability:Q', 'actual:N', 'predicted:N']\n",
        "                ).properties(title=\"Attack Prediction Probability\")\n",
        "                actual_attacks = base_chart.transform_filter(alt.datum.actual == 1).mark_circle(size=100, color='red', opacity=0.7).encode(y='probability:Q')\n",
        "                predicted_markers = base_chart.transform_filter(alt.datum.predicted == 1).mark_point(shape='diamond', size=80, color='orange', opacity=0.7, filled=True).encode(y='probability:Q')\n",
        "                st.altair_chart(line_prob + actual_attacks + predicted_markers, use_container_width=True)\n",
        "\n",
        "            with confidence_plot_placeholder.container():\n",
        "                conf_chart = alt.Chart(chart_data).mark_bar().encode(\n",
        "                    x='time_obj:T',\n",
        "                    y=alt.Y('probability:Q', title='Confidence', scale=alt.Scale(domain=(0,1))),\n",
        "                    color=alt.condition(alt.datum.probability > 0.5, alt.value('#e45756'), alt.value('#54a24b'))\n",
        "                ).properties(title=\"Prediction Confidence\", height=200)\n",
        "                st.altair_chart(conf_chart, use_container_width=True)\n",
        "        else:\n",
        "            with event_plot_placeholder.container():\n",
        "                if predictions_log_df.empty: st.caption(\"Waiting for data to plot...\")\n",
        "                else: st.warning(\"Timestamp conversion failed or no valid data for plotting main chart.\")\n",
        "\n",
        "\n",
        "    with stats_placeholder.container():\n",
        "        tp, fp, tn, fn = metrics['tp'], metrics['fp'], metrics['tn'], metrics['fn']\n",
        "        st.markdown(f\"**Confusion Matrix (Session Totals):**\")\n",
        "        cm_df = pd.DataFrame([[tn, fp], [fn, tp]], columns=['Pred Normal', 'Pred Attack'], index=['Actual Normal', 'Actual Attack'])\n",
        "        st.table(cm_df)\n",
        "        total = tp + tn + fp + fn\n",
        "        accuracy = (tp + tn) / total if total > 0 else 0\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2*(precision*recall)/(precision+recall) if (precision+recall) > 0 else 0\n",
        "        st.metric(\"Accuracy\", f\"{accuracy:.2%}\")\n",
        "        m_col1, m_col2, m_col3 = st.columns(3)\n",
        "        m_col1.metric(\"Precision\", f\"{precision:.2%}\")\n",
        "        m_col2.metric(\"Recall\", f\"{recall:.2%}\")\n",
        "        m_col3.metric(\"F1-Score\", f\"{f1:.2%}\")\n",
        "\n",
        "    with alerts_placeholder.container():\n",
        "        if not predictions_log_df.empty:\n",
        "            recent_alerts = predictions_log_df[predictions_log_df['predicted'] == 1].tail(5)\n",
        "            st.dataframe(recent_alerts[['timestamp', 'probability', 'actual']], hide_index=True, use_container_width=True)\n",
        "        else: st.info(\"No attacks detected recently.\")\n",
        "\n",
        "    current_active_model_name = st.session_state.data['selected_model_name'] # Use the most up-to-date selected model\n",
        "    selected_model_obj_fi = st.session_state.data['loaded_models'].get(current_active_model_name) # Re-fetch for feature importance part\n",
        "\n",
        "    if current_active_model_name not in [\"LSTM\", \"SVM\"] and selected_model_obj_fi and \\\n",
        "       (hasattr(selected_model_obj_fi, 'feature_importances_') or hasattr(selected_model_obj_fi, 'coef_')) and \\\n",
        "       ('fi_placeholder' in locals() or 'fi_placeholder' in globals()):\n",
        "        importances = None\n",
        "        if hasattr(selected_model_obj_fi, 'feature_importances_'): importances = selected_model_obj_fi.feature_importances_\n",
        "        elif hasattr(selected_model_obj_fi, 'coef_'): importances = selected_model_obj_fi.coef_[0]\n",
        "\n",
        "        if importances is not None and 'fi_placeholder' in locals():\n",
        "            with fi_placeholder.container():\n",
        "                fi_df = pd.DataFrame({'feature': FEATURE_COLUMNS,\n",
        "                                      'importance': np.abs(importances) if importances is not None else [0]*len(FEATURE_COLUMNS)}\n",
        "                                    ).sort_values('importance', ascending=False).head(10)\n",
        "                if not fi_df.empty:\n",
        "                    fi_chart = alt.Chart(fi_df).mark_bar().encode(\n",
        "                        x='importance:Q',\n",
        "                        y=alt.Y('feature:N', sort='-x')\n",
        "                    ).properties(title=f\"Top 10 Features ({current_active_model_name})\", height=250)\n",
        "                    st.altair_chart(fi_chart, use_container_width=True)\n",
        "                else:\n",
        "                    st.caption(\"Could not generate feature importances.\")\n",
        "        elif 'fi_placeholder' in locals():\n",
        "             with fi_placeholder.container(): st.caption(\"Feature importance not available for this model type.\")\n",
        "\n",
        "\n",
        "    time.sleep(update_interval_ms / 1000)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ... (Rest of the Colab script: with open('app.py'...) and running Streamlit server code remains the same)\n",
        "# Write the app content to app.py\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(app_content)\n",
        "\n",
        "# 4. Run Streamlit\n",
        "import threading\n",
        "import time\n",
        "import os # Ensure os is imported here as well\n",
        "\n",
        "def run_streamlit_server():\n",
        "    # Kill existing streamlit processes to avoid port conflicts\n",
        "    os.system('pkill -f streamlit')\n",
        "    time.sleep(2) # Give it a moment to kill\n",
        "    # Command to run Streamlit, with specific flags for Colab compatibility\n",
        "    os.system(f'streamlit run app.py --server.port 8501 --server.headless true --server.enableCORS false --server.enableXsrfProtection false')\n",
        "\n",
        "# Start Streamlit in a background thread\n",
        "thread = threading.Thread(target=run_streamlit_server, daemon=True)\n",
        "thread.start()\n",
        "print(\"Streamlit server starting in background...\")\n",
        "time.sleep(5)  # Wait for Streamlit to initialize\n",
        "\n",
        "# 5. Provide access link using Colab's proxy\n",
        "from google.colab.output import eval_js\n",
        "print(f\"\\n📊 Access your dashboard at: {eval_js('google.colab.kernel.proxyPort(8501)')}\")\n",
        "print(\"If the link above doesn't work or shows an error, please check the Colab cell output for any Streamlit errors.\")\n",
        "print(\"Ensure your Google Drive is mounted and all file paths are correct.\")"
      ]
    }
  ]
}