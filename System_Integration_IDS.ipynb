{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1R9N4SKojJzF9EBwg2P9jAzxr-AzyMsQh",
      "authorship_tag": "ABX9TyNpWO7wNC6V+iT40dRu+0xW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khalidaman9555/IDS-AI/blob/main/System_Integration_IDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQedQ1QsjjFv",
        "outputId": "31fb2512-f581-45f1-a777-7b574dcbaab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔔 Using default example BASE_DIR. Make sure this is the path you want in your Google Drive.\n",
            "   If not, please modify the BASE_DIR variable in this cell before proceeding.\n",
            "\n",
            "Project base directory: /content/drive/MyDrive/IDS_AI_Project\n",
            "Data directory: /content/drive/MyDrive/IDS_AI_Project/data\n",
            "Models directory: /content/drive/MyDrive/IDS_AI_Project/models\n",
            "Logs directory: /content/drive/MyDrive/IDS_AI_Project/logs\n",
            "\n",
            "Loaded SENSOR_CONFIG: 7 sensors configured.\n",
            "Loaded ATTACK_TYPES: 14 types defined (including 'normal').\n",
            "\n",
            "✅ Section 1 (Setup and Configuration) is ready to run.\n"
          ]
        }
      ],
      "source": [
        "# @title 🧭 Set up project directories and imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle # We might need to change this later for model loading\n",
        "import threading\n",
        "import argparse # For command-line argument parsing, usually not run directly in Colab cells like this\n",
        "from datetime import datetime\n",
        "\n",
        "# --- ACTION REQUIRED: Mount your Google Drive if not already mounted ---\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# --- ACTION REQUIRED: Set your project's base directory path ---\n",
        "# Replace the example path below with your desired Google Drive path.\n",
        "# If you don't set this, a temporary directory in /content/ will be used,\n",
        "# and data will NOT persist after your Colab session ends.\n",
        "BASE_DIR = \"/content/drive/MyDrive/IDS_AI_Project\" # <<<< CHANGE THIS TO YOUR PREFERRED GDRIVE PATH\n",
        "\n",
        "if BASE_DIR == \"/content/drive/MyDrive/IDS_AI_Project\":\n",
        "    print(\"🔔 Using default example BASE_DIR. Make sure this is the path you want in your Google Drive.\")\n",
        "    print(\"   If not, please modify the BASE_DIR variable in this cell before proceeding.\")\n",
        "elif not BASE_DIR or BASE_DIR.strip() == \"\":\n",
        "    print(\"🛑 BASE_DIR is not set. Please provide a valid Google Drive path.\")\n",
        "    print(\"   Defaulting to a temporary path: /content/ids_ai_project_temp\")\n",
        "    print(\"   ⚠️ Data in this temporary path will NOT persist across Colab sessions.\")\n",
        "    BASE_DIR = \"/content/ids_ai_project_temp\"\n",
        "else:\n",
        "    print(f\"✅ BASE_DIR set to: {BASE_DIR}\")\n",
        "\n",
        "\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
        "MODEL_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "LOG_DIR = os.path.join(BASE_DIR, \"logs\")\n",
        "\n",
        "# Create directories if they don't exist\n",
        "for directory in [BASE_DIR, DATA_DIR, MODEL_DIR, LOG_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(f\"\\nProject base directory: {BASE_DIR}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Models directory: {MODEL_DIR}\")\n",
        "print(f\"Logs directory: {LOG_DIR}\")\n",
        "\n",
        "# Sensor configuration (derived from your Sensors_and_attacks.ipynb)\n",
        "SENSOR_CONFIG = {\n",
        "    \"temperature_1\": {\"normal_range\": (15, 30), \"attack_value\": 100, \"unit\": \"°C\"},\n",
        "    \"humidity_1\": {\"normal_range\": (30, 60), \"attack_value\": 99, \"unit\": \"%\"},\n",
        "    \"pressure_1\": {\"normal_range\": (1000, 1020), \"attack_value\": 500, \"unit\": \"hPa\"},\n",
        "    \"vibration_1\": {\"normal_range\": (0, 0.5), \"attack_value\": 5.0, \"unit\": \"g\"},\n",
        "    \"plc_register_A\": {\"normal_range\": (0, 100), \"attack_value\": 999, \"unit\": \"value\"},\n",
        "    \"flow_rate_1\": {\"normal_range\": (50, 150), \"attack_value\": 0, \"unit\": \"L/min\"},\n",
        "    \"motor_current_1\": {\"normal_range\": (1.0, 5.0), \"attack_value\": 20.0, \"unit\": \"A\"}\n",
        "}\n",
        "print(f\"\\nLoaded SENSOR_CONFIG: {len(SENSOR_CONFIG)} sensors configured.\")\n",
        "\n",
        "# Attack types (derived from your Sensors_and_attacks.ipynb and script)\n",
        "ATTACK_TYPES = [\n",
        "    \"normal\", # Represents normal operation\n",
        "    \"tcp_syn_flood\",\n",
        "    \"port_scan\",\n",
        "    \"sql_injection\",\n",
        "    \"udp_flood\",\n",
        "    \"arp_spoofing\",\n",
        "    \"data_manipulation\", # Affects sensor readings directly\n",
        "    \"sensor_outage\",     # Affects sensor availability\n",
        "    \"command_injection\",\n",
        "    \"brute_force\",\n",
        "    \"mitm\",              # Man-in-the-middle\n",
        "    \"replay_attack\",\n",
        "    \"backdoor\",\n",
        "    \"ransomware\"\n",
        "]\n",
        "print(f\"Loaded ATTACK_TYPES: {len(ATTACK_TYPES)} types defined (including 'normal').\")\n",
        "\n",
        "print(\"\\n✅ Section 1 (Setup and Configuration) is ready to run.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🌡️ Section 2: Sensor Class Definition\n",
        "class Sensor:\n",
        "    \"\"\"Class representing an IoT sensor with normal and attack behaviors.\"\"\"\n",
        "\n",
        "    def __init__(self, sensor_id, sensor_type, normal_range, unit):\n",
        "        self.sensor_id = sensor_id\n",
        "        self.sensor_type = sensor_type\n",
        "        self.normal_range = normal_range\n",
        "        self.unit = unit\n",
        "        # Initialize with a normal value\n",
        "        self.current_value = self.generate_normal_value()\n",
        "        # print(f\"Sensor initialized: {self.sensor_id} (Type: {self.sensor_type}, Range: {self.normal_range}, Unit: {self.unit})\") # Optional: for verbose init\n",
        "\n",
        "    def generate_normal_value(self):\n",
        "        \"\"\"Generate a normal sensor reading within the expected range.\"\"\"\n",
        "        return round(random.uniform(self.normal_range[0], self.normal_range[1]), 2)\n",
        "\n",
        "    def read_value(self, under_attack=False, attack_type=None, attack_intensity=1):\n",
        "        \"\"\"\n",
        "        Generates a sensor reading.\n",
        "        If under_attack, the value might be anomalous based on attack_type.\n",
        "        attack_intensity can be used to scale the anomaly (e.g., 1 to 5).\n",
        "        \"\"\"\n",
        "        timestamp = time.time()\n",
        "        current_status = \"normal\"\n",
        "        generated_value = self.current_value # Start with current value\n",
        "\n",
        "        if under_attack:\n",
        "            current_status = \"anomalous\"\n",
        "            # Use specific attack_value from SENSOR_CONFIG if defined for this sensor\n",
        "            if self.sensor_id in SENSOR_CONFIG and 'attack_value' in SENSOR_CONFIG[self.sensor_id]:\n",
        "                base_attack_val = SENSOR_CONFIG[self.sensor_id]['attack_value']\n",
        "                # Add some minor variation to the defined attack value based on intensity\n",
        "                variation_range = (self.normal_range[1] - self.normal_range[0]) * 0.05 * attack_intensity # Using normal_range to scale variation\n",
        "                variation = random.uniform(-variation_range, variation_range) if variation_range > 0 else 0\n",
        "                generated_value = round(base_attack_val + variation, 2)\n",
        "\n",
        "            elif attack_type == \"data_manipulation\":\n",
        "                # General data manipulation if no specific attack_value is set in SENSOR_CONFIG\n",
        "                # Make the anomaly more significant based on intensity\n",
        "                # Anomaly could be a multiple of the normal range bounds, or a fixed large deviation\n",
        "                anomaly_magnitude_factor = 0.5 + (attack_intensity / 5.0) # Factor from 0.7 to 1.5\n",
        "                if random.random() < 0.5 : # deviate from lower bound\n",
        "                    generated_value = round(self.normal_range[0] - ( (self.normal_range[1]-self.normal_range[0]) * anomaly_magnitude_factor * random.uniform(0.5,1.0)),2)\n",
        "                else: # deviate from upper bound\n",
        "                    generated_value = round(self.normal_range[1] + ( (self.normal_range[1]-self.normal_range[0]) * anomaly_magnitude_factor * random.uniform(0.5,1.0)),2)\n",
        "\n",
        "\n",
        "            elif attack_type == \"sensor_outage\":\n",
        "                return {\n",
        "                    \"timestamp\": timestamp,\n",
        "                    \"sensor_id\": self.sensor_id,\n",
        "                    \"value\": None, # Or use np.nan if pandas/numpy is heavily used later\n",
        "                    \"unit\": self.unit,\n",
        "                    \"status\": \"outage\",\n",
        "                    \"event_type\": \"sensor_reading\"\n",
        "                }\n",
        "            else:\n",
        "                # For other generic attacks, simulate some deviation from normal, scaled by intensity\n",
        "                # This makes the sensor reading less predictable but still anomalous\n",
        "                noise_magnitude = (self.normal_range[1] - self.normal_range[0]) * 0.2 * attack_intensity # 20% of range * intensity\n",
        "                noise = random.uniform(-noise_magnitude, noise_magnitude) if noise_magnitude > 0 else 0\n",
        "                # Deviate from the center of the normal range or an extreme\n",
        "                base_for_noise = self.normal_range[0] if random.random() < 0.5 else self.normal_range[1]\n",
        "                generated_value = round(base_for_noise + noise, 2)\n",
        "        else:\n",
        "            # Normal operation: simulate slight variations or a new normal value\n",
        "            if random.random() < 0.7: # 70% chance to stay close to the previous value\n",
        "                variation = (self.normal_range[1] - self.normal_range[0]) * 0.05 # Max 5% variation of the range\n",
        "                noise = random.uniform(-variation, variation) if variation > 0 else 0\n",
        "                generated_value = round(self.current_value + noise, 2)\n",
        "                # Keep within normal range during normal operation\n",
        "                generated_value = max(self.normal_range[0], min(self.normal_range[1], generated_value))\n",
        "            else: # 30% chance to generate a completely new normal value\n",
        "                generated_value = self.generate_normal_value()\n",
        "\n",
        "        self.current_value = generated_value # Update sensor's last known value\n",
        "\n",
        "        return {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"sensor_id\": self.sensor_id,\n",
        "            \"value\": self.current_value,\n",
        "            \"unit\": self.unit,\n",
        "            \"status\": current_status,\n",
        "            \"event_type\": \"sensor_reading\"\n",
        "        }\n",
        "\n",
        "# --- Test the Sensor class (optional, for this cell) ---\n",
        "print(\"Sensor Class Definition:\")\n",
        "# Example of creating and reading from a sensor\n",
        "if SENSOR_CONFIG:\n",
        "    # Create a list of sensor objects based on SENSOR_CONFIG\n",
        "    # This step will be done formally in the IDSSimulator class later\n",
        "    temp_sensors_for_testing = []\n",
        "    for s_id, s_conf in SENSOR_CONFIG.items():\n",
        "        s_type = s_id.split('_')[0] if '_' in s_id else 'unknown'\n",
        "        temp_sensors_for_testing.append(Sensor(s_id, s_type, s_conf[\"normal_range\"], s_conf[\"unit\"]))\n",
        "\n",
        "    if temp_sensors_for_testing:\n",
        "        print(f\"\\n--- Testing Sensor Logic with {len(temp_sensors_for_testing)} configured sensors ---\")\n",
        "        # Test with the first configured sensor\n",
        "        test_sensor = temp_sensors_for_testing[0]\n",
        "        print(f\"\\n--- Testing Behavior of Sensor: {test_sensor.sensor_id} ---\")\n",
        "        print(\"Initial value (after __init__):\", test_sensor.current_value)\n",
        "        time.sleep(0.01) # simulate time passing\n",
        "        print(\"Normal reading 1:\", test_sensor.read_value())\n",
        "        time.sleep(0.01)\n",
        "        print(\"Normal reading 2:\", test_sensor.read_value())\n",
        "        time.sleep(0.01)\n",
        "        print(\"Attack (data_manipulation, intensity 2) reading:\", test_sensor.read_value(under_attack=True, attack_type=\"data_manipulation\", attack_intensity=2))\n",
        "        time.sleep(0.01)\n",
        "        print(\"Attack (sensor_outage) reading:\", test_sensor.read_value(under_attack=True, attack_type=\"sensor_outage\"))\n",
        "        time.sleep(0.01)\n",
        "        # Example of an attack type not directly handled by specific sensor logic but should still show anomaly\n",
        "        print(\"Attack (e.g., 'tcp_syn_flood' affecting environment, intensity 1) reading:\", test_sensor.read_value(under_attack=True, attack_type=\"tcp_syn_flood\", attack_intensity=1))\n",
        "        print(\"--- End Sensor Behavior Test ---\")\n",
        "else:\n",
        "    print(\"SENSOR_CONFIG is not defined or empty. Cannot run sensor test.\")\n",
        "\n",
        "print(\"\\n✅ Section 2 (Sensor Class Definition) is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzxYdLVrl0HO",
        "outputId": "3dff3c98-8f27-420a-aeca-4924f9cc0507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensor Class Definition:\n",
            "\n",
            "--- Testing Sensor Logic with 7 configured sensors ---\n",
            "\n",
            "--- Testing Behavior of Sensor: temperature_1 ---\n",
            "Initial value (after __init__): 29.03\n",
            "Normal reading 1: {'timestamp': 1748475987.239072, 'sensor_id': 'temperature_1', 'value': 16.92, 'unit': '°C', 'status': 'normal', 'event_type': 'sensor_reading'}\n",
            "Normal reading 2: {'timestamp': 1748475987.2493358, 'sensor_id': 'temperature_1', 'value': 17.1, 'unit': '°C', 'status': 'normal', 'event_type': 'sensor_reading'}\n",
            "Attack (data_manipulation, intensity 2) reading: {'timestamp': 1748475987.2595518, 'sensor_id': 'temperature_1', 'value': 100.32, 'unit': '°C', 'status': 'anomalous', 'event_type': 'sensor_reading'}\n",
            "Attack (sensor_outage) reading: {'timestamp': 1748475987.269801, 'sensor_id': 'temperature_1', 'value': 100.71, 'unit': '°C', 'status': 'anomalous', 'event_type': 'sensor_reading'}\n",
            "Attack (e.g., 'tcp_syn_flood' affecting environment, intensity 1) reading: {'timestamp': 1748475987.2800288, 'sensor_id': 'temperature_1', 'value': 99.99, 'unit': '°C', 'status': 'anomalous', 'event_type': 'sensor_reading'}\n",
            "--- End Sensor Behavior Test ---\n",
            "\n",
            "✅ Section 2 (Sensor Class Definition) is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 💻 Section 3: NetworkDevice Class Definition\n",
        "class NetworkDevice:\n",
        "    \"\"\"Class representing a network device that can be targeted by attacks.\"\"\"\n",
        "\n",
        "    def __init__(self, device_id, device_type, ip_address):\n",
        "        self.device_id = device_id\n",
        "        self.device_type = device_type\n",
        "        self.ip_address = ip_address\n",
        "        self.status = \"normal\" # General operational status\n",
        "        self.connections = random.randint(5, 50) # Initial normal connections\n",
        "        self.traffic_bytes = random.randint(1000, 100000) # Initial normal traffic in bytes\n",
        "        # print(f\"NetworkDevice initialized: {self.device_id} (Type: {self.device_type}, IP: {self.ip_address})\") # Optional\n",
        "\n",
        "    def generate_log(self, under_attack=False, attack_type=None, attack_intensity=1):\n",
        "        \"\"\"Generate network logs, potentially showing attack patterns.\"\"\"\n",
        "        timestamp = time.time()\n",
        "        log_entry = {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"device_id\": self.device_id,\n",
        "            \"ip_address\": self.ip_address,\n",
        "            \"event_type\": \"network_log\" # Differentiating from sensor events\n",
        "        }\n",
        "\n",
        "        current_device_status = \"normal\" # Status for this specific log entry\n",
        "        alert_message = None\n",
        "\n",
        "        if under_attack:\n",
        "            current_device_status = \"anomalous_activity_detected\" # General alert status\n",
        "            log_entry[\"attack_indicator\"] = attack_type\n",
        "            log_entry[\"attack_intensity_level\"] = attack_intensity\n",
        "\n",
        "            if attack_type == \"tcp_syn_flood\":\n",
        "                self.connections += int(100 * attack_intensity * random.uniform(0.8, 1.2))\n",
        "                self.traffic_bytes += int(5000 * attack_intensity * random.uniform(0.8, 1.2))\n",
        "                current_device_status = \"high_load_syn_flood\"\n",
        "                alert_message = \"Abnormal increase in half-open TCP connections.\"\n",
        "            elif attack_type == \"port_scan\":\n",
        "                self.connections += int(20 * attack_intensity * random.uniform(0.5, 1.5))\n",
        "                self.traffic_bytes += int(1000 * attack_intensity * random.uniform(0.5, 1.5))\n",
        "                current_device_status = \"suspicious_port_activity\"\n",
        "                alert_message = \"Rapid connection attempts to multiple ports detected.\"\n",
        "                log_entry[\"scanned_ports_count_estimate\"] = random.randint(10, 100) * attack_intensity\n",
        "            elif attack_type == \"udp_flood\":\n",
        "                self.traffic_bytes += int(10000 * attack_intensity * random.uniform(0.8, 1.2))\n",
        "                # UDP is connectionless; 'connections' might represent active communication sessions or flows\n",
        "                self.connections += int(5 * attack_intensity * random.uniform(0.5, 1.5))\n",
        "                current_device_status = \"high_udp_traffic_volume\"\n",
        "                alert_message = \"Excessive UDP traffic volume observed.\"\n",
        "            elif attack_type == \"arp_spoofing\":\n",
        "                current_device_status = \"arp_cache_anomaly\"\n",
        "                alert_message = \"Potential ARP cache poisoning: MAC address conflict or unsolicited ARP replies.\"\n",
        "                log_entry[\"conflicting_mac_example\"] = f\"00:1A:2B:3C:4D:{random.randint(10,99):02X}\"\n",
        "            elif attack_type == \"mitm\":\n",
        "                current_device_status = \"mitm_traffic_redirection\"\n",
        "                alert_message = \"Suspicious traffic redirection or interception; potential MITM.\"\n",
        "            elif attack_type == \"sql_injection\":\n",
        "                current_device_status = \"sql_injection_attempt_detected\"\n",
        "                alert_message = \"Potentially malicious SQL query pattern detected in application traffic.\"\n",
        "                log_entry[\"example_sql_pattern\"] = \"1' OR '1'='1\"\n",
        "            elif attack_type == \"command_injection\":\n",
        "                current_device_status = \"command_injection_attempt_detected\"\n",
        "                alert_message = \"Suspected OS command injection attempt in network request.\"\n",
        "                log_entry[\"example_command_pattern\"] = \";&cat /etc/passwd\"\n",
        "            elif attack_type == \"brute_force\":\n",
        "                self.connections += int(10 * attack_intensity * random.uniform(0.8, 1.2)) # More attempts\n",
        "                current_device_status = \"brute_force_login_attempts\"\n",
        "                alert_message = \"High rate of failed login attempts detected.\"\n",
        "                log_entry[\"failed_login_attempts_estimate\"] = random.randint(20, 100) * attack_intensity\n",
        "            elif attack_type == \"replay_attack\":\n",
        "                current_device_status = \"replay_attack_suspected\"\n",
        "                alert_message = \"Duplicate or out-of-sequence packets suggesting a replay attack.\"\n",
        "            elif attack_type == \"backdoor\":\n",
        "                current_device_status = \"unauthorized_outbound_connection\"\n",
        "                alert_message = \"Unusual outbound connection to an unknown or suspicious IP.\"\n",
        "                log_entry[\"suspicious_destination_ip\"] = f\"198.51.100.{random.randint(1,254)}\"\n",
        "            elif attack_type == \"ransomware\":\n",
        "                current_device_status = \"ransomware_network_activity\"\n",
        "                alert_message = \"Network traffic patterns consistent with ransomware (e.g., C&C, lateral movement).\"\n",
        "                self.traffic_bytes += int(2000 * attack_intensity * random.uniform(0.8, 1.2)) # May involve data exfil or C&C\n",
        "            else: # Generic or other attack types\n",
        "                self.traffic_bytes += int(1000 * attack_intensity * random.uniform(0.5,1.5))\n",
        "                self.connections += int(10 * attack_intensity * random.uniform(0.5,1.5))\n",
        "                alert_message = f\"General anomalous network activity of type: {attack_type}\"\n",
        "\n",
        "            if alert_message:\n",
        "                 log_entry[\"alert_message\"] = alert_message\n",
        "        else: # Normal operation\n",
        "            # Simulate some fluctuation\n",
        "            self.connections = max(5, self.connections + random.randint(-5, 5)) # Ensure connections don't drop too low\n",
        "            self.traffic_bytes = max(1000, self.traffic_bytes + random.randint(-2000, 2000)) # Ensure traffic doesn't drop too low\n",
        "            # Cap normal operation to prevent unrealistic indefinite growth\n",
        "            self.connections = min(self.connections, 200) # Cap normal connections\n",
        "            self.traffic_bytes = min(self.traffic_bytes, 500000) # Cap normal traffic\n",
        "            current_device_status = \"normal\"\n",
        "\n",
        "        log_entry[\"active_connections\"] = self.connections\n",
        "        log_entry[\"traffic_total_bytes\"] = self.traffic_bytes # Use a distinct name for total traffic\n",
        "        log_entry[\"device_operational_status\"] = current_device_status\n",
        "\n",
        "        # Update the class's overall status (might be a smoothed/persistent state)\n",
        "        self.status = current_device_status\n",
        "\n",
        "        return log_entry\n",
        "\n",
        "# --- Test the NetworkDevice class (optional, for this cell) ---\n",
        "print(\"NetworkDevice Class Definition:\")\n",
        "test_device = NetworkDevice(device_id=\"router_core\", device_type=\"router\", ip_address=\"192.168.1.1\")\n",
        "print(f\"\\n--- Testing Device: {test_device.device_id} ---\")\n",
        "print(\"Normal log:\", json.dumps(test_device.generate_log(), indent=2))\n",
        "time.sleep(0.01)\n",
        "print(\"\\nAttack (tcp_syn_flood) log:\", json.dumps(test_device.generate_log(under_attack=True, attack_type=\"tcp_syn_flood\", attack_intensity=3), indent=2))\n",
        "time.sleep(0.01)\n",
        "print(\"\\nAttack (port_scan) log:\", json.dumps(test_device.generate_log(under_attack=True, attack_type=\"port_scan\", attack_intensity=2), indent=2))\n",
        "print(\"\\n--- End NetworkDevice Test ---\")\n",
        "\n",
        "print(\"\\n✅ Section 3 (NetworkDevice Class Definition) is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB-wL-pbnHa8",
        "outputId": "916fb4ad-5995-47a6-fcfd-5f593eaf3147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NetworkDevice Class Definition:\n",
            "\n",
            "--- Testing Device: router_core ---\n",
            "Normal log: {\n",
            "  \"timestamp\": 1748475987.3047683,\n",
            "  \"device_id\": \"router_core\",\n",
            "  \"ip_address\": \"192.168.1.1\",\n",
            "  \"event_type\": \"network_log\",\n",
            "  \"active_connections\": 48,\n",
            "  \"traffic_total_bytes\": 27381,\n",
            "  \"device_operational_status\": \"normal\"\n",
            "}\n",
            "\n",
            "Attack (tcp_syn_flood) log: {\n",
            "  \"timestamp\": 1748475987.3152955,\n",
            "  \"device_id\": \"router_core\",\n",
            "  \"ip_address\": \"192.168.1.1\",\n",
            "  \"event_type\": \"network_log\",\n",
            "  \"attack_indicator\": \"tcp_syn_flood\",\n",
            "  \"attack_intensity_level\": 3,\n",
            "  \"alert_message\": \"Abnormal increase in half-open TCP connections.\",\n",
            "  \"active_connections\": 346,\n",
            "  \"traffic_total_bytes\": 44206,\n",
            "  \"device_operational_status\": \"high_load_syn_flood\"\n",
            "}\n",
            "\n",
            "Attack (port_scan) log: {\n",
            "  \"timestamp\": 1748475987.3260686,\n",
            "  \"device_id\": \"router_core\",\n",
            "  \"ip_address\": \"192.168.1.1\",\n",
            "  \"event_type\": \"network_log\",\n",
            "  \"attack_indicator\": \"port_scan\",\n",
            "  \"attack_intensity_level\": 2,\n",
            "  \"scanned_ports_count_estimate\": 166,\n",
            "  \"alert_message\": \"Rapid connection attempts to multiple ports detected.\",\n",
            "  \"active_connections\": 392,\n",
            "  \"traffic_total_bytes\": 45662,\n",
            "  \"device_operational_status\": \"suspicious_port_activity\"\n",
            "}\n",
            "\n",
            "--- End NetworkDevice Test ---\n",
            "\n",
            "✅ Section 3 (NetworkDevice Class Definition) is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🚀 Section 4: IDSSimulator Class Definition\n",
        "class IDSSimulator:\n",
        "    \"\"\"\n",
        "    Simulates an IoT environment with sensors and network devices,\n",
        "    and generates both normal and attack data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.sensors = []\n",
        "        self.network_devices = []\n",
        "        self.attack_in_progress = False\n",
        "        self.current_attack_type = \"normal\" # Start with normal state\n",
        "        self.attack_intensity = 0\n",
        "        self.attack_start_time = None\n",
        "        self.attack_duration = 0\n",
        "        self.logs = [] # In-memory log for the current session, if needed for quick inspection\n",
        "        self.running = False\n",
        "        self.simulation_thread = None\n",
        "\n",
        "        # Initialize sensors from SENSOR_CONFIG\n",
        "        if not SENSOR_CONFIG:\n",
        "            print(\"⚠️ SENSOR_CONFIG is empty. No sensors will be initialized.\")\n",
        "        for sensor_id, config in SENSOR_CONFIG.items():\n",
        "            sensor_type = sensor_id.split('_')[0] if '_' in sensor_id else 'generic_sensor'\n",
        "            self.sensors.append(Sensor(\n",
        "                sensor_id=sensor_id,\n",
        "                sensor_type=sensor_type,\n",
        "                normal_range=config[\"normal_range\"],\n",
        "                unit=config[\"unit\"]\n",
        "            ))\n",
        "        print(f\"IDSSimulator initialized {len(self.sensors)} sensors.\")\n",
        "\n",
        "        # Initialize some network devices\n",
        "        num_devices = 5 # Example number of network devices\n",
        "        for i in range(num_devices):\n",
        "            self.network_devices.append(NetworkDevice(\n",
        "                device_id=f\"device_{i}\",\n",
        "                device_type=random.choice([\"router\", \"switch\", \"server\", \"plc\", \"gateway\"]),\n",
        "                ip_address=f\"192.168.1.{10+i}\" # Example IP range\n",
        "            ))\n",
        "        print(f\"IDSSimulator initialized {len(self.network_devices)} network devices.\")\n",
        "\n",
        "    def start_attack(self, attack_type, duration=60, intensity=1):\n",
        "        \"\"\"Start an attack of the specified type for the given duration.\"\"\"\n",
        "        if attack_type not in ATTACK_TYPES or attack_type == \"normal\":\n",
        "            print(f\"❌ Invalid or 'normal' attack type specified: {attack_type}. Attack not started.\")\n",
        "            return\n",
        "\n",
        "        self.attack_in_progress = True\n",
        "        self.current_attack_type = attack_type\n",
        "        self.attack_intensity = max(1, min(5, intensity))  # Clamp intensity between 1-5\n",
        "        self.attack_start_time = time.time()\n",
        "        self.attack_duration = duration\n",
        "\n",
        "        print(f\"💥 Attack started: {self.current_attack_type} (Intensity: {self.attack_intensity}, Duration: {self.attack_duration:.2f}s)\")\n",
        "\n",
        "    def stop_attack(self):\n",
        "        \"\"\"Stop any ongoing attack and revert to normal.\"\"\"\n",
        "        if self.attack_in_progress:\n",
        "            attack_ended_duration = time.time() - self.attack_start_time\n",
        "            print(f\"🛡️ Attack stopped: {self.current_attack_type} (Actual Duration: {attack_ended_duration:.2f}s)\")\n",
        "            self.attack_in_progress = False\n",
        "            self.current_attack_type = \"normal\" # Revert to normal\n",
        "            self.attack_intensity = 0\n",
        "            self.attack_start_time = None\n",
        "            self.attack_duration = 0\n",
        "\n",
        "    def generate_data_point(self):\n",
        "        \"\"\"Generate a single comprehensive data point from all sensors and devices.\"\"\"\n",
        "        current_time = time.time()\n",
        "\n",
        "        # Check if an ongoing attack should end\n",
        "        if self.attack_in_progress and (current_time - self.attack_start_time > self.attack_duration):\n",
        "            self.stop_attack()\n",
        "\n",
        "        data_point = {\n",
        "            \"timestamp\": current_time,\n",
        "            \"sensors\": {},\n",
        "            \"network_devices\": {}, # Changed from \"network\" to \"network_devices\" for clarity\n",
        "            \"overall_attack_status\": { # More descriptive key\n",
        "                \"is_attack_ongoing\": self.attack_in_progress, # Changed from \"under_attack\"\n",
        "                \"active_attack_type\": self.current_attack_type, # Changed from \"attack_type\"\n",
        "                \"current_attack_intensity\": self.attack_intensity, # Changed from \"attack_intensity\"\n",
        "                \"attack_elapsed_time_seconds\": (current_time - self.attack_start_time) if self.attack_in_progress else 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Generate sensor readings\n",
        "        for sensor in self.sensors:\n",
        "            reading = sensor.read_value(\n",
        "                under_attack=self.attack_in_progress,\n",
        "                attack_type=self.current_attack_type,\n",
        "                attack_intensity=self.attack_intensity\n",
        "            )\n",
        "            data_point[\"sensors\"][sensor.sensor_id] = reading\n",
        "\n",
        "        # Generate network device logs\n",
        "        for device in self.network_devices:\n",
        "            log = device.generate_log(\n",
        "                under_attack=self.attack_in_progress,\n",
        "                attack_type=self.current_attack_type,\n",
        "                attack_intensity=self.attack_intensity\n",
        "            )\n",
        "            data_point[\"network_devices\"][device.device_id] = log\n",
        "\n",
        "        return data_point\n",
        "\n",
        "    def run_simulation(self, duration_seconds=None, data_interval_seconds=1.0, allow_random_attacks=True):\n",
        "        \"\"\"\n",
        "        Run the simulation for the specified duration, generating data at the given interval.\n",
        "        If allow_random_attacks is True, randomly start attacks during the simulation.\n",
        "        \"\"\"\n",
        "        self.running = True\n",
        "        simulation_start_time = time.time()\n",
        "\n",
        "        # Determine when the first random attack might start\n",
        "        # Allow at least some normal behavior before the first attack.\n",
        "        min_time_to_first_attack = 10 # seconds\n",
        "        max_time_to_first_attack = 30 # seconds\n",
        "        next_attack_initiation_time = simulation_start_time + random.uniform(min_time_to_first_attack, max_time_to_first_attack) if allow_random_attacks else float('inf')\n",
        "\n",
        "        print(f\"Simulation run starting. Duration: {duration_seconds if duration_seconds else 'Infinite'}. Interval: {data_interval_seconds}s. Random Attacks: {allow_random_attacks}\")\n",
        "        if allow_random_attacks:\n",
        "            print(f\"Next potential random attack initiation around: {datetime.fromtimestamp(next_attack_initiation_time).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "        log_file_path = os.path.join(LOG_DIR, f\"ids_runtime_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\")\n",
        "        print(f\"Real-time logs will be written to: {log_file_path}\")\n",
        "\n",
        "        try:\n",
        "            while self.running:\n",
        "                current_loop_time = time.time()\n",
        "                if duration_seconds is not None and (current_loop_time - simulation_start_time >= duration_seconds):\n",
        "                    print(\"Specified simulation duration reached.\")\n",
        "                    break\n",
        "\n",
        "                data_point = self.generate_data_point()\n",
        "                self.logs.append(data_point) # Append to in-memory log (optional, can be removed for long runs)\n",
        "\n",
        "                # Save individual data point to log file (JSON Lines format)\n",
        "                try:\n",
        "                    with open(log_file_path, 'a') as f:\n",
        "                        f.write(json.dumps(data_point) + '\\n')\n",
        "                except IOError as e:\n",
        "                    print(f\"Error writing to log file {log_file_path}: {e}\")\n",
        "                    # Potentially stop simulation or retry, depending on desired robustness\n",
        "\n",
        "                # Randomly start attacks if enabled and no attack is currently in progress\n",
        "                if allow_random_attacks and not self.attack_in_progress and current_loop_time >= next_attack_initiation_time:\n",
        "                    # Exclude \"normal\" from random attack choices\n",
        "                    possible_attack_types = [at for at in ATTACK_TYPES if at != \"normal\"]\n",
        "                    if possible_attack_types:\n",
        "                        chosen_attack_type = random.choice(possible_attack_types)\n",
        "                        attack_duration = random.uniform(10, 20) # Shorter, more frequent attacks for testing\n",
        "                        attack_intensity = random.randint(1, 3) # Varied intensity\n",
        "                        self.start_attack(chosen_attack_type, attack_duration, attack_intensity)\n",
        "\n",
        "                        # Schedule when the system might consider initiating the *next* attack (after this one ends + a pause)\n",
        "                        # Pause after an attack before considering another one.\n",
        "                        min_pause_after_attack = 15\n",
        "                        max_pause_after_attack = 40\n",
        "                        next_attack_initiation_time = current_loop_time + attack_duration + random.uniform(min_pause_after_attack, max_pause_after_attack)\n",
        "                        print(f\"Next potential random attack initiation around: {datetime.fromtimestamp(next_attack_initiation_time).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "                    else:\n",
        "                        print(\"No attack types available to choose from (excluding 'normal').\")\n",
        "                        next_attack_initiation_time = float('inf') # No more attacks\n",
        "\n",
        "                time.sleep(data_interval_seconds)\n",
        "        finally:\n",
        "            self.stop_attack() # Ensure any active attack is marked as stopped\n",
        "            self.running = False # Explicitly set running to false\n",
        "            print(\"Simulation run finished.\")\n",
        "\n",
        "\n",
        "    def start_simulation_thread(self, duration_seconds=None, data_interval_seconds=1.0, allow_random_attacks=True):\n",
        "        \"\"\"Start the simulation in a separate thread.\"\"\"\n",
        "        if self.simulation_thread and self.simulation_thread.is_alive():\n",
        "            print(\"⚠️ Simulation thread is already running.\")\n",
        "            return\n",
        "\n",
        "        self.simulation_thread = threading.Thread(\n",
        "            target=self.run_simulation,\n",
        "            args=(duration_seconds, data_interval_seconds, allow_random_attacks)\n",
        "        )\n",
        "        self.simulation_thread.daemon = True # Allows main program to exit even if thread is running\n",
        "        self.simulation_thread.start()\n",
        "        print(f\"🚀 Simulation thread started. Random attacks: {allow_random_attacks}. Interval: {data_interval_seconds}s.\")\n",
        "\n",
        "    def stop_simulation(self):\n",
        "        \"\"\"Stop the simulation if it's running in a thread.\"\"\"\n",
        "        print(\"Attempting to stop simulation...\")\n",
        "        self.running = False # Signal the run_simulation loop to stop\n",
        "        if self.simulation_thread and self.simulation_thread.is_alive():\n",
        "            self.simulation_thread.join(timeout=max(1.0, data_interval_seconds * 2 if 'data_interval_seconds' in locals() else 2.0) ) # Wait for the thread to finish\n",
        "            if self.simulation_thread.is_alive():\n",
        "                print(\"⚠️ Simulation thread did not terminate cleanly after timeout.\")\n",
        "            else:\n",
        "                print(\"✅ Simulation thread stopped.\")\n",
        "        else:\n",
        "            print(\"ℹ️ Simulation thread was not running or already stopped.\")\n",
        "        self.stop_attack() # Also ensure any direct attack state is cleared\n",
        "\n",
        "    def save_collected_logs(self, filename=None):\n",
        "        \"\"\"Save all in-memory logs collected during the session to a single JSON file.\"\"\"\n",
        "        if not self.logs:\n",
        "            print(\"No in-memory logs to save.\")\n",
        "            return None\n",
        "\n",
        "        if filename is None:\n",
        "            # Use DATA_DIR defined in Section 1\n",
        "            filename = os.path.join(DATA_DIR, f\"ids_collected_session_logs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
        "\n",
        "        try:\n",
        "            with open(filename, 'w') as f:\n",
        "                json.dump(self.logs, f, indent=2)\n",
        "            print(f\"📚 In-memory logs successfully saved to {filename} ({len(self.logs)} entries).\")\n",
        "            return filename\n",
        "        except IOError as e:\n",
        "            print(f\"❌ Error saving in-memory logs to {filename}: {e}\")\n",
        "            return None\n",
        "\n",
        "# --- Test the IDSSimulator class (optional, for this cell) ---\n",
        "if ('google.colab' in sys.modules) or (__name__ == \"__main__\"):\n",
        "    print(\"\\nIDSSimulator Class Definition (with testing if run as main):\")\n",
        "    simulator_test_instance = IDSSimulator()\n",
        "\n",
        "    # Test generating a single data point\n",
        "    print(\"\\n--- Testing single data point generation ---\")\n",
        "    single_point = simulator_test_instance.generate_data_point()\n",
        "    print(json.dumps(single_point, indent=2))\n",
        "    # Test starting an attack\n",
        "    print(\"\\n--- Testing attack simulation ---\")\n",
        "    simulator_test_instance.start_attack(attack_type=\"tcp_syn_flood\", duration=5, intensity=2) # Short duration for test\n",
        "    # Generate a few points while attack is active\n",
        "    for i in range(3):\n",
        "        print(f\"\\nData point during attack (iteration {i+1}):\")\n",
        "        point_during_attack = simulator_test_instance.generate_data_point()\n",
        "        print(json.dumps(point_during_attack[\"overall_attack_status\"], indent=2)) # Show attack status\n",
        "        # print(json.dumps(point_during_attack[\"sensors\"][\"temperature_1\"], indent=2)) # Example sensor\n",
        "        # print(json.dumps(point_during_attack[\"network_devices\"][\"device_0\"], indent=2)) # Example device\n",
        "        time.sleep(0.1) # Small delay for readability\n",
        "\n",
        "    # Manually check if attack should stop (generate_data_point does this, but we can force it for testing after duration)\n",
        "    print(f\"\\nWaiting for attack to auto-stop (simulated {simulator_test_instance.attack_duration}s)...\")\n",
        "    # time.sleep(simulator_test_instance.attack_duration + 0.5) # Wait for attack duration to pass\n",
        "    # print(\"Generating point after attack duration:\")\n",
        "    # point_after_attack_duration = simulator_test_instance.generate_data_point() # This call should trigger stop_attack\n",
        "    # print(json.dumps(point_after_attack_duration[\"overall_attack_status\"], indent=2))\n",
        "\n",
        "    simulator_test_instance.stop_attack() # Explicitly stop for test cleanup\n",
        "    print(\"\\n--- End IDSSimulator Test ---\")\n",
        "else:\n",
        "    print(\"IDSSimulator Class Definition (Loaded as module).\")\n",
        "\n",
        "print(\"\\n✅ Section 4 (IDSSimulator Class Definition) is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CcZTdtEnIpY",
        "outputId": "ba3bbe31-a64d-463a-8db4-a031a7f6ff88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IDSSimulator Class Definition (with testing if run as main):\n",
            "IDSSimulator initialized 7 sensors.\n",
            "IDSSimulator initialized 5 network devices.\n",
            "\n",
            "--- Testing single data point generation ---\n",
            "{\n",
            "  \"timestamp\": 1748475987.3643665,\n",
            "  \"sensors\": {\n",
            "    \"temperature_1\": {\n",
            "      \"timestamp\": 1748475987.3643703,\n",
            "      \"sensor_id\": \"temperature_1\",\n",
            "      \"value\": 19.77,\n",
            "      \"unit\": \"\\u00b0C\",\n",
            "      \"status\": \"normal\",\n",
            "      \"event_type\": \"sensor_reading\"\n",
            "    },\n",
            "    \"humidity_1\": {\n",
            "      \"timestamp\": 1748475987.3643792,\n",
            "      \"sensor_id\": \"humidity_1\",\n",
            "      \"value\": 49.17,\n",
            "      \"unit\": \"%\",\n",
            "      \"status\": \"normal\",\n",
            "      \"event_type\": \"sensor_reading\"\n",
            "    },\n",
            "    \"pressure_1\": {\n",
            "      \"timestamp\": 1748475987.3643866,\n",
            "      \"sensor_id\": \"pressure_1\",\n",
            "      \"value\": 1014.26,\n",
            "      \"unit\": \"hPa\",\n",
            "      \"status\": \"normal\",\n",
            "      \"event_type\": \"sensor_reading\"\n",
            "    },\n",
            "    \"vibration_1\": {\n",
            "      \"timestamp\": 1748475987.3643913,\n",
            "      \"sensor_id\": \"vibration_1\",\n",
            "      \"value\": 0.38,\n",
            "      \"unit\": \"g\",\n",
            "      \"status\": \"normal\",\n",
            "      \"event_type\": \"sensor_reading\"\n",
            "    },\n",
            "    \"plc_register_A\": {\n",
            "      \"timestamp\": 1748475987.3643944,\n",
            "      \"sensor_id\": \"plc_register_A\",\n",
            "      \"value\": 87.69,\n",
            "      \"unit\": \"value\",\n",
            "      \"status\": \"normal\",\n",
            "      \"event_type\": \"sensor_reading\"\n",
            "    },\n",
            "    \"flow_rate_1\": {\n",
            "      \"timestamp\": 1748475987.3643968,\n",
            "      \"sensor_id\": \"flow_rate_1\",\n",
            "      \"value\": 100.88,\n",
            "      \"unit\": \"L/min\",\n",
            "      \"status\": \"normal\",\n",
            "      \"event_type\": \"sensor_reading\"\n",
            "    },\n",
            "    \"motor_current_1\": {\n",
            "      \"timestamp\": 1748475987.3644001,\n",
            "      \"sensor_id\": \"motor_current_1\",\n",
            "      \"value\": 1.02,\n",
            "      \"unit\": \"A\",\n",
            "      \"status\": \"normal\",\n",
            "      \"event_type\": \"sensor_reading\"\n",
            "    }\n",
            "  },\n",
            "  \"network_devices\": {\n",
            "    \"device_0\": {\n",
            "      \"timestamp\": 1748475987.3644054,\n",
            "      \"device_id\": \"device_0\",\n",
            "      \"ip_address\": \"192.168.1.10\",\n",
            "      \"event_type\": \"network_log\",\n",
            "      \"active_connections\": 30,\n",
            "      \"traffic_total_bytes\": 21967,\n",
            "      \"device_operational_status\": \"normal\"\n",
            "    },\n",
            "    \"device_1\": {\n",
            "      \"timestamp\": 1748475987.3644128,\n",
            "      \"device_id\": \"device_1\",\n",
            "      \"ip_address\": \"192.168.1.11\",\n",
            "      \"event_type\": \"network_log\",\n",
            "      \"active_connections\": 24,\n",
            "      \"traffic_total_bytes\": 20616,\n",
            "      \"device_operational_status\": \"normal\"\n",
            "    },\n",
            "    \"device_2\": {\n",
            "      \"timestamp\": 1748475987.3644168,\n",
            "      \"device_id\": \"device_2\",\n",
            "      \"ip_address\": \"192.168.1.12\",\n",
            "      \"event_type\": \"network_log\",\n",
            "      \"active_connections\": 23,\n",
            "      \"traffic_total_bytes\": 58529,\n",
            "      \"device_operational_status\": \"normal\"\n",
            "    },\n",
            "    \"device_3\": {\n",
            "      \"timestamp\": 1748475987.3644202,\n",
            "      \"device_id\": \"device_3\",\n",
            "      \"ip_address\": \"192.168.1.13\",\n",
            "      \"event_type\": \"network_log\",\n",
            "      \"active_connections\": 25,\n",
            "      \"traffic_total_bytes\": 5496,\n",
            "      \"device_operational_status\": \"normal\"\n",
            "    },\n",
            "    \"device_4\": {\n",
            "      \"timestamp\": 1748475987.3644276,\n",
            "      \"device_id\": \"device_4\",\n",
            "      \"ip_address\": \"192.168.1.14\",\n",
            "      \"event_type\": \"network_log\",\n",
            "      \"active_connections\": 12,\n",
            "      \"traffic_total_bytes\": 6664,\n",
            "      \"device_operational_status\": \"normal\"\n",
            "    }\n",
            "  },\n",
            "  \"overall_attack_status\": {\n",
            "    \"is_attack_ongoing\": false,\n",
            "    \"active_attack_type\": \"normal\",\n",
            "    \"current_attack_intensity\": 0,\n",
            "    \"attack_elapsed_time_seconds\": 0\n",
            "  }\n",
            "}\n",
            "\n",
            "--- Testing attack simulation ---\n",
            "💥 Attack started: tcp_syn_flood (Intensity: 2, Duration: 5.00s)\n",
            "\n",
            "Data point during attack (iteration 1):\n",
            "{\n",
            "  \"is_attack_ongoing\": true,\n",
            "  \"active_attack_type\": \"tcp_syn_flood\",\n",
            "  \"current_attack_intensity\": 2,\n",
            "  \"attack_elapsed_time_seconds\": 1.430511474609375e-05\n",
            "}\n",
            "\n",
            "Data point during attack (iteration 2):\n",
            "{\n",
            "  \"is_attack_ongoing\": true,\n",
            "  \"active_attack_type\": \"tcp_syn_flood\",\n",
            "  \"current_attack_intensity\": 2,\n",
            "  \"attack_elapsed_time_seconds\": 0.1002969741821289\n",
            "}\n",
            "\n",
            "Data point during attack (iteration 3):\n",
            "{\n",
            "  \"is_attack_ongoing\": true,\n",
            "  \"active_attack_type\": \"tcp_syn_flood\",\n",
            "  \"current_attack_intensity\": 2,\n",
            "  \"attack_elapsed_time_seconds\": 0.2007431983947754\n",
            "}\n",
            "\n",
            "Waiting for attack to auto-stop (simulated 5s)...\n",
            "🛡️ Attack stopped: tcp_syn_flood (Actual Duration: 0.30s)\n",
            "\n",
            "--- End IDSSimulator Test ---\n",
            "\n",
            "✅ Section 4 (IDSSimulator Class Definition) is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 🧠 Section 5: IDSModelPredictor Class Definition\n",
        "#Ensure necessary imports if running this cell standalone\n",
        "import os\n",
        "import sys # <<<< ADDED THIS IMPORT\n",
        "import json\n",
        "import time\n",
        "import random # Should be available from Section 1\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import datetime # For printing timestamps in predict_on_stream\n",
        "\n",
        "class IDSModelPredictor:\n",
        "    \"\"\"\n",
        "    Loads a trained model and makes predictions on incoming data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path=None, feature_columns=None, scaler_path=None):\n",
        "        self.model = None\n",
        "        self.feature_columns = feature_columns\n",
        "        self.scaler = None\n",
        "        self.predictions = []\n",
        "        self.running = False\n",
        "        self.prediction_thread = None\n",
        "\n",
        "        if not self.feature_columns:\n",
        "            print(\"🔴 CRITICAL WARNING: 'feature_columns' list was not provided to IDSModelPredictor. \" \\\n",
        "                  \"The 'preprocess_data' method will not function correctly without it.\")\n",
        "\n",
        "        if model_path:\n",
        "            self.load_model(model_path)\n",
        "        else:\n",
        "            print(\"🔴 WARNING: No model_path provided to IDSModelPredictor. Model not loaded.\")\n",
        "\n",
        "        if scaler_path:\n",
        "            self.load_scaler(scaler_path)\n",
        "        else:\n",
        "            print(\"🔴 WARNING: No scaler_path provided to IDSModelPredictor. Initializing a new StandardScaler. \" \\\n",
        "                  \"This is for placeholder/testing only. For accurate predictions, \" \\\n",
        "                  \"the scaler used during training MUST be loaded and used.\")\n",
        "            self.scaler = StandardScaler() # Fallback, not ideal\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        \"\"\"Load a trained Keras model from the specified .h5 path.\"\"\"\n",
        "        try:\n",
        "            self.model = tf.keras.models.load_model(model_path)\n",
        "            print(f\"✅ Model successfully loaded from {model_path}\")\n",
        "            # self.model.summary()\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading Keras model from {model_path}: {e}\")\n",
        "            self.model = None\n",
        "            return False\n",
        "\n",
        "    def load_scaler(self, scaler_path):\n",
        "        \"\"\"Load a pre-fitted scaler object from pickle.\"\"\"\n",
        "        try:\n",
        "            with open(scaler_path, 'rb') as f:\n",
        "                self.scaler = pickle.load(f)\n",
        "            print(f\"✅ Scaler successfully loaded from {scaler_path}\")\n",
        "            if not (hasattr(self.scaler, 'mean_') and self.scaler.mean_ is not None):\n",
        "                 print(\"🔶 WARNING: Loaded scaler does not appear to be fitted (missing 'mean_' attribute). Preprocessing might fail or be incorrect.\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading scaler from {scaler_path}: {e}\")\n",
        "            print(\"   A new, unfitted StandardScaler will be used as a fallback (suboptimal).\")\n",
        "            if not self.scaler:\n",
        "                 self.scaler = StandardScaler()\n",
        "\n",
        "\n",
        "    def preprocess_data(self, data_point):\n",
        "        if not self.feature_columns:\n",
        "            print(\"🔴 ERROR: IDSModelPredictor.feature_columns list is not set. Cannot preprocess.\")\n",
        "            return None\n",
        "\n",
        "        raw_feature_values = {name: 0.0 for name in self.feature_columns}\n",
        "\n",
        "        # --- START USER IMPLEMENTATION TO MAP SIMULATED DATA TO MODEL FEATURES ---\n",
        "        # This is a placeholder and needs significant adaptation by you.\n",
        "        network_devices_data = data_point.get(\"network_devices\", {})\n",
        "        sensors_data = data_point.get(\"sensors\", {})\n",
        "        overall_status = data_point.get(\"overall_attack_status\", {})\n",
        "        active_attack = overall_status.get(\"active_attack_type\", \"normal\")\n",
        "\n",
        "        if network_devices_data:\n",
        "            all_traffic = [d.get(\"traffic_total_bytes\", 0) for d in network_devices_data.values()]\n",
        "            if 'tcp.len' in raw_feature_values:\n",
        "                raw_feature_values['tcp.len'] = sum(all_traffic) / len(all_traffic) if all_traffic else 0.0\n",
        "            if 'tcp.dstport' in raw_feature_values:\n",
        "                 raw_feature_values['tcp.dstport'] = 80.0 if \"http\" in active_attack.lower() or \"sql\" in active_attack.lower() else 0.0\n",
        "            if 'tcp.connection.syn' in raw_feature_values and \"syn_flood\" in active_attack.lower():\n",
        "                raw_feature_values['tcp.connection.syn'] = 1.0\n",
        "\n",
        "        if 'mbtcp.len' in raw_feature_values and \"plc_register_A\" in sensors_data:\n",
        "            plc_val = sensors_data[\"plc_register_A\"].get(\"value\")\n",
        "            raw_feature_values['mbtcp.len'] = float(plc_val) if plc_val is not None else 0.0\n",
        "        # --- END USER IMPLEMENTATION ---\n",
        "\n",
        "        feature_vector_list = [raw_feature_values.get(col_name, 0.0) for col_name in self.feature_columns]\n",
        "        features_df = pd.DataFrame([feature_vector_list], columns=self.feature_columns)\n",
        "\n",
        "        if not self.scaler:\n",
        "            print(\"🔴 ERROR: Scaler not available in preprocess_data. Cannot scale features.\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            is_scaler_fitted = hasattr(self.scaler, 'mean_') and self.scaler.mean_ is not None\n",
        "            if not is_scaler_fitted:\n",
        "                 print(\"⚠️ SCALER NOT FITTED. Fitting with current data sample. DEMO ONLY. Load pre-fitted scaler.\")\n",
        "                 scaled_features_array = self.scaler.fit_transform(features_df)\n",
        "            else:\n",
        "                 scaled_features_array = self.scaler.transform(features_df)\n",
        "        except Exception as e:\n",
        "            print(f\"🔴 ERROR during feature scaling: {e}. Features: {features_df.to_dict()}\")\n",
        "            return None\n",
        "\n",
        "        reshaped_features = scaled_features_array.reshape((1, 1, len(self.feature_columns)))\n",
        "        return reshaped_features\n",
        "\n",
        "    def predict(self, data_point):\n",
        "        if self.model is None: return None\n",
        "        processed_input = self.preprocess_data(data_point)\n",
        "        if processed_input is None: return None\n",
        "        try:\n",
        "            prediction_probs = self.model.predict(processed_input, verbose=0)[0]\n",
        "            predicted_class_index = np.argmax(prediction_probs)\n",
        "            predicted_class_name = \"Attack\" if predicted_class_index == 1 else \"Normal\"\n",
        "            actual_attack_type = data_point.get(\"overall_attack_status\", {}).get(\"active_attack_type\", \"normal\")\n",
        "            is_actually_attack = data_point.get(\"overall_attack_status\", {}).get(\"is_attack_ongoing\", False)\n",
        "            actual_label_for_comparison = \"Attack\" if is_actually_attack else \"Normal\"\n",
        "            result = {\n",
        "                \"timestamp\": data_point.get(\"timestamp\", time.time()),\n",
        "                \"predicted_class\": predicted_class_name,\n",
        "                \"prediction_probabilities\": prediction_probs.tolist(),\n",
        "                \"actual_type_from_simulator\": actual_attack_type,\n",
        "                \"is_prediction_correct\": predicted_class_name == actual_label_for_comparison\n",
        "            }\n",
        "            self.predictions.append(result)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error during model prediction: {e}\")\n",
        "            return None\n",
        "\n",
        "    def run_predictions_on_stream(self, simulator_instance, data_interval_seconds=1.0):\n",
        "        self.running = True\n",
        "        print(f\"🤖 Prediction thread started. Checking simulator data every {data_interval_seconds}s.\")\n",
        "        last_processed_log_timestamp = 0\n",
        "        self._data_interval_seconds_for_thread = data_interval_seconds\n",
        "\n",
        "        while self.running:\n",
        "            current_simulator_logs = list(simulator_instance.logs)\n",
        "            new_logs_to_process = [log for log in current_simulator_logs if log.get('timestamp', 0) > last_processed_log_timestamp]\n",
        "\n",
        "            if new_logs_to_process:\n",
        "                for data_point in new_logs_to_process:\n",
        "                    if not self.running: break\n",
        "                    prediction_result = self.predict(data_point)\n",
        "                    if prediction_result:\n",
        "                        ts_human = datetime.fromtimestamp(prediction_result['timestamp']).strftime('%H:%M:%S')\n",
        "                        probs_str = \", \".join([f\"{p:.2f}\" for p in prediction_result['prediction_probabilities']])\n",
        "                        print(f\"🔎 Prediction @ {ts_human}: Pred={prediction_result['predicted_class']}, ActualSimType='{prediction_result['actual_type_from_simulator']}' (Probs: [{probs_str}])\")\n",
        "                    last_processed_log_timestamp = data_point.get('timestamp', last_processed_log_timestamp)\n",
        "            time.sleep(data_interval_seconds)\n",
        "        print(\"🤖 Prediction thread finished.\")\n",
        "\n",
        "    def start_prediction_thread(self, simulator_instance, data_interval_seconds=1.0):\n",
        "        if not self.model: print(\"❌ Cannot start prediction: Model not loaded.\"); return\n",
        "        if self.prediction_thread and self.prediction_thread.is_alive(): print(\"⚠️ Prediction thread already running.\"); return\n",
        "        self._data_interval_seconds_for_thread = data_interval_seconds\n",
        "        self.prediction_thread = threading.Thread(target=self.run_predictions_on_stream, args=(simulator_instance, data_interval_seconds))\n",
        "        self.prediction_thread.daemon = True\n",
        "        self.prediction_thread.start()\n",
        "\n",
        "    def stop_predictions(self):\n",
        "        print(\"Attempting to stop prediction thread...\")\n",
        "        self.running = False\n",
        "        if self.prediction_thread and self.prediction_thread.is_alive():\n",
        "            timeout_wait = getattr(self, '_data_interval_seconds_for_thread', 1.0) * 2 + 1\n",
        "            self.prediction_thread.join(timeout=max(2.0, timeout_wait))\n",
        "            if self.prediction_thread.is_alive(): print(\"⚠️ Prediction thread did not terminate cleanly.\")\n",
        "            else: print(\"✅ Prediction thread stopped.\")\n",
        "        else: print(\"ℹ️ Prediction thread was not running or already stopped.\")\n",
        "\n",
        "    def save_predictions(self, filename=None):\n",
        "        if not self.predictions: print(\"No predictions to save.\"); return None\n",
        "        if filename is None:\n",
        "            # Use DATA_DIR from Section 1\n",
        "            filename = os.path.join(DATA_DIR, f\"ids_model_predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
        "        try:\n",
        "            with open(filename, 'w') as f: json.dump(self.predictions, f, indent=2)\n",
        "            print(f\"📚 Predictions saved to {filename} ({len(self.predictions)} entries).\")\n",
        "            return filename\n",
        "        except IOError as e: print(f\"❌ Error saving predictions: {e}\"); return None\n",
        "\n",
        "# --- Configuration for Predictor ---\n",
        "YOUR_MODEL_PATH = \"/content/drive/MyDrive/Colab Notebooks/results/lstm_model.h5\"\n",
        "YOUR_SCALER_PATH = \"/content/drive/MyDrive/Colab Notebooks/results/scaler.pkl\"\n",
        "\n",
        "FEATURE_NAMES_FOR_MODEL = [\n",
        "    'arp.hw.size', 'http.content_length', 'http.response', 'http.tls_port',\n",
        "    'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst',\n",
        "    'tcp.connection.syn', 'tcp.connection.synack', 'tcp.dstport', 'tcp.flags.ack',\n",
        "    'tcp.len', 'udp.stream', 'udp.time_delta', 'dns.qry.qu', 'dns.qry.type',\n",
        "    'dns.retransmission', 'dns.retransmit_request', 'dns.retransmit_request_in',\n",
        "    'mqtt.conflag.cleansess', 'mqtt.hdrflags', 'mqtt.len', 'mqtt.msg_decoded_as',\n",
        "    'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id'\n",
        "]\n",
        "print(f\"ℹ️ IDSModelPredictor configured with {len(FEATURE_NAMES_FOR_MODEL)} features.\")\n",
        "\n",
        "if not os.path.exists(YOUR_MODEL_PATH):\n",
        "    print(f\"🛑 CRITICAL WARNING: Model file NOT FOUND at {YOUR_MODEL_PATH}.\")\n",
        "if YOUR_SCALER_PATH and not os.path.exists(YOUR_SCALER_PATH):\n",
        "    print(f\"🛑 WARNING: Scaler file NOT FOUND at {YOUR_SCALER_PATH}. \" \\\n",
        "          \"Predictor will use a new, unfitted scaler (predictions will be inaccurate).\")\n",
        "elif not YOUR_SCALER_PATH:\n",
        "    print(f\"🔶 INFO: YOUR_SCALER_PATH is not set. Predictor will use a new, unfitted scaler (predictions will be inaccurate).\")\n",
        "\n",
        "\n",
        "# --- Test the IDSModelPredictor class ---\n",
        "if ('google.colab' in sys.modules) or (__name__ == \"__main__\"): # Check if in Colab or run as script\n",
        "    print(\"\\nIDSModelPredictor Class (Testing Block):\")\n",
        "\n",
        "    if not FEATURE_NAMES_FOR_MODEL:\n",
        "        print(\"\\n⚠️ Predictor test cannot run: FEATURE_NAMES_FOR_MODEL is empty.\")\n",
        "    elif not os.path.exists(YOUR_MODEL_PATH):\n",
        "        print(f\"\\n⚠️ Predictor test cannot run: Model file not found at '{YOUR_MODEL_PATH}'.\")\n",
        "    else:\n",
        "        print(\"\\n--- Initializing components for Predictor Test ---\")\n",
        "        if 'IDSSimulator' not in locals() or 'Sensor' not in locals() or 'NetworkDevice' not in locals():\n",
        "            print(\"🔴 ERROR: IDSSimulator, Sensor, or NetworkDevice class not found. Please ensure Sections 2, 3, and 4 were run successfully.\")\n",
        "        else:\n",
        "            test_simulator_for_predictor_test_block = IDSSimulator()\n",
        "\n",
        "            predictor_test_instance = IDSModelPredictor(\n",
        "                model_path=YOUR_MODEL_PATH,\n",
        "                feature_columns=FEATURE_NAMES_FOR_MODEL,\n",
        "                scaler_path=YOUR_SCALER_PATH\n",
        "            )\n",
        "\n",
        "            if predictor_test_instance.model and predictor_test_instance.scaler:\n",
        "                print(\"\\n--- Testing IDSModelPredictor.predict() method with one sample ---\")\n",
        "\n",
        "                test_data_point = test_simulator_for_predictor_test_block.generate_data_point()\n",
        "\n",
        "                print(\"\\n    Sample simulated data point (structure preview):\")\n",
        "                print(f\"    Timestamp: {test_data_point.get('timestamp')}\")\n",
        "                print(f\"    Overall attack status: {test_data_point.get('overall_attack_status')}\")\n",
        "\n",
        "                print(\"\\n    Attempting to preprocess and predict the sample data point...\")\n",
        "                prediction_output = predictor_test_instance.predict(test_data_point)\n",
        "\n",
        "                if prediction_output:\n",
        "                    print(\"\\n    Prediction Output (Sample):\")\n",
        "                    print(json.dumps(prediction_output, indent=2))\n",
        "                else:\n",
        "                    print(\"\\n    Prediction failed or returned None. Review `preprocess_data` and scaler status.\")\n",
        "            else:\n",
        "                print(\"\\n    Predictor test skipped: Model or Scaler could not be loaded/initialized properly within IDSModelPredictor. Check paths and file integrity.\")\n",
        "\n",
        "    print(\"\\n--- End IDSModelPredictor Test ---\")\n",
        "\n",
        "print(\"\\n✅ Section 5 (IDSModelPredictor Class Definition) is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRBys8Te0nFv",
        "outputId": "720127fb-ded6-45de-cbce-5360f82d8ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ℹ️ IDSModelPredictor configured with 27 features.\n",
            "\n",
            "IDSModelPredictor Class (Testing Block):\n",
            "\n",
            "--- Initializing components for Predictor Test ---\n",
            "IDSSimulator initialized 7 sensors.\n",
            "IDSSimulator initialized 5 network devices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model successfully loaded from /content/drive/MyDrive/Colab Notebooks/results/lstm_model.h5\n",
            "✅ Scaler successfully loaded from /content/drive/MyDrive/Colab Notebooks/results/scaler.pkl\n",
            "\n",
            "--- Testing IDSModelPredictor.predict() method with one sample ---\n",
            "\n",
            "    Sample simulated data point (structure preview):\n",
            "    Timestamp: 1748475987.8551893\n",
            "    Overall attack status: {'is_attack_ongoing': False, 'active_attack_type': 'normal', 'current_attack_intensity': 0, 'attack_elapsed_time_seconds': 0}\n",
            "\n",
            "    Attempting to preprocess and predict the sample data point...\n",
            "\n",
            "    Prediction Output (Sample):\n",
            "{\n",
            "  \"timestamp\": 1748475987.8551893,\n",
            "  \"predicted_class\": \"Attack\",\n",
            "  \"prediction_probabilities\": [\n",
            "    6.583260412501568e-13,\n",
            "    1.0\n",
            "  ],\n",
            "  \"actual_type_from_simulator\": \"normal\",\n",
            "  \"is_prediction_correct\": false\n",
            "}\n",
            "\n",
            "--- End IDSModelPredictor Test ---\n",
            "\n",
            "✅ Section 5 (IDSModelPredictor Class Definition) is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure necessary imports are available (should be from previous cells)\n",
        "# import os, sys, json, time, random, pandas as pd, numpy as np, pickle\n",
        "# import tensorflow as tf\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from datetime import datetime\n",
        "\n",
        "class IDSModelPredictor:\n",
        "    \"\"\"\n",
        "    Loads a trained model and makes predictions on incoming data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path=None, feature_columns=None, scaler_path=None):\n",
        "        self.model = None\n",
        "        self.feature_columns = feature_columns\n",
        "        self.scaler = None\n",
        "        self.predictions = []\n",
        "        self.running = False\n",
        "        self.prediction_thread = None\n",
        "\n",
        "        if not self.feature_columns:\n",
        "            print(\"🔴 CRITICAL WARNING: 'feature_columns' list was not provided to IDSModelPredictor. \" \\\n",
        "                  \"The 'preprocess_data' method will not function correctly without it.\")\n",
        "\n",
        "        if model_path:\n",
        "            self.load_model(model_path)\n",
        "        else:\n",
        "            print(\"🔴 WARNING: No model_path provided to IDSModelPredictor. Model not loaded.\")\n",
        "\n",
        "        if scaler_path:\n",
        "            self.load_scaler(scaler_path)\n",
        "        else:\n",
        "            print(\"🔴 WARNING: No scaler_path provided. Initializing a new StandardScaler. \" \\\n",
        "                  \"This is for placeholder/testing only. Load the original scaler for accurate predictions.\")\n",
        "            self.scaler = StandardScaler()\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        try:\n",
        "            self.model = tf.keras.models.load_model(model_path)\n",
        "            print(f\"✅ Model successfully loaded from {model_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading Keras model from {model_path}: {e}\")\n",
        "            self.model = None\n",
        "            return False\n",
        "\n",
        "    def load_scaler(self, scaler_path):\n",
        "        try:\n",
        "            with open(scaler_path, 'rb') as f:\n",
        "                self.scaler = pickle.load(f)\n",
        "            print(f\"✅ Scaler successfully loaded from {scaler_path}\")\n",
        "            if not (hasattr(self.scaler, 'mean_') and self.scaler.mean_ is not None):\n",
        "                 print(\"🔶 WARNING: Loaded scaler does not appear to be fitted. Preprocessing may be incorrect.\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading scaler from {scaler_path}: {e}\")\n",
        "            if not self.scaler: self.scaler = StandardScaler()\n",
        "\n",
        "    def preprocess_data(self, data_point):\n",
        "        if not self.feature_columns:\n",
        "            print(\"🔴 ERROR: IDSModelPredictor.feature_columns list is not set. Cannot preprocess.\")\n",
        "            return None\n",
        "\n",
        "        # Initialize all expected features with a default value (0.0)\n",
        "        raw_feature_values = {feature_name: 0.0 for feature_name in self.feature_columns}\n",
        "\n",
        "        # Extract data from the simulator's data_point\n",
        "        network_devices_data = data_point.get(\"network_devices\", {})\n",
        "        sensors_data = data_point.get(\"sensors\", {})\n",
        "        overall_status = data_point.get(\"overall_attack_status\", {})\n",
        "        active_attack = overall_status.get(\"active_attack_type\", \"normal\")\n",
        "        is_attack_ongoing = overall_status.get(\"is_attack_ongoing\", False)\n",
        "        attack_intensity = overall_status.get(\"current_attack_intensity\", 1)\n",
        "\n",
        "\n",
        "        # --- Attempting to map simulated data to the 27 features ---\n",
        "        # Many of these will be rough approximations or will remain 0.0\n",
        "        # due to the high-level nature of the current simulator.\n",
        "\n",
        "        # Network-based features (approximations)\n",
        "        if network_devices_data:\n",
        "            # Aggregate some values from all devices\n",
        "            total_traffic_bytes = sum(d.get(\"traffic_total_bytes\", 0) for d in network_devices_data.values())\n",
        "            total_active_connections = sum(d.get(\"active_connections\", 0) for d in network_devices_data.values())\n",
        "\n",
        "            # Example: 'tcp.len' - Could be average packet size if we had packet count,\n",
        "            # or just a fraction of total traffic, or total traffic if it's a single flow.\n",
        "            # For now, let's use total traffic as a proxy, scaled down.\n",
        "            raw_feature_values['tcp.len'] = total_traffic_bytes / (total_active_connections + 1) # Avoid division by zero\n",
        "\n",
        "            # Example: 'tcp.dstport' - Very speculative.\n",
        "            # If a specific attack known to target a port is active, we might set it.\n",
        "            if is_attack_ongoing:\n",
        "                if active_attack == \"sql_injection\": raw_feature_values['tcp.dstport'] = 80.0\n",
        "                elif active_attack == \"tcp_syn_flood\": raw_feature_values['tcp.dstport'] = 80.0 # Common target\n",
        "                elif active_attack == \"port_scan\": # Port scan implies multiple ports, pick one for representation or an average\n",
        "                    # The 'scanned_ports_count_estimate' is a count, not a port number.\n",
        "                    # Let's use a common scanned port range start.\n",
        "                    raw_feature_values['tcp.dstport'] = 1.0 # Just an example\n",
        "                elif active_attack == \"udp_flood\" and \"dns\" in active_attack.lower(): # If DNS based UDP flood\n",
        "                    raw_feature_values['tcp.dstport'] = 53.0 # DNS port (though this feature is tcp.dstport)\n",
        "            else: # Normal\n",
        "                raw_feature_values['tcp.dstport'] = 0.0 # Or a typical port for normal traffic if known\n",
        "\n",
        "            # Example: 'tcp.connection.syn'\n",
        "            # Set to 1.0 if a SYN flood or port scan is active\n",
        "            if active_attack in [\"tcp_syn_flood\", \"port_scan\"]:\n",
        "                raw_feature_values['tcp.connection.syn'] = 1.0 * attack_intensity # Scale by intensity\n",
        "\n",
        "            # Example: 'tcp.flags.ack' - Could be high during normal traffic or some attacks\n",
        "            # This is a very rough guess\n",
        "            raw_feature_values['tcp.flags.ack'] = 1.0 if total_active_connections > 10 else 0.0\n",
        "\n",
        "\n",
        "        # Sensor-based features (only if they map to network protocol features)\n",
        "        # mbtcp features are for Modbus/TCP, often used with PLCs\n",
        "        plc_sensor_id = \"plc_register_A\"\n",
        "        if plc_sensor_id in sensors_data:\n",
        "            plc_reading = sensors_data[plc_sensor_id]\n",
        "            if plc_reading and plc_reading.get(\"value\") is not None:\n",
        "                raw_feature_values['mbtcp.len'] = float(plc_reading.get(\"value\", 0.0)) # Assuming value maps to length\n",
        "                raw_feature_values['mbtcp.trans_id'] = float(plc_reading.get(\"timestamp\", 0.0) % 1000) # Using timestamp as a proxy for transaction ID\n",
        "                raw_feature_values['mbtcp.unit_id'] = 1.0 # Assuming unit ID 1 for this PLC\n",
        "\n",
        "        # Features that are hard to simulate without detailed packet info will remain 0.0 or need defaults:\n",
        "        # 'arp.hw.size': Typically 6 for Ethernet. Let's set a default if no ARP spoofing.\n",
        "        raw_feature_values['arp.hw.size'] = 6.0 if active_attack != \"arp_spoofing\" else 0.0 # Or a different value during spoofing\n",
        "\n",
        "        # 'http.content_length', 'http.response', 'http.tls_port':\n",
        "        # These would need specific HTTP simulation.\n",
        "        if active_attack in [\"sql_injection\", \"command_injection\"] and \"http\" in active_attack.lower(): # if it's an HTTP based attack\n",
        "            raw_feature_values['http.content_length'] = random.uniform(50,500) # Example\n",
        "            raw_feature_values['http.response'] = 1.0 # Assuming a response is generated\n",
        "\n",
        "        # TCP specific flags and values:\n",
        "        # 'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst', 'tcp.connection.synack'\n",
        "        # These are highly specific and would remain 0.0 or need very specific attack simulation logic.\n",
        "\n",
        "        # UDP specific:\n",
        "        # 'udp.stream', 'udp.time_delta'\n",
        "        if active_attack == \"udp_flood\":\n",
        "            raw_feature_values['udp.stream'] = float(attack_intensity) # Example mapping\n",
        "            raw_feature_values['udp.time_delta'] = random.uniform(0.001, 0.1) # Example\n",
        "\n",
        "        # DNS specific:\n",
        "        # 'dns.qry.qu', 'dns.qry.type', 'dns.retransmission', 'dns.retransmit_request', 'dns.retransmit_request_in'\n",
        "        # Would require DNS query/response simulation.\n",
        "\n",
        "        # MQTT specific:\n",
        "        # 'mqtt.conflag.cleansess', 'mqtt.hdrflags', 'mqtt.len', 'mqtt.msg_decoded_as'\n",
        "        # Would require MQTT protocol simulation.\n",
        "\n",
        "        # Final vector assembly\n",
        "        feature_vector_list = [raw_feature_values.get(col_name, 0.0) for col_name in self.feature_columns]\n",
        "        features_df = pd.DataFrame([feature_vector_list], columns=self.feature_columns)\n",
        "\n",
        "        if not self.scaler:\n",
        "            print(\"🔴 ERROR: Scaler not available in preprocess_data. Cannot scale features.\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            is_scaler_fitted = hasattr(self.scaler, 'mean_') and self.scaler.mean_ is not None\n",
        "            if not is_scaler_fitted:\n",
        "                 print(\"⚠️ SCALER NOT FITTED. Fitting with current data sample. DEMO ONLY. Load pre-fitted scaler.\")\n",
        "                 scaled_features_array = self.scaler.fit_transform(features_df)\n",
        "            else:\n",
        "                 scaled_features_array = self.scaler.transform(features_df)\n",
        "        except Exception as e:\n",
        "            print(f\"🔴 ERROR during feature scaling: {e}. Features: {features_df.to_dict()}\")\n",
        "            return None\n",
        "\n",
        "        reshaped_features = scaled_features_array.reshape((1, 1, len(self.feature_columns)))\n",
        "        return reshaped_features\n",
        "\n",
        "    # ... (predict, run_predictions_on_stream, start_prediction_thread, stop_predictions, save_predictions methods remain the same as in the previous Section 5 response) ...\n",
        "    def predict(self, data_point):\n",
        "        if self.model is None: return None\n",
        "        processed_input = self.preprocess_data(data_point)\n",
        "        if processed_input is None: return None\n",
        "        try:\n",
        "            prediction_probs = self.model.predict(processed_input, verbose=0)[0]\n",
        "            predicted_class_index = np.argmax(prediction_probs)\n",
        "            predicted_class_name = \"Attack\" if predicted_class_index == 1 else \"Normal\"\n",
        "            actual_attack_type = data_point.get(\"overall_attack_status\", {}).get(\"active_attack_type\", \"normal\")\n",
        "            is_actually_attack = data_point.get(\"overall_attack_status\", {}).get(\"is_attack_ongoing\", False)\n",
        "            actual_label_for_comparison = \"Attack\" if is_actually_attack else \"Normal\"\n",
        "            result = {\n",
        "                \"timestamp\": data_point.get(\"timestamp\", time.time()),\n",
        "                \"predicted_class\": predicted_class_name,\n",
        "                \"prediction_probabilities\": prediction_probs.tolist(), # [prob_normal, prob_attack]\n",
        "                \"actual_type_from_simulator\": actual_attack_type,\n",
        "                \"is_prediction_correct\": predicted_class_name == actual_label_for_comparison\n",
        "            }\n",
        "            self.predictions.append(result)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error during model prediction: {e}\")\n",
        "            return None\n",
        "\n",
        "    def run_predictions_on_stream(self, simulator_instance, data_interval_seconds=1.0):\n",
        "        self.running = True\n",
        "        print(f\"🤖 Prediction thread started. Checking simulator data every {data_interval_seconds}s.\")\n",
        "        last_processed_log_timestamp = 0\n",
        "        self._data_interval_seconds_for_thread = data_interval_seconds\n",
        "\n",
        "        while self.running:\n",
        "            current_simulator_logs = list(simulator_instance.logs)\n",
        "            new_logs_to_process = [log for log in current_simulator_logs if log.get('timestamp', 0) > last_processed_log_timestamp]\n",
        "\n",
        "            if new_logs_to_process:\n",
        "                for data_point in new_logs_to_process:\n",
        "                    if not self.running: break\n",
        "                    prediction_result = self.predict(data_point)\n",
        "                    if prediction_result:\n",
        "                        ts_human = datetime.fromtimestamp(prediction_result['timestamp']).strftime('%H:%M:%S')\n",
        "                        probs_str = \", \".join([f\"{p:.2f}\" for p in prediction_result['prediction_probabilities']])\n",
        "                        print(f\"🔎 Prediction @ {ts_human}: Pred={prediction_result['predicted_class']}, ActualSimType='{prediction_result['actual_type_from_simulator']}' (Probs: [{probs_str}])\")\n",
        "                    last_processed_log_timestamp = data_point.get('timestamp', last_processed_log_timestamp)\n",
        "            time.sleep(data_interval_seconds)\n",
        "        print(\"🤖 Prediction thread finished.\")\n",
        "\n",
        "    def start_prediction_thread(self, simulator_instance, data_interval_seconds=1.0):\n",
        "        if not self.model: print(\"❌ Cannot start prediction: Model not loaded.\"); return\n",
        "        if self.prediction_thread and self.prediction_thread.is_alive(): print(\"⚠️ Prediction thread already running.\"); return\n",
        "        self._data_interval_seconds_for_thread = data_interval_seconds\n",
        "        self.prediction_thread = threading.Thread(target=self.run_predictions_on_stream, args=(simulator_instance, data_interval_seconds))\n",
        "        self.prediction_thread.daemon = True\n",
        "        self.prediction_thread.start()\n",
        "\n",
        "    def stop_predictions(self):\n",
        "        print(\"Attempting to stop prediction thread...\")\n",
        "        self.running = False\n",
        "        if self.prediction_thread and self.prediction_thread.is_alive():\n",
        "            timeout_wait = getattr(self, '_data_interval_seconds_for_thread', 1.0) * 2 + 1\n",
        "            self.prediction_thread.join(timeout=max(2.0, timeout_wait))\n",
        "            if self.prediction_thread.is_alive(): print(\"⚠️ Prediction thread did not terminate cleanly.\")\n",
        "            else: print(\"✅ Prediction thread stopped.\")\n",
        "        else: print(\"ℹ️ Prediction thread was not running or already stopped.\")\n",
        "\n",
        "    def save_predictions(self, filename=None):\n",
        "        if not self.predictions: print(\"No predictions to save.\"); return None\n",
        "        if filename is None:\n",
        "            # Use DATA_DIR from Section 1\n",
        "            filename = os.path.join(DATA_DIR, f\"ids_model_predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
        "        try:\n",
        "            with open(filename, 'w') as f: json.dump(self.predictions, f, indent=2)\n",
        "            print(f\"📚 Predictions saved to {filename} ({len(self.predictions)} entries).\")\n",
        "            return filename\n",
        "        except IOError as e: print(f\"❌ Error saving predictions: {e}\"); return None\n",
        "\n",
        "# --- Configuration for Predictor (using paths from your successful MLP_LSTM run) ---\n",
        "YOUR_MODEL_PATH = \"/content/drive/MyDrive/Colab Notebooks/results/lstm_model.h5\"\n",
        "YOUR_SCALER_PATH = \"/content/drive/MyDrive/Colab Notebooks/results/scaler.pkl\"\n",
        "\n",
        "FEATURE_NAMES_FOR_MODEL = [\n",
        "    'arp.hw.size', 'http.content_length', 'http.response', 'http.tls_port',\n",
        "    'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst',\n",
        "    'tcp.connection.syn', 'tcp.connection.synack', 'tcp.dstport', 'tcp.flags.ack',\n",
        "    'tcp.len', 'udp.stream', 'udp.time_delta', 'dns.qry.qu', 'dns.qry.type',\n",
        "    'dns.retransmission', 'dns.retransmit_request', 'dns.retransmit_request_in',\n",
        "    'mqtt.conflag.cleansess', 'mqtt.hdrflags', 'mqtt.len', 'mqtt.msg_decoded_as',\n",
        "    'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id'\n",
        "]\n",
        "print(f\"ℹ️ IDSModelPredictor configured with {len(FEATURE_NAMES_FOR_MODEL)} features.\")\n",
        "\n",
        "if not os.path.exists(YOUR_MODEL_PATH):\n",
        "    print(f\"🛑 CRITICAL WARNING: Model file NOT FOUND at {YOUR_MODEL_PATH}.\")\n",
        "if YOUR_SCALER_PATH and not os.path.exists(YOUR_SCALER_PATH): # Check only if path is provided\n",
        "    print(f\"🛑 WARNING: Scaler file NOT FOUND at {YOUR_SCALER_PATH}.\")\n",
        "elif not YOUR_SCALER_PATH:\n",
        "     print(f\"🔶 INFO: YOUR_SCALER_PATH is not set. Predictor will use a new, unfitted scaler (not recommended for accuracy).\")\n",
        "\n",
        "\n",
        "# --- Test the IDSModelPredictor class ---\n",
        "if ('google.colab' in sys.modules) or (__name__ == \"__main__\"):\n",
        "    print(\"\\nIDSModelPredictor Class (Testing Block with Improved Preprocessing Attempt):\")\n",
        "\n",
        "    if not FEATURE_NAMES_FOR_MODEL:\n",
        "        print(\"\\n⚠️ Predictor test cannot run: FEATURE_NAMES_FOR_MODEL is empty.\")\n",
        "    elif not os.path.exists(YOUR_MODEL_PATH):\n",
        "        print(f\"\\n⚠️ Predictor test cannot run: Model file not found at '{YOUR_MODEL_PATH}'.\")\n",
        "    else:\n",
        "        print(\"\\n--- Initializing components for Predictor Test ---\")\n",
        "        if 'IDSSimulator' not in locals() or 'Sensor' not in locals() or 'NetworkDevice' not in locals():\n",
        "            print(\"🔴 ERROR: Prerequisite classes (IDSSimulator, Sensor, NetworkDevice) not found. Ensure Sections 1-4 ran.\")\n",
        "        else:\n",
        "            test_simulator_for_predictor_test_block = IDSSimulator()\n",
        "\n",
        "            predictor_test_instance = IDSModelPredictor(\n",
        "                model_path=YOUR_MODEL_PATH,\n",
        "                feature_columns=FEATURE_NAMES_FOR_MODEL,\n",
        "                scaler_path=YOUR_SCALER_PATH\n",
        "            )\n",
        "\n",
        "            if predictor_test_instance.model and predictor_test_instance.scaler:\n",
        "                print(\"\\n--- Testing IDSModelPredictor.predict() with one sample using updated preprocess_data ---\")\n",
        "\n",
        "                print(\"\\n   Generating a NORMAL data point for testing...\")\n",
        "                test_data_point_normal = test_simulator_for_predictor_test_block.generate_data_point()\n",
        "                prediction_normal = predictor_test_instance.predict(test_data_point_normal)\n",
        "                if prediction_normal:\n",
        "                    print(\"    Prediction for NORMAL event:\")\n",
        "                    print(json.dumps(prediction_normal, indent=2))\n",
        "\n",
        "                print(\"\\n   Generating an ATTACK data point for testing (tcp_syn_flood)...\")\n",
        "                test_simulator_for_predictor_test_block.start_attack(attack_type=\"tcp_syn_flood\", duration=1, intensity=3)\n",
        "                test_data_point_attack = test_simulator_for_predictor_test_block.generate_data_point()\n",
        "                test_simulator_for_predictor_test_block.stop_attack() # Clean up attack state\n",
        "                prediction_attack = predictor_test_instance.predict(test_data_point_attack)\n",
        "                if prediction_attack:\n",
        "                    print(\"    Prediction for ATTACK event (tcp_syn_flood):\")\n",
        "                    print(json.dumps(prediction_attack, indent=2))\n",
        "\n",
        "                print(\"\\n   Note: The accuracy of these predictions is highly dependent on how well the \" \\\n",
        "                      \"simulated features in `preprocess_data` match the real dataset features your model learned.\")\n",
        "            else:\n",
        "                print(\"\\n    Predictor test skipped: Model or Scaler could not be loaded/initialized properly.\")\n",
        "\n",
        "    print(\"\\n--- End IDSModelPredictor Test ---\")\n",
        "\n",
        "print(\"\\n✅ Section 5 (IDSModelPredictor Class Definition with improved preprocess_data) is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N60n5lnr8ois",
        "outputId": "34c4c0fb-0d90-479d-9eef-666cbb806e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ℹ️ IDSModelPredictor configured with 27 features.\n",
            "\n",
            "IDSModelPredictor Class (Testing Block with Improved Preprocessing Attempt):\n",
            "\n",
            "--- Initializing components for Predictor Test ---\n",
            "IDSSimulator initialized 7 sensors.\n",
            "IDSSimulator initialized 5 network devices.\n",
            "✅ Model successfully loaded from /content/drive/MyDrive/Colab Notebooks/results/lstm_model.h5\n",
            "✅ Scaler successfully loaded from /content/drive/MyDrive/Colab Notebooks/results/scaler.pkl\n",
            "\n",
            "--- Testing IDSModelPredictor.predict() with one sample using updated preprocess_data ---\n",
            "\n",
            "   Generating a NORMAL data point for testing...\n",
            "    Prediction for NORMAL event:\n",
            "{\n",
            "  \"timestamp\": 1748475988.416281,\n",
            "  \"predicted_class\": \"Attack\",\n",
            "  \"prediction_probabilities\": [\n",
            "    1.8337467508899863e-06,\n",
            "    0.9999982118606567\n",
            "  ],\n",
            "  \"actual_type_from_simulator\": \"normal\",\n",
            "  \"is_prediction_correct\": false\n",
            "}\n",
            "\n",
            "   Generating an ATTACK data point for testing (tcp_syn_flood)...\n",
            "💥 Attack started: tcp_syn_flood (Intensity: 3, Duration: 1.00s)\n",
            "🛡️ Attack stopped: tcp_syn_flood (Actual Duration: 0.00s)\n",
            "    Prediction for ATTACK event (tcp_syn_flood):\n",
            "{\n",
            "  \"timestamp\": 1748475988.7994184,\n",
            "  \"predicted_class\": \"Attack\",\n",
            "  \"prediction_probabilities\": [\n",
            "    4.870793546274399e-09,\n",
            "    1.0\n",
            "  ],\n",
            "  \"actual_type_from_simulator\": \"tcp_syn_flood\",\n",
            "  \"is_prediction_correct\": true\n",
            "}\n",
            "\n",
            "   Note: The accuracy of these predictions is highly dependent on how well the simulated features in `preprocess_data` match the real dataset features your model learned.\n",
            "\n",
            "--- End IDSModelPredictor Test ---\n",
            "\n",
            "✅ Section 5 (IDSModelPredictor Class Definition with improved preprocess_data) is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure necessary imports if running this cell standalone\n",
        "import os\n",
        "import sys # For sys.executable\n",
        "import json # For saving results\n",
        "import time\n",
        "import threading\n",
        "from datetime import datetime # For filenames\n",
        "# Sensor, NetworkDevice, IDSSimulator, IDSModelPredictor classes should be defined\n",
        "# from running the previous cells (Sections 2, 3, 4, 5).\n",
        "# BASE_DIR, DATA_DIR, LOG_DIR, YOUR_MODEL_PATH, YOUR_SCALER_PATH, FEATURE_NAMES_FOR_MODEL\n",
        "# should also be defined from previous cells (Sections 1 and 5).\n",
        "\n",
        "class IDSOrchestrator:\n",
        "    \"\"\"\n",
        "    Orchestrates the IDS components: simulator, model predictor, and (optionally) a dashboard.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path_from_config, feature_columns_from_config, scaler_path_from_config, dashboard_script_name=\"dashboard.py\"):\n",
        "        # Initialize simulator\n",
        "        if 'IDSSimulator' not in globals():\n",
        "            print(\"🔴 ERROR: IDSSimulator class definition not found. Please run Section 4.\")\n",
        "            self.simulator = None\n",
        "        else:\n",
        "            self.simulator = IDSSimulator() # This will print its own init messages\n",
        "\n",
        "        # Initialize predictor\n",
        "        if 'IDSModelPredictor' not in globals():\n",
        "            print(\"🔴 ERROR: IDSModelPredictor class definition not found. Please run Section 5.\")\n",
        "            self.predictor = None\n",
        "        else:\n",
        "            self.predictor = IDSModelPredictor(\n",
        "                model_path=model_path_from_config,\n",
        "                feature_columns=feature_columns_from_config,\n",
        "                scaler_path=scaler_path_from_config\n",
        "            ) # This will print its own init messages for model and scaler\n",
        "\n",
        "        # BASE_DIR should be globally available from Section 1\n",
        "        self.dashboard_script_path = os.path.join(BASE_DIR, dashboard_script_name)\n",
        "        self.running = False\n",
        "        self.dashboard_process = None\n",
        "        print(\"✅ IDSOrchestrator initialized.\")\n",
        "        if not self.simulator: print(\"   - Simulator: Not initialized (IDSSimulator class missing)\")\n",
        "        if not self.predictor: print(\"   - Predictor: Not initialized (IDSModelPredictor class missing)\")\n",
        "        # Further checks for model/scaler loading are done by IDSModelPredictor's init\n",
        "\n",
        "    def start(self, simulation_duration_seconds=None, data_interval_seconds=1.0, allow_random_attacks=True, start_dashboard=False):\n",
        "        \"\"\"Start the IDS system: simulation, prediction, and optionally the dashboard.\"\"\"\n",
        "        if not self.simulator or not self.predictor:\n",
        "            print(\"❌ Orchestrator cannot start: Simulator or Predictor not properly initialized (check previous cell outputs).\")\n",
        "            return\n",
        "\n",
        "        self.running = True\n",
        "        print(f\"\\n🚀 Starting IDS Orchestration... (Duration: {simulation_duration_seconds or 'Continuous'}, Data Interval: {data_interval_seconds}s)\")\n",
        "\n",
        "        # Start simulator thread\n",
        "        self.simulator.start_simulation_thread(\n",
        "            duration_seconds=simulation_duration_seconds,\n",
        "            data_interval_seconds=data_interval_seconds,\n",
        "            allow_random_attacks=allow_random_attacks\n",
        "        )\n",
        "\n",
        "        # Start predictor thread (only if model was successfully loaded in predictor's init)\n",
        "        if self.predictor.model and self.predictor.scaler:\n",
        "            # Prediction interval can be the same as data interval, or different if desired\n",
        "            self.predictor.start_prediction_thread(\n",
        "                simulator_instance=self.simulator, # Pass the simulator instance\n",
        "                data_interval_seconds=data_interval_seconds # Predictor checks for new logs at this interval\n",
        "            )\n",
        "        else:\n",
        "            print(\"🔶 INFO: Model or Scaler not loaded in predictor. Real-time predictions will not run.\")\n",
        "            print(\"   Please check model/scaler paths and ensure they loaded correctly in Section 5.\")\n",
        "\n",
        "\n",
        "        # Start dashboard (optional)\n",
        "        if start_dashboard:\n",
        "            if os.path.exists(self.dashboard_script_path):\n",
        "                try:\n",
        "                    import subprocess\n",
        "                    print(f\"Attempting to start dashboard: {self.dashboard_script_path}\")\n",
        "                    # For Colab, running Streamlit this way is complex for viewing.\n",
        "                    # It's better to run dashboard.py in its own dedicated environment/process.\n",
        "                    self.dashboard_process = subprocess.Popen(\n",
        "                        [sys.executable, self.dashboard_script_path],\n",
        "                        stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
        "                    )\n",
        "                    print(f\"✅ Dashboard process initiated (PID: {self.dashboard_process.pid}). Monitor its logs separately.\")\n",
        "                    # Check if it errored out immediately (very basic check)\n",
        "                    time.sleep(3) # Give it a moment\n",
        "                    if self.dashboard_process.poll() is not None:\n",
        "                        stdout, stderr = self.dashboard_process.communicate()\n",
        "                        print(f\"⚠️ Dashboard process terminated early. RC: {self.dashboard_process.returncode}\")\n",
        "                        print(f\"   Dashboard stdout: {stdout.decode(errors='ignore')}\")\n",
        "                        print(f\"   Dashboard stderr: {stderr.decode(errors='ignore')}\")\n",
        "                        self.dashboard_process = None\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error starting dashboard subprocess: {e}\")\n",
        "                    self.dashboard_process = None\n",
        "            else:\n",
        "                print(f\"🔶 INFO: Dashboard script not found at '{self.dashboard_script_path}'. Dashboard not started.\")\n",
        "\n",
        "        print(\"✅ IDS Orchestration components (Simulator & Predictor Threads) initiated.\")\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop all components of the IDS system.\"\"\"\n",
        "        print(\"\\n🛑 Stopping IDS Orchestration...\")\n",
        "        self.running = False\n",
        "\n",
        "        if self.predictor:\n",
        "            self.predictor.stop_predictions() # Signals predictor's internal loop to stop\n",
        "        if self.simulator:\n",
        "            self.simulator.stop_simulation() # Signals simulator's internal loop to stop\n",
        "\n",
        "        # Wait for threads to actually finish (join them)\n",
        "        if self.predictor and self.predictor.prediction_thread and self.predictor.prediction_thread.is_alive():\n",
        "            print(\"   Waiting for prediction thread to complete...\")\n",
        "            self.predictor.prediction_thread.join(timeout=5.0)\n",
        "        if self.simulator and self.simulator.simulation_thread and self.simulator.simulation_thread.is_alive():\n",
        "            print(\"   Waiting for simulation thread to complete...\")\n",
        "            self.simulator.simulation_thread.join(timeout=5.0)\n",
        "\n",
        "        if self.dashboard_process:\n",
        "            try:\n",
        "                print(\"Terminating dashboard process...\")\n",
        "                self.dashboard_process.terminate()\n",
        "                self.dashboard_process.wait(timeout=5)\n",
        "                print(\"✅ Dashboard process terminated.\")\n",
        "            except subprocess.TimeoutExpired:\n",
        "                print(\"⚠️ Dashboard process did not terminate gracefully, killing...\")\n",
        "                self.dashboard_process.kill()\n",
        "                print(\"✅ Dashboard process killed.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error stopping dashboard process: {e}\")\n",
        "            finally:\n",
        "                self.dashboard_process = None\n",
        "\n",
        "        print(\"✅ IDS Orchestration fully stopped.\")\n",
        "\n",
        "    def save_session_results(self, base_filename_prefix=None):\n",
        "        \"\"\"Save all collected logs and predictions from the session.\"\"\"\n",
        "        if base_filename_prefix is None:\n",
        "            # datetime should be available from Section 1 imports\n",
        "            base_filename_prefix = f\"ids_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "        saved_files = {}\n",
        "        print(\"\\n💾 Saving session results...\")\n",
        "        # DATA_DIR and LOG_DIR should be globally available from Section 1\n",
        "        if self.simulator:\n",
        "            # This saves the self.logs list (in-memory copy) from the simulator\n",
        "            sim_log_file = self.simulator.save_collected_logs(\n",
        "                os.path.join(DATA_DIR, f\"{base_filename_prefix}_simulator_collected_all.json\")\n",
        "            )\n",
        "            if sim_log_file: saved_files['simulator_collected_logs'] = sim_log_file\n",
        "            print(f\"   Note: Continuous real-time logs from simulator were also saved to individual files in: {LOG_DIR}/\")\n",
        "\n",
        "        if self.predictor:\n",
        "            pred_file = self.predictor.save_predictions(\n",
        "                os.path.join(DATA_DIR, f\"{base_filename_prefix}_model_predictions.json\")\n",
        "            )\n",
        "            if pred_file: saved_files['model_predictions'] = pred_file\n",
        "\n",
        "        if saved_files:\n",
        "            print(f\"✅ Session results saved. Check files: {saved_files}\")\n",
        "        else:\n",
        "            print(\"ℹ️ No results to save or saving failed for all components.\")\n",
        "        return saved_files\n",
        "\n",
        "# --- Test the IDSOrchestrator class (optional for this cell) ---\n",
        "# This test block will run if the cell is executed directly.\n",
        "if ('google.colab' in sys.modules) or (__name__ == \"__main__\"):\n",
        "    print(\"\\nIDSOrchestrator Class (Testing Block):\")\n",
        "\n",
        "    # Check if prerequisite variables (defined in previous cells) exist\n",
        "    prerequisites_ok = True\n",
        "    required_globals = ['BASE_DIR', 'YOUR_MODEL_PATH', 'YOUR_SCALER_PATH', 'FEATURE_NAMES_FOR_MODEL',\n",
        "                        'IDSSimulator', 'IDSModelPredictor', 'Sensor', 'NetworkDevice']\n",
        "    for req_var in required_globals:\n",
        "        if req_var not in globals():\n",
        "            print(f\"🔴 ERROR: Prerequisite '{req_var}' not found in global scope. \" \\\n",
        "                  \"Please ensure all previous sections (1-5) were run successfully.\")\n",
        "            prerequisites_ok = False\n",
        "            break\n",
        "\n",
        "    if prerequisites_ok and (not os.path.exists(YOUR_MODEL_PATH) or (YOUR_SCALER_PATH and not os.path.exists(YOUR_SCALER_PATH))):\n",
        "        print(\"🔴 ERROR: Model or Scaler file path is invalid or file not found. Orchestrator test cannot proceed meaningfully.\")\n",
        "        if not os.path.exists(YOUR_MODEL_PATH): print(f\"   Missing Model: {YOUR_MODEL_PATH}\")\n",
        "        if YOUR_SCALER_PATH and not os.path.exists(YOUR_SCALER_PATH): print(f\"   Missing Scaler: {YOUR_SCALER_PATH}\")\n",
        "        prerequisites_ok = False\n",
        "\n",
        "    if prerequisites_ok:\n",
        "        print(\"\\n--- Initializing Orchestrator for Test ---\")\n",
        "        orchestrator_test_instance = IDSOrchestrator(\n",
        "            model_path_from_config=YOUR_MODEL_PATH,       # Defined in Section 5\n",
        "            feature_columns_from_config=FEATURE_NAMES_FOR_MODEL, # Defined in Section 5\n",
        "            scaler_path_from_config=YOUR_SCALER_PATH         # Defined in Section 5\n",
        "        )\n",
        "\n",
        "        if orchestrator_test_instance.simulator and orchestrator_test_instance.predictor and \\\n",
        "           orchestrator_test_instance.predictor.model and orchestrator_test_instance.predictor.scaler:\n",
        "            print(\"\\n--- Starting Orchestrator Test (short duration: 7s, interval: 1s) ---\")\n",
        "            # Set start_dashboard=False for this test to avoid Colab subprocess issues with Streamlit\n",
        "            orchestrator_test_instance.start(\n",
        "                simulation_duration_seconds=7,\n",
        "                data_interval_seconds=1,\n",
        "                allow_random_attacks=True,\n",
        "                start_dashboard=False\n",
        "            )\n",
        "\n",
        "            print(f\"\\n⏳ Orchestrator test running for ~7 seconds... Check console for simulation and prediction logs.\")\n",
        "            # The threads run in the background. We wait here for them to do some work.\n",
        "            # The simulation itself will stop after 7 seconds due to its internal duration.\n",
        "            time.sleep(8) # Wait a bit longer than simulation duration to allow logs to print and threads to wind down.\n",
        "\n",
        "            print(\"\\n--- Stopping Orchestrator Test ---\")\n",
        "            orchestrator_test_instance.stop()\n",
        "\n",
        "            print(\"\\n--- Saving Orchestrator Test Session Results ---\")\n",
        "            orchestrator_test_instance.save_session_results(\"orchestrator_test_session\")\n",
        "        else:\n",
        "            print(\"🔴 Orchestrator test skipped: Simulator, Predictor, Model, or Scaler failed to initialize properly within Orchestrator. Check earlier logs.\")\n",
        "    else:\n",
        "        print(\"🔴 Orchestrator test skipped due to missing prerequisites or files.\")\n",
        "\n",
        "    print(\"\\n--- End IDSOrchestrator Test ---\")\n",
        "\n",
        "print(\"\\n✅ Section 6 (IDSOrchestrator Class Definition) is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUJAhrpy4k9P",
        "outputId": "402fdc6b-3cdf-4f90-90e2-157dc1081fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IDSOrchestrator Class (Testing Block):\n",
            "\n",
            "--- Initializing Orchestrator for Test ---\n",
            "IDSSimulator initialized 7 sensors.\n",
            "IDSSimulator initialized 5 network devices.\n",
            "✅ Model successfully loaded from /content/drive/MyDrive/Colab Notebooks/results/lstm_model.h5\n",
            "✅ Scaler successfully loaded from /content/drive/MyDrive/Colab Notebooks/results/scaler.pkl\n",
            "✅ IDSOrchestrator initialized.\n",
            "\n",
            "--- Starting Orchestrator Test (short duration: 7s, interval: 1s) ---\n",
            "\n",
            "🚀 Starting IDS Orchestration... (Duration: 7, Data Interval: 1s)\n",
            "Simulation run starting. Duration: 7. Interval: 1s. Random Attacks: True\n",
            "Next potential random attack initiation around: 2025-05-28 23:46:42\n",
            "Real-time logs will be written to: /content/drive/MyDrive/IDS_AI_Project/logs/ids_runtime_log_20250528_234629.jsonl\n",
            "🚀 Simulation thread started. Random attacks: True. Interval: 1s.\n",
            "🤖 Prediction thread started. Checking simulator data every 1s.\n",
            "✅ IDS Orchestration components (Simulator & Predictor Threads) initiated.\n",
            "\n",
            "⏳ Orchestrator test running for ~7 seconds... Check console for simulation and prediction logs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 26 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ae0e466e7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Prediction @ 23:46:29: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:30: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:31: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:32: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:33: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:34: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "Specified simulation duration reached.\n",
            "Simulation run finished.\n",
            "🔎 Prediction @ 23:46:35: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "\n",
            "--- Stopping Orchestrator Test ---\n",
            "\n",
            "🛑 Stopping IDS Orchestration...\n",
            "Attempting to stop prediction thread...\n",
            "🤖 Prediction thread finished.\n",
            "✅ Prediction thread stopped.\n",
            "Attempting to stop simulation...\n",
            "ℹ️ Simulation thread was not running or already stopped.\n",
            "✅ IDS Orchestration fully stopped.\n",
            "\n",
            "--- Saving Orchestrator Test Session Results ---\n",
            "\n",
            "💾 Saving session results...\n",
            "📚 In-memory logs successfully saved to /content/drive/MyDrive/IDS_AI_Project/data/orchestrator_test_session_simulator_collected_all.json (7 entries).\n",
            "   Note: Continuous real-time logs from simulator were also saved to individual files in: /content/drive/MyDrive/IDS_AI_Project/logs/\n",
            "📚 Predictions saved to /content/drive/MyDrive/IDS_AI_Project/data/orchestrator_test_session_model_predictions.json (7 entries).\n",
            "✅ Session results saved. Check files: {'simulator_collected_logs': '/content/drive/MyDrive/IDS_AI_Project/data/orchestrator_test_session_simulator_collected_all.json', 'model_predictions': '/content/drive/MyDrive/IDS_AI_Project/data/orchestrator_test_session_model_predictions.json'}\n",
            "\n",
            "--- End IDSOrchestrator Test ---\n",
            "\n",
            "✅ Section 6 (IDSOrchestrator Class Definition) is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure necessary imports if running this cell standalone\n",
        "# (os, sys, json, time, threading, datetime, argparse should be imported from Section 1)\n",
        "# All classes (Sensor, NetworkDevice, IDSSimulator, IDSModelPredictor, IDSOrchestrator)\n",
        "# and global configurations (BASE_DIR, YOUR_MODEL_PATH, etc.) should be defined from previous cells.\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to initialize and run the IDS system based on command-line arguments.\n",
        "    In a Colab environment, we will simulate these arguments.\n",
        "    \"\"\"\n",
        "    # For Colab, we'll define args manually instead of parsing them from command line\n",
        "    # In a standalone .py script, argparse would parse them.\n",
        "    class Args:\n",
        "        def __init__(self):\n",
        "            # --- Use the paths and configurations we've established ---\n",
        "            self.model = YOUR_MODEL_PATH                # Defined in Section 5\n",
        "            self.scaler = YOUR_SCALER_PATH              # Defined in Section 5\n",
        "            self.feature_list = FEATURE_NAMES_FOR_MODEL # Defined in Section 5\n",
        "\n",
        "            # --- Simulation Parameters (you can adjust these for testing) ---\n",
        "            self.duration = 15  # Run for 15 seconds for this test\n",
        "            self.interval = 1.0 # Generate data every 1 second\n",
        "            self.no_random_attacks = False # Allow random attacks during the 15s run\n",
        "            self.start_dashboard_process = False # Keep dashboard off for Colab test simplicity\n",
        "            self.save_results_at_end = True # Save results after this run\n",
        "\n",
        "    args = Args() # Simulate parsed arguments\n",
        "\n",
        "    print(\"--- Initializing IDS System (via main function) ---\")\n",
        "\n",
        "    # Check if prerequisite global variables are available\n",
        "    if 'YOUR_MODEL_PATH' not in globals() or \\\n",
        "       'FEATURE_NAMES_FOR_MODEL' not in globals(): # YOUR_SCALER_PATH can be None\n",
        "        print(\"🔴 ERROR: Critical configuration variables (YOUR_MODEL_PATH, FEATURE_NAMES_FOR_MODEL) are not defined.\")\n",
        "        print(\"   Please ensure Section 5 was run and these variables are set.\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(args.model):\n",
        "        print(f\"🔴 ERROR: Model file not found at the specified path: {args.model}\")\n",
        "        print(\"   Please ensure the model path is correct and the file exists.\")\n",
        "        return\n",
        "\n",
        "    if args.scaler and not os.path.exists(args.scaler):\n",
        "        print(f\"🔶 WARNING: Scaler file not found at {args.scaler}. Predictor will use a new, unfitted scaler.\")\n",
        "\n",
        "\n",
        "    # Create and configure orchestrator\n",
        "    orchestrator = IDSOrchestrator(\n",
        "        model_path_from_config=args.model,\n",
        "        feature_columns_from_config=args.feature_list,\n",
        "        scaler_path_from_config=args.scaler\n",
        "        # dashboard_script_name can be customized if needed, default is \"dashboard.py\"\n",
        "    )\n",
        "\n",
        "    if not orchestrator.simulator or not orchestrator.predictor or not orchestrator.predictor.model:\n",
        "        print(\"🔴 ERROR: Orchestrator components (simulator, predictor, or model) failed to initialize. Cannot start.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        print(\"\\n--- Starting IDS System Operation (via main function) ---\")\n",
        "        orchestrator.start(\n",
        "            simulation_duration_seconds=args.duration,\n",
        "            data_interval_seconds=args.interval,\n",
        "            allow_random_attacks=(not args.no_random_attacks),\n",
        "            start_dashboard=args.start_dashboard_process\n",
        "        )\n",
        "\n",
        "        # Let the simulation run.\n",
        "        # The orchestrator's threads will manage the duration.\n",
        "        # We'll wait for a bit longer than the duration to allow for shutdown messages.\n",
        "        if args.duration:\n",
        "            print(f\"\\n⏳ Simulation will run for {args.duration} seconds. Monitoring threads...\")\n",
        "            # Check if threads are alive periodically\n",
        "            main_thread_start_time = time.time()\n",
        "            while time.time() - main_thread_start_time < (args.duration + 5): # Wait duration + buffer\n",
        "                if orchestrator.simulator.simulation_thread and not orchestrator.simulator.simulation_thread.is_alive() and \\\n",
        "                   orchestrator.predictor.prediction_thread and not orchestrator.predictor.prediction_thread.is_alive():\n",
        "                    print(\"ℹ️ Simulator and Predictor threads have completed their work based on duration.\")\n",
        "                    break\n",
        "                time.sleep(1) # Check status every second\n",
        "            print(\"ℹ️ Main function wait period complete.\")\n",
        "        else:\n",
        "            # If running continuously (no duration), this part of main would typically not be reached\n",
        "            # unless an external signal (like Ctrl+C in a script) stops it.\n",
        "            # For Colab, continuous run would need manual interruption of the cell.\n",
        "            print(\"ℹ️ Simulation set to run continuously (or until cell is interrupted).\")\n",
        "            # In a real script, you'd have a loop here or just let threads run until KeyboardInterrupt.\n",
        "            # For this Colab cell, we'll just let the threads run for a bit if duration is None.\n",
        "            if orchestrator.running: # Check if it actually started\n",
        "                 time.sleep(10) # Example wait for a \"continuous\" test run\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n🚨 KeyboardInterrupt received! Stopping IDS system...\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An unexpected error occurred in main: {e}\")\n",
        "    finally:\n",
        "        print(\"\\n--- Shutting Down IDS System (via main function) ---\")\n",
        "        orchestrator.stop() # Ensure components are stopped\n",
        "\n",
        "        if args.save_results_at_end:\n",
        "            print(\"\\n--- Saving Final Session Results (via main function) ---\")\n",
        "            orchestrator.save_session_results(f\"ids_main_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
        "\n",
        "    print(\"\\n🏁 IDS System Main Execution Finished.\")\n",
        "\n",
        "# --- This is how you would typically run the main function ---\n",
        "# We will call it directly in Colab for testing.\n",
        "if ('google.colab' in sys.modules) or (__name__ == \"__main__\"):\n",
        "    print(\"\\nRunning main() function for a demonstration of the IDS System...\")\n",
        "    # Before running main, ensure all class definitions and global configs are loaded\n",
        "    # (i.e., all previous cells for Sections 1-6 have been executed in this session)\n",
        "\n",
        "    # Quick check for some essential globals from previous sections\n",
        "    essential_globals_for_main = ['YOUR_MODEL_PATH', 'YOUR_SCALER_PATH', 'FEATURE_NAMES_FOR_MODEL', 'BASE_DIR', 'DATA_DIR', 'LOG_DIR', 'IDSSimulator', 'IDSModelPredictor', 'IDSOrchestrator']\n",
        "    all_essentials_defined = True\n",
        "    for var_name in essential_globals_for_main:\n",
        "        if var_name not in globals():\n",
        "            print(f\"🔴 CRITICAL ERROR for main(): Global variable '{var_name}' is not defined. \" \\\n",
        "                  \"Please ensure all previous code sections (1-6) have been run successfully.\")\n",
        "            all_essentials_defined = False\n",
        "            break\n",
        "\n",
        "    if all_essentials_defined:\n",
        "        main()\n",
        "    else:\n",
        "        print(\"\\n❌ Main function execution aborted due to missing definitions from previous sections.\")\n",
        "\n",
        "else:\n",
        "    print(\"Script loaded as a module, not running main() automatically.\")\n",
        "\n",
        "print(\"\\n✅ Section 7 (Main Execution Block) is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPfymQrN5SET",
        "outputId": "b23d0b7a-29d3-420d-b096-b24aeb4da224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running main() function for a demonstration of the IDS System...\n",
            "--- Initializing IDS System (via main function) ---\n",
            "IDSSimulator initialized 7 sensors.\n",
            "IDSSimulator initialized 5 network devices.\n",
            "✅ Model successfully loaded from /content/drive/MyDrive/Colab Notebooks/results/lstm_model.h5\n",
            "✅ Scaler successfully loaded from /content/drive/MyDrive/Colab Notebooks/results/scaler.pkl\n",
            "✅ IDSOrchestrator initialized.\n",
            "\n",
            "--- Starting IDS System Operation (via main function) ---\n",
            "\n",
            "🚀 Starting IDS Orchestration... (Duration: 15, Data Interval: 1.0s)\n",
            "Simulation run starting. Duration: 15. Interval: 1.0s. Random Attacks: True\n",
            "Next potential random attack initiation around: 2025-05-28 23:47:03\n",
            "Real-time logs will be written to: /content/drive/MyDrive/IDS_AI_Project/logs/ids_runtime_log_20250528_234637.jsonl\n",
            "🚀 Simulation thread started. Random attacks: True. Interval: 1.0s.\n",
            "🤖 Prediction thread started. Checking simulator data every 1.0s.\n",
            "✅ IDS Orchestration components (Simulator & Predictor Threads) initiated.\n",
            "\n",
            "⏳ Simulation will run for 15 seconds. Monitoring threads...\n",
            "🔎 Prediction @ 23:46:37: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:38: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:39: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:40: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:41: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:42: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:43: Pred=Normal, ActualSimType='normal' (Probs: [0.66, 0.34])\n",
            "🔎 Prediction @ 23:46:44: Pred=Normal, ActualSimType='normal' (Probs: [0.87, 0.13])\n",
            "🔎 Prediction @ 23:46:45: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:46: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:47: Pred=Normal, ActualSimType='normal' (Probs: [0.80, 0.20])\n",
            "🔎 Prediction @ 23:46:48: Pred=Attack, ActualSimType='normal' (Probs: [0.12, 0.88])\n",
            "🔎 Prediction @ 23:46:49: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:50: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:51: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "Specified simulation duration reached.\n",
            "Simulation run finished.\n",
            "ℹ️ Main function wait period complete.\n",
            "\n",
            "--- Shutting Down IDS System (via main function) ---\n",
            "\n",
            "🛑 Stopping IDS Orchestration...\n",
            "Attempting to stop prediction thread...\n",
            "🤖 Prediction thread finished.\n",
            "✅ Prediction thread stopped.\n",
            "Attempting to stop simulation...\n",
            "ℹ️ Simulation thread was not running or already stopped.\n",
            "✅ IDS Orchestration fully stopped.\n",
            "\n",
            "--- Saving Final Session Results (via main function) ---\n",
            "\n",
            "💾 Saving session results...\n",
            "📚 In-memory logs successfully saved to /content/drive/MyDrive/IDS_AI_Project/data/ids_main_run_20250528_234658_simulator_collected_all.json (15 entries).\n",
            "   Note: Continuous real-time logs from simulator were also saved to individual files in: /content/drive/MyDrive/IDS_AI_Project/logs/\n",
            "📚 Predictions saved to /content/drive/MyDrive/IDS_AI_Project/data/ids_main_run_20250528_234658_model_predictions.json (15 entries).\n",
            "✅ Session results saved. Check files: {'simulator_collected_logs': '/content/drive/MyDrive/IDS_AI_Project/data/ids_main_run_20250528_234658_simulator_collected_all.json', 'model_predictions': '/content/drive/MyDrive/IDS_AI_Project/data/ids_main_run_20250528_234658_model_predictions.json'}\n",
            "\n",
            "🏁 IDS System Main Execution Finished.\n",
            "\n",
            "✅ Section 7 (Main Execution Block) is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure necessary imports if running this cell standalone\n",
        "# os, sys, json, time, threading, datetime, argparse were imported in Section 1.\n",
        "# All classes (Sensor, NetworkDevice, IDSSimulator, IDSModelPredictor, IDSOrchestrator)\n",
        "# and global configurations (BASE_DIR, YOUR_MODEL_PATH, etc.) should be defined\n",
        "# from running the previous cells.\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to initialize and run the IDS system.\n",
        "    In a Colab environment, we simulate command-line arguments.\n",
        "    \"\"\"\n",
        "    # For Colab, we define args manually. In a standalone .py script, argparse would parse them.\n",
        "    class Args:\n",
        "        def __init__(self):\n",
        "            # --- Use the paths and configurations established in previous sections ---\n",
        "            self.model = YOUR_MODEL_PATH                # Should be globally defined from Section 5\n",
        "            self.scaler = YOUR_SCALER_PATH              # Should be globally defined from Section 5\n",
        "            self.feature_list = FEATURE_NAMES_FOR_MODEL # Should be globally defined from Section 5\n",
        "\n",
        "            # --- Simulation Parameters (adjust for different test runs) ---\n",
        "            self.duration = 20  # Run for 20 seconds for this demonstration\n",
        "            self.interval = 1.0 # Generate data every 1 second\n",
        "            self.no_random_attacks = False # Allow random attacks during the run\n",
        "            self.start_dashboard_process = False # Keep dashboard off for Colab simplicity\n",
        "            self.save_results_at_end = True # Save results after this run\n",
        "\n",
        "    args = Args() # Simulate parsed arguments\n",
        "\n",
        "    print(\"--- Initializing IDS System (via main function) ---\")\n",
        "\n",
        "    # Basic checks for essential configurations\n",
        "    if not all(arg is not None for arg in [args.model, args.feature_list]): # Scaler can be None and handled by Predictor\n",
        "        print(\"🔴 ERROR: Critical configurations (model path or feature list) are not set. Cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(args.model):\n",
        "        print(f\"🔴 ERROR: Model file not found at the specified path: {args.model}\")\n",
        "        return\n",
        "\n",
        "    if args.scaler and not os.path.exists(args.scaler):\n",
        "        print(f\"🔶 WARNING: Specified Scaler file not found at {args.scaler}. \"\n",
        "              \"Predictor will use a new, unfitted scaler (suboptimal).\")\n",
        "\n",
        "    # Create and configure orchestrator\n",
        "    orchestrator = IDSOrchestrator(\n",
        "        model_path_from_config=args.model,\n",
        "        feature_columns_from_config=args.feature_list,\n",
        "        scaler_path_from_config=args.scaler\n",
        "    )\n",
        "\n",
        "    # Check if orchestrator's components initialized correctly\n",
        "    if not orchestrator.simulator or \\\n",
        "       not orchestrator.predictor or \\\n",
        "       (orchestrator.predictor and not orchestrator.predictor.model) or \\\n",
        "       (orchestrator.predictor and not orchestrator.predictor.scaler and args.scaler): # If scaler was expected but not loaded\n",
        "        print(\"🔴 ERROR: Orchestrator components (simulator, predictor, model, or expected scaler) \"\n",
        "              \"failed to initialize properly. Cannot start.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        print(\"\\n--- Starting IDS System Operation (via main function) ---\")\n",
        "        orchestrator.start(\n",
        "            simulation_duration_seconds=args.duration,\n",
        "            data_interval_seconds=args.interval,\n",
        "            allow_random_attacks=(not args.no_random_attacks),\n",
        "            start_dashboard=args.start_dashboard_process\n",
        "        )\n",
        "\n",
        "        if args.duration:\n",
        "            print(f\"\\n⏳ Simulation will run for {args.duration} seconds. Monitoring...\")\n",
        "            # The simulation runs in a thread managed by the orchestrator.\n",
        "            # This main thread will wait for the duration plus a buffer to allow for clean shutdown.\n",
        "            # A more robust implementation might join the orchestrator's main thread if it had one,\n",
        "            # or use other synchronization primitives.\n",
        "            time.sleep(args.duration + 5) # Wait for duration + buffer for logs and shutdown\n",
        "            print(\"ℹ️ Main function wait period for simulation duration complete.\")\n",
        "        else:\n",
        "            print(\"ℹ️ Simulation set to run continuously. Manually interrupt cell to stop.\")\n",
        "            # In a script, this would loop until KeyboardInterrupt. In Colab, cell needs manual stop.\n",
        "            while orchestrator.running: # Fallback if orchestrator manages a global running state for main\n",
        "                time.sleep(1)\n",
        "\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n🚨 KeyboardInterrupt received! Initiating graceful shutdown of IDS system...\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An unexpected error occurred in main execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Print full traceback for unexpected errors\n",
        "    finally:\n",
        "        print(\"\\n--- Shutting Down IDS System (via main function) ---\")\n",
        "        if 'orchestrator' in locals() and orchestrator: # Ensure orchestrator was initialized\n",
        "            orchestrator.stop() # Ensure components are stopped\n",
        "\n",
        "            if args.save_results_at_end:\n",
        "                print(\"\\n--- Saving Final Session Results (via main function) ---\")\n",
        "                orchestrator.save_session_results(f\"ids_main_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
        "\n",
        "    print(\"\\n🏁 IDS System Main Execution Finished.\")\n",
        "\n",
        "# This block ensures main() is called when the cell is run.\n",
        "if ('google.colab' in sys.modules) or (__name__ == \"__main__\"):\n",
        "    print(\"\\nRunning main() function for a demonstration of the IDS System...\")\n",
        "    # Check if essential global variables from previous sections are available\n",
        "    essential_globals = ['YOUR_MODEL_PATH', 'YOUR_SCALER_PATH', 'FEATURE_NAMES_FOR_MODEL',\n",
        "                         'BASE_DIR', 'DATA_DIR', 'LOG_DIR',\n",
        "                         'IDSSimulator', 'IDSModelPredictor', 'IDSOrchestrator',\n",
        "                         'Sensor', 'NetworkDevice', 'SENSOR_CONFIG', 'ATTACK_TYPES']\n",
        "    all_defined = True\n",
        "    for var_name in essential_globals:\n",
        "        if var_name not in globals():\n",
        "            print(f\"🔴 CRITICAL ERROR for main(): Global variable or class '{var_name}' is not defined. \"\n",
        "                  \"Please ensure all previous code sections (1-6) have been run successfully in this session.\")\n",
        "            all_defined = False\n",
        "            break\n",
        "\n",
        "    if all_defined:\n",
        "        main()\n",
        "    else:\n",
        "        print(\"\\n❌ Main function execution aborted due to missing definitions from previous sections.\")\n",
        "else:\n",
        "    print(\"Script loaded as a module, not running main() automatically.\")\n",
        "\n",
        "print(\"\\n✅ Section 7 (Main Execution Block) is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyYSbM2N7hF2",
        "outputId": "f9fad841-cbc1-4e2c-e895-c4f810724e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running main() function for a demonstration of the IDS System...\n",
            "--- Initializing IDS System (via main function) ---\n",
            "IDSSimulator initialized 7 sensors.\n",
            "IDSSimulator initialized 5 network devices.\n",
            "✅ Model successfully loaded from /content/drive/MyDrive/Colab Notebooks/results/lstm_model.h5\n",
            "✅ Scaler successfully loaded from /content/drive/MyDrive/Colab Notebooks/results/scaler.pkl\n",
            "✅ IDSOrchestrator initialized.\n",
            "\n",
            "--- Starting IDS System Operation (via main function) ---\n",
            "\n",
            "🚀 Starting IDS Orchestration... (Duration: 20, Data Interval: 1.0s)\n",
            "Simulation run starting. Duration: 20. Interval: 1.0s. Random Attacks: True\n",
            "Next potential random attack initiation around: 2025-05-28 23:47:26\n",
            "Real-time logs will be written to: /content/drive/MyDrive/IDS_AI_Project/logs/ids_runtime_log_20250528_234658.jsonl\n",
            "🚀 Simulation thread started. Random attacks: True. Interval: 1.0s.\n",
            "🤖 Prediction thread started. Checking simulator data every 1.0s.\n",
            "✅ IDS Orchestration components (Simulator & Predictor Threads) initiated.\n",
            "\n",
            "⏳ Simulation will run for 20 seconds. Monitoring...\n",
            "🔎 Prediction @ 23:46:58: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:46:59: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:00: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:01: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:02: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:03: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:04: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:05: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:06: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:07: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:08: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:09: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:10: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:11: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:12: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:13: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:14: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:15: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "🔎 Prediction @ 23:47:16: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "Specified simulation duration reached.\n",
            "Simulation run finished.\n",
            "🔎 Prediction @ 23:47:17: Pred=Attack, ActualSimType='normal' (Probs: [0.00, 1.00])\n",
            "ℹ️ Main function wait period for simulation duration complete.\n",
            "\n",
            "--- Shutting Down IDS System (via main function) ---\n",
            "\n",
            "🛑 Stopping IDS Orchestration...\n",
            "Attempting to stop prediction thread...\n",
            "🤖 Prediction thread finished.\n",
            "✅ Prediction thread stopped.\n",
            "Attempting to stop simulation...\n",
            "ℹ️ Simulation thread was not running or already stopped.\n",
            "✅ IDS Orchestration fully stopped.\n",
            "\n",
            "--- Saving Final Session Results (via main function) ---\n",
            "\n",
            "💾 Saving session results...\n",
            "📚 In-memory logs successfully saved to /content/drive/MyDrive/IDS_AI_Project/data/ids_main_run_20250528_234723_simulator_collected_all.json (20 entries).\n",
            "   Note: Continuous real-time logs from simulator were also saved to individual files in: /content/drive/MyDrive/IDS_AI_Project/logs/\n",
            "📚 Predictions saved to /content/drive/MyDrive/IDS_AI_Project/data/ids_main_run_20250528_234723_model_predictions.json (20 entries).\n",
            "✅ Session results saved. Check files: {'simulator_collected_logs': '/content/drive/MyDrive/IDS_AI_Project/data/ids_main_run_20250528_234723_simulator_collected_all.json', 'model_predictions': '/content/drive/MyDrive/IDS_AI_Project/data/ids_main_run_20250528_234723_model_predictions.json'}\n",
            "\n",
            "🏁 IDS System Main Execution Finished.\n",
            "\n",
            "✅ Section 7 (Main Execution Block) is ready.\n"
          ]
        }
      ]
    }
  ]
}