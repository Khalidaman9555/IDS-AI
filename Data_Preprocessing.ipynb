{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1CgM72bgm5-YAB-EJrTstP4jjtMmY-nIt",
      "authorship_tag": "ABX9TyMJpABxO1MX/B8j0MsYYG5r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khalidaman9555/IDS-AI/blob/main/Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7jhcPcEkvI4",
        "outputId": "8b53a9a4-6aa6-4bcf-d94f-b2c84f468f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n"
          ]
        }
      ],
      "source": [
        "# Step II.A: Mount Google Drive\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    # Consider adding error handling or exiting if Drive mount fails"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure kagglehub is installed (usually pre-installed in Colab)\n",
        "!pip install kagglehub -q"
      ],
      "metadata": {
        "id": "bxwMkfbNqQg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step II.B: Download dataset using kagglehub\n",
        "import kagglehub\n",
        "import os # Import os for path operations later\n",
        "\n",
        "try:\n",
        "    # Download latest version\n",
        "    dataset_path_runtime = kagglehub.dataset_download(\"mohamedamineferrag/edgeiiotset-cyber-security-dataset-of-iot-iiot\")\n",
        "    print(f\"Dataset downloaded to temporary runtime path: {dataset_path_runtime}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading dataset via kagglehub: {e}\")\n",
        "    dataset_path_runtime = None # Ensure variable exists but indicates failure"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44IFGAb2qSBo",
        "outputId": "8c4d88a3-38a3-4d5c-c7c4-9b2375f9e708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to temporary runtime path: /kaggle/input/edgeiiotset-cyber-security-dataset-of-iot-iiot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step II.C: Inspect downloaded files and identify target CSV\n",
        "# Ensure the 'os' module was imported in the previous step (II.B)\n",
        "# Ensure 'dataset_path_runtime' variable holds the correct path from step II.B output\n",
        "\n",
        "target_csv_filename = 'ML-EdgeIIoT-dataset.csv'\n",
        "target_csv_path_runtime = None # Initialize path variable\n",
        "\n",
        "# Use the dataset_path_runtime variable from the previous step's output\n",
        "# Make sure it's defined correctly in your notebook session.\n",
        "# If not, you might need to manually set it:\n",
        "# dataset_path_runtime = '/kaggle/input/edgeiiotset-cyber-security-dataset-of-iot-iiot' # Example if needed\n",
        "\n",
        "if dataset_path_runtime and os.path.exists(dataset_path_runtime):\n",
        "  print(\"\\nInspecting downloaded dataset contents...\")\n",
        "\n",
        "  # Check common potential subdirectories based on Kaggle examples and the PDF\n",
        "  potential_base_dirs = [\n",
        "      dataset_path_runtime,\n",
        "      os.path.join(dataset_path_runtime, 'Edge-IIoTset dataset'),\n",
        "      os.path.join(dataset_path_runtime, 'Edge-IIoTset dataset', 'Selected dataset for ML and DL')\n",
        "  ]\n",
        "  potential_paths = [os.path.join(base, target_csv_filename) for base in potential_base_dirs]\n",
        "\n",
        "  for potential_path in potential_paths:\n",
        "    if os.path.exists(potential_path):\n",
        "      target_csv_path_runtime = potential_path\n",
        "      print(f\"Identified target CSV file at: {target_csv_path_runtime}\")\n",
        "      break # Stop searching once found\n",
        "\n",
        "  if not target_csv_path_runtime:\n",
        "    print(f\"\\nError: Could not automatically locate '{target_csv_filename}'.\")\n",
        "    print(\"Please manually inspect the contents of:\")\n",
        "    print(f\"{dataset_path_runtime}\")\n",
        "    print(\"And update the 'target_csv_path_runtime' variable accordingly.\")\n",
        "    # List top-level contents to help user\n",
        "    try:\n",
        "        print(\"\\nTop-level contents:\")\n",
        "        for item in os.listdir(dataset_path_runtime):\n",
        "            print(os.path.join(dataset_path_runtime, item))\n",
        "    except Exception as e:\n",
        "        print(f\"Could not list directory contents: {e}\")\n",
        "\n",
        "else:\n",
        "  print(\"\\nSkipping file inspection: Dataset download failed or path is invalid.\")\n",
        "  print(f\"Please ensure 'dataset_path_runtime' is set correctly to: {dataset_path_runtime}\")\n",
        "\n",
        "# This variable will be used in the next step\n",
        "print(f\"\\n'target_csv_path_runtime' is now set to: {target_csv_path_runtime}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DiykmgUqcl8",
        "outputId": "511eb5df-037d-4251-9870-79e996f674d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inspecting downloaded dataset contents...\n",
            "Identified target CSV file at: /kaggle/input/edgeiiotset-cyber-security-dataset-of-iot-iiot/Edge-IIoTset dataset/Selected dataset for ML and DL/ML-EdgeIIoT-dataset.csv\n",
            "\n",
            "'target_csv_path_runtime' is now set to: /kaggle/input/edgeiiotset-cyber-security-dataset-of-iot-iiot/Edge-IIoTset dataset/Selected dataset for ML and DL/ML-EdgeIIoT-dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step II.D: Copy Target CSV to Google Drive for Persistence\n",
        "import shutil # Import shutil for file operations\n",
        "\n",
        "# Ensure 'os' module was imported earlier\n",
        "# Ensure 'target_csv_path_runtime' holds the correct path from step II.C\n",
        "\n",
        "# Define the destination path in Google Drive\n",
        "# You can change 'Colab Notebooks/datasets/' if you prefer a different location\n",
        "destination_folder_drive = '/content/drive/MyDrive/Colab Notebooks/datasets/'\n",
        "destination_path_drive = os.path.join(destination_folder_drive, target_csv_filename) # Reuse target_csv_filename\n",
        "\n",
        "if target_csv_path_runtime and os.path.exists(target_csv_path_runtime):\n",
        "  try:\n",
        "    # Create the destination directory in Google Drive if it doesn't exist\n",
        "    os.makedirs(destination_folder_drive, exist_ok=True)\n",
        "    print(f\"Destination folder '{destination_folder_drive}' ensured.\")\n",
        "\n",
        "    # Copy the file\n",
        "    shutil.copyfile(target_csv_path_runtime, destination_path_drive)\n",
        "    print(f\"\\nSuccessfully copied:\")\n",
        "    print(f\"  FROM (runtime): {target_csv_path_runtime}\")\n",
        "    print(f\"  TO (Drive):     {destination_path_drive}\")\n",
        "\n",
        "    # Verify the copy (optional but recommended)\n",
        "    if os.path.exists(destination_path_drive):\n",
        "      print(\"\\nCopy verified successfully in Google Drive.\")\n",
        "      # Store the Drive path for the next step\n",
        "      target_csv_path_drive = destination_path_drive\n",
        "    else:\n",
        "      print(\"\\nError: Copy verification failed. File not found at destination.\")\n",
        "      target_csv_path_drive = None\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"\\nError copying file to Google Drive: {e}\")\n",
        "    target_csv_path_drive = None\n",
        "\n",
        "else:\n",
        "  print(\"\\nSkipping copy: Source CSV path is invalid or file does not exist.\")\n",
        "  print(f\"Please ensure 'target_csv_path_runtime' ({target_csv_path_runtime}) is correct.\")\n",
        "  target_csv_path_drive = None\n",
        "\n",
        "# This variable holds the path in Google Drive for future steps\n",
        "print(f\"\\n'target_csv_path_drive' is now set to: {target_csv_path_drive}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrnsSKgsrKBJ",
        "outputId": "791738c8-8456-42c2-fcbb-35d01a82b409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destination folder '/content/drive/MyDrive/Colab Notebooks/datasets/' ensured.\n",
            "\n",
            "Successfully copied:\n",
            "  FROM (runtime): /kaggle/input/edgeiiotset-cyber-security-dataset-of-iot-iiot/Edge-IIoTset dataset/Selected dataset for ML and DL/ML-EdgeIIoT-dataset.csv\n",
            "  TO (Drive):     /content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\n",
            "\n",
            "Copy verified successfully in Google Drive.\n",
            "\n",
            "'target_csv_path_drive' is now set to: /content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step III: Load the Dataset from Google Drive using Pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure 'target_csv_path_drive' holds the correct Google Drive path from step II.D\n",
        "# Ensure 'os' module was imported earlier if you want to re-verify path existence\n",
        "\n",
        "# Define the path explicitly if the variable is not available (replace if needed)\n",
        "# target_csv_path_drive = '/content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv'\n",
        "\n",
        "df = None # Initialize DataFrame variable\n",
        "\n",
        "if target_csv_path_drive and os.path.exists(target_csv_path_drive):\n",
        "  try:\n",
        "    print(f\"Loading dataset from: {target_csv_path_drive}\")\n",
        "    # Load the CSV file into a pandas DataFrame\n",
        "    # The PDF mentions potential \"low_memory=False\" usage if dtype warnings appear\n",
        "    df = pd.read_csv(target_csv_path_drive, low_memory=False)\n",
        "\n",
        "    print(\"\\nDataset loaded successfully.\")\n",
        "    # Display the dimensions (shape) of the DataFrame\n",
        "    print(f\"DataFrame shape (rows, columns): {df.shape}\")\n",
        "\n",
        "    # Display the first few rows to verify loading\n",
        "    print(\"\\nFirst 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: File not found at the specified path: {target_csv_path_drive}\")\n",
        "    print(\"Please ensure the path is correct and the file exists in your Google Drive.\")\n",
        "  except pd.errors.EmptyDataError:\n",
        "    print(f\"Error: The file at {target_csv_path_drive} is empty.\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred while loading the dataset: {e}\")\n",
        "else:\n",
        "  print(\"\\nSkipping dataset loading: Invalid or non-existent path.\")\n",
        "  print(f\"Please ensure 'target_csv_path_drive' ({target_csv_path_drive}) is correct.\")\n",
        "\n",
        "# The 'df' variable now holds the loaded data (or None if loading failed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYZAaTw2viUN",
        "outputId": "3c00ab1d-5012-45db-fd7b-7fbcc90fe386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from: /content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\n",
            "\n",
            "Dataset loaded successfully.\n",
            "DataFrame shape (rows, columns): (157800, 63)\n",
            "\n",
            "First 5 rows of the DataFrame:\n",
            "  frame.time    ip.src_host ip.dst_host arp.dst.proto_ipv4  arp.opcode  \\\n",
            "0        6.0  192.168.0.152         0.0                0.0         0.0   \n",
            "1        6.0  192.168.0.101         0.0                0.0         0.0   \n",
            "2        6.0  192.168.0.152         0.0                0.0         0.0   \n",
            "3        6.0  192.168.0.101         0.0                0.0         0.0   \n",
            "4        6.0  192.168.0.152         0.0                0.0         0.0   \n",
            "\n",
            "   arp.hw.size arp.src.proto_ipv4  icmp.checksum  icmp.seq_le  \\\n",
            "0          0.0                0.0            0.0          0.0   \n",
            "1          0.0                0.0            0.0          0.0   \n",
            "2          0.0                0.0            0.0          0.0   \n",
            "3          0.0                0.0            0.0          0.0   \n",
            "4          0.0                0.0            0.0          0.0   \n",
            "\n",
            "   icmp.transmit_timestamp  ...  mqtt.proto_len mqtt.protoname  mqtt.topic  \\\n",
            "0                      0.0  ...             0.0            0.0         0.0   \n",
            "1                      0.0  ...             0.0            0.0         0.0   \n",
            "2                      0.0  ...             0.0            0.0         0.0   \n",
            "3                      0.0  ...             0.0            0.0         0.0   \n",
            "4                      0.0  ...             0.0            0.0         0.0   \n",
            "\n",
            "  mqtt.topic_len mqtt.ver mbtcp.len mbtcp.trans_id mbtcp.unit_id  \\\n",
            "0            0.0      0.0       0.0            0.0           0.0   \n",
            "1            0.0      0.0       0.0            0.0           0.0   \n",
            "2            0.0      0.0       0.0            0.0           0.0   \n",
            "3            0.0      0.0       0.0            0.0           0.0   \n",
            "4            0.0      0.0       0.0            0.0           0.0   \n",
            "\n",
            "   Attack_label  Attack_type  \n",
            "0             1         MITM  \n",
            "1             1         MITM  \n",
            "2             1         MITM  \n",
            "3             1         MITM  \n",
            "4             1         MITM  \n",
            "\n",
            "[5 rows x 63 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step IV: Initial Data Exploration\n",
        "\n",
        "# Ensure the DataFrame 'df' exists from Step III\n",
        "\n",
        "if df is not None:\n",
        "  print(\"--- IV.A: Displaying Basic Information ---\")\n",
        "  # Get concise summary: column names, non-null counts, data types, memory usage\n",
        "  df.info()\n",
        "\n",
        "  print(\"\\n\\n--- IV.B: Checking for Missing Values (Sum per Column) ---\")\n",
        "  # Calculate and display the total number of missing values for each column\n",
        "  missing_values = df.isnull().sum()\n",
        "  print(missing_values[missing_values > 0]) # Only display columns with missing values\n",
        "  if missing_values.sum() == 0:\n",
        "      print(\"No missing values found.\")\n",
        "\n",
        "  print(\"\\n\\n--- IV.C: Generating Descriptive Statistics (Numerical Columns) ---\")\n",
        "  # Generate statistics like count, mean, std, min, max, quartiles for numerical columns\n",
        "  # Using .describe(include='all') might show stats for object types too if needed\n",
        "  print(df.describe())\n",
        "\n",
        "  # Optional: If you suspect object/categorical columns need separate stats\n",
        "  # print(\"\\n\\n--- Descriptive Statistics (Object Columns) ---\")\n",
        "  # print(df.describe(include='object'))\n",
        "\n",
        "else:\n",
        "  print(\"Skipping Data Exploration: DataFrame 'df' is not loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQI7CTECvsmc",
        "outputId": "a2e26674-61d1-4cf3-b192-67a98393f402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- IV.A: Displaying Basic Information ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 157800 entries, 0 to 157799\n",
            "Data columns (total 63 columns):\n",
            " #   Column                     Non-Null Count   Dtype  \n",
            "---  ------                     --------------   -----  \n",
            " 0   frame.time                 157800 non-null  object \n",
            " 1   ip.src_host                157800 non-null  object \n",
            " 2   ip.dst_host                157800 non-null  object \n",
            " 3   arp.dst.proto_ipv4         157800 non-null  object \n",
            " 4   arp.opcode                 157800 non-null  float64\n",
            " 5   arp.hw.size                157800 non-null  float64\n",
            " 6   arp.src.proto_ipv4         157800 non-null  object \n",
            " 7   icmp.checksum              157800 non-null  float64\n",
            " 8   icmp.seq_le                157800 non-null  float64\n",
            " 9   icmp.transmit_timestamp    157800 non-null  float64\n",
            " 10  icmp.unused                157800 non-null  float64\n",
            " 11  http.file_data             157800 non-null  object \n",
            " 12  http.content_length        157800 non-null  float64\n",
            " 13  http.request.uri.query     157800 non-null  object \n",
            " 14  http.request.method        157800 non-null  object \n",
            " 15  http.referer               157800 non-null  object \n",
            " 16  http.request.full_uri      157800 non-null  object \n",
            " 17  http.request.version       157800 non-null  object \n",
            " 18  http.response              157800 non-null  float64\n",
            " 19  http.tls_port              157800 non-null  float64\n",
            " 20  tcp.ack                    157800 non-null  float64\n",
            " 21  tcp.ack_raw                157800 non-null  float64\n",
            " 22  tcp.checksum               157800 non-null  float64\n",
            " 23  tcp.connection.fin         157800 non-null  float64\n",
            " 24  tcp.connection.rst         157800 non-null  float64\n",
            " 25  tcp.connection.syn         157800 non-null  float64\n",
            " 26  tcp.connection.synack      157800 non-null  float64\n",
            " 27  tcp.dstport                157800 non-null  float64\n",
            " 28  tcp.flags                  157800 non-null  float64\n",
            " 29  tcp.flags.ack              157800 non-null  float64\n",
            " 30  tcp.len                    157800 non-null  float64\n",
            " 31  tcp.options                157800 non-null  object \n",
            " 32  tcp.payload                157800 non-null  object \n",
            " 33  tcp.seq                    157800 non-null  float64\n",
            " 34  tcp.srcport                157800 non-null  object \n",
            " 35  udp.port                   157800 non-null  float64\n",
            " 36  udp.stream                 157800 non-null  float64\n",
            " 37  udp.time_delta             157800 non-null  float64\n",
            " 38  dns.qry.name               157800 non-null  float64\n",
            " 39  dns.qry.name.len           157800 non-null  object \n",
            " 40  dns.qry.qu                 157800 non-null  float64\n",
            " 41  dns.qry.type               157800 non-null  float64\n",
            " 42  dns.retransmission         157800 non-null  float64\n",
            " 43  dns.retransmit_request     157800 non-null  float64\n",
            " 44  dns.retransmit_request_in  157800 non-null  float64\n",
            " 45  mqtt.conack.flags          157800 non-null  object \n",
            " 46  mqtt.conflag.cleansess     157800 non-null  float64\n",
            " 47  mqtt.conflags              157800 non-null  float64\n",
            " 48  mqtt.hdrflags              157800 non-null  float64\n",
            " 49  mqtt.len                   157800 non-null  float64\n",
            " 50  mqtt.msg_decoded_as        157800 non-null  float64\n",
            " 51  mqtt.msg                   157800 non-null  object \n",
            " 52  mqtt.msgtype               157800 non-null  float64\n",
            " 53  mqtt.proto_len             157800 non-null  float64\n",
            " 54  mqtt.protoname             157800 non-null  object \n",
            " 55  mqtt.topic                 157800 non-null  object \n",
            " 56  mqtt.topic_len             157800 non-null  float64\n",
            " 57  mqtt.ver                   157800 non-null  float64\n",
            " 58  mbtcp.len                  157800 non-null  float64\n",
            " 59  mbtcp.trans_id             157800 non-null  float64\n",
            " 60  mbtcp.unit_id              157800 non-null  float64\n",
            " 61  Attack_label               157800 non-null  int64  \n",
            " 62  Attack_type                157800 non-null  object \n",
            "dtypes: float64(42), int64(1), object(20)\n",
            "memory usage: 75.8+ MB\n",
            "\n",
            "\n",
            "--- IV.B: Checking for Missing Values (Sum per Column) ---\n",
            "Series([], dtype: int64)\n",
            "No missing values found.\n",
            "\n",
            "\n",
            "--- IV.C: Generating Descriptive Statistics (Numerical Columns) ---\n",
            "          arp.opcode    arp.hw.size  icmp.checksum    icmp.seq_le  \\\n",
            "count  157800.000000  157800.000000  157800.000000  157800.000000   \n",
            "mean        0.014195       0.059848    3047.291838    3239.979778   \n",
            "std         0.149783       0.596245   11144.328203   11406.072994   \n",
            "min         0.000000       0.000000       0.000000       0.000000   \n",
            "25%         0.000000       0.000000       0.000000       0.000000   \n",
            "50%         0.000000       0.000000       0.000000       0.000000   \n",
            "75%         0.000000       0.000000       0.000000       0.000000   \n",
            "max         2.000000       6.000000   65532.000000   65524.000000   \n",
            "\n",
            "       icmp.transmit_timestamp  icmp.unused  http.content_length  \\\n",
            "count             1.578000e+05     157800.0        157800.000000   \n",
            "mean              4.046816e+04          0.0            14.715520   \n",
            "std               1.764075e+06          0.0           229.659671   \n",
            "min               0.000000e+00          0.0             0.000000   \n",
            "25%               0.000000e+00          0.0             0.000000   \n",
            "50%               0.000000e+00          0.0             0.000000   \n",
            "75%               0.000000e+00          0.0             0.000000   \n",
            "max               7.728902e+07          0.0         83655.000000   \n",
            "\n",
            "       http.response  http.tls_port       tcp.ack  ...       mqtt.len  \\\n",
            "count  157800.000000       157800.0  1.578000e+05  ...  157800.000000   \n",
            "mean        0.045748            0.0  7.160039e+07  ...       0.419341   \n",
            "std         0.208938            0.0  3.101231e+08  ...       3.606594   \n",
            "min         0.000000            0.0  0.000000e+00  ...       0.000000   \n",
            "25%         0.000000            0.0  0.000000e+00  ...       0.000000   \n",
            "50%         0.000000            0.0  1.000000e+00  ...       0.000000   \n",
            "75%         0.000000            0.0  4.790000e+02  ...       0.000000   \n",
            "max         1.000000            0.0  2.147333e+09  ...      39.000000   \n",
            "\n",
            "       mqtt.msg_decoded_as   mqtt.msgtype  mqtt.proto_len  mqtt.topic_len  \\\n",
            "count             157800.0  157800.000000   157800.000000   157800.000000   \n",
            "mean                   0.0       0.161331        0.031686        0.189506   \n",
            "std                    0.0       1.293453        0.354598        2.124206   \n",
            "min                    0.0       0.000000        0.000000        0.000000   \n",
            "25%                    0.0       0.000000        0.000000        0.000000   \n",
            "50%                    0.0       0.000000        0.000000        0.000000   \n",
            "75%                    0.0       0.000000        0.000000        0.000000   \n",
            "max                    0.0      14.000000        4.000000       24.000000   \n",
            "\n",
            "            mqtt.ver  mbtcp.len  mbtcp.trans_id  mbtcp.unit_id   Attack_label  \n",
            "count  157800.000000   157800.0        157800.0       157800.0  157800.000000  \n",
            "mean        0.031686        0.0             0.0            0.0       0.846001  \n",
            "std         0.354598        0.0             0.0            0.0       0.360949  \n",
            "min         0.000000        0.0             0.0            0.0       0.000000  \n",
            "25%         0.000000        0.0             0.0            0.0       1.000000  \n",
            "50%         0.000000        0.0             0.0            0.0       1.000000  \n",
            "75%         0.000000        0.0             0.0            0.0       1.000000  \n",
            "max         4.000000        0.0             0.0            0.0       1.000000  \n",
            "\n",
            "[8 rows x 43 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step V.A: Handle Mixed Data Types - Convert Object Columns to Numeric\n",
        "\n",
        "# Ensure the DataFrame 'df' exists from Step III\n",
        "\n",
        "if df is not None:\n",
        "  print(\"--- V.A: Converting relevant 'object' columns to numeric ---\")\n",
        "\n",
        "  # Columns identified as 'object' that likely should be numeric based on PDF/info()\n",
        "  cols_to_convert = [\n",
        "      'frame.time',\n",
        "      'ip.src_host',\n",
        "      'ip.dst_host',\n",
        "      'arp.dst.proto_ipv4',\n",
        "      'arp.src.proto_ipv4',\n",
        "      'http.file_data', # Might contain non-numeric strings\n",
        "      'http.request.uri.query', # Might contain non-numeric strings\n",
        "      'http.request.method', # Treat as numeric for now, may revisit\n",
        "      'http.referer', # Might contain non-numeric strings\n",
        "      'http.request.full_uri', # Might contain non-numeric strings\n",
        "      'http.request.version', # E.g., '1.1' - needs conversion\n",
        "      'tcp.options', # Complex, likely non-numeric\n",
        "      'tcp.payload', # Hex or byte strings, likely non-numeric\n",
        "      'tcp.srcport', # Should be numeric port number\n",
        "      'dns.qry.name.len', # Should be numeric length\n",
        "      'mqtt.conack.flags', # Often hex strings\n",
        "      'mqtt.msg', # Message content, can be varied\n",
        "      'mqtt.protoname', # Protocol name string\n",
        "      'mqtt.topic' # Topic string\n",
        "      # Note: 'Attack_type' is object but is a label, handle later if needed\n",
        "  ]\n",
        "\n",
        "  # Keep track of NaNs introduced\n",
        "  nan_counts_before = df.isnull().sum()\n",
        "\n",
        "  for col in cols_to_convert:\n",
        "    if col in df.columns and df[col].dtype == 'object':\n",
        "      print(f\"Converting column: {col}\")\n",
        "      original_non_numeric = pd.to_numeric(df[col], errors='coerce').isnull().sum()\n",
        "      df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "      new_nan_count = df[col].isnull().sum()\n",
        "      introduced_nans = new_nan_count - nan_counts_before.get(col, 0)\n",
        "      if introduced_nans > 0:\n",
        "          print(f\"  -> Introduced {introduced_nans} NaN values by coercing non-numeric entries.\")\n",
        "      else:\n",
        "          print(f\"  -> Conversion successful without introducing new NaNs (or column was already numeric).\")\n",
        "    elif col not in df.columns:\n",
        "       print(f\"Skipping column {col}: Not found in DataFrame.\")\n",
        "    # else: Column exists but is not object, skip conversion\n",
        "\n",
        "\n",
        "  print(\"\\n--- Data types after conversion attempt: ---\")\n",
        "  df.info() # Display info again to see changes in Dtypes and non-null counts\n",
        "\n",
        "else:\n",
        "  print(\"Skipping Data Cleaning: DataFrame 'df' is not loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WzsmMNzv9hW",
        "outputId": "f6b6a53d-457a-4937-b20b-7c7589a6028a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- V.A: Converting relevant 'object' columns to numeric ---\n",
            "Converting column: frame.time\n",
            "  -> Introduced 156586 NaN values by coercing non-numeric entries.\n",
            "Converting column: ip.src_host\n",
            "  -> Introduced 149809 NaN values by coercing non-numeric entries.\n",
            "Converting column: ip.dst_host\n",
            "  -> Introduced 136372 NaN values by coercing non-numeric entries.\n",
            "Converting column: arp.dst.proto_ipv4\n",
            "  -> Introduced 2976 NaN values by coercing non-numeric entries.\n",
            "Converting column: arp.src.proto_ipv4\n",
            "  -> Introduced 1574 NaN values by coercing non-numeric entries.\n",
            "Converting column: http.file_data\n",
            "  -> Introduced 9471 NaN values by coercing non-numeric entries.\n",
            "Converting column: http.request.uri.query\n",
            "  -> Introduced 2751 NaN values by coercing non-numeric entries.\n",
            "Converting column: http.request.method\n",
            "  -> Introduced 7196 NaN values by coercing non-numeric entries.\n",
            "Converting column: http.referer\n",
            "  -> Introduced 290 NaN values by coercing non-numeric entries.\n",
            "Converting column: http.request.full_uri\n",
            "  -> Introduced 7174 NaN values by coercing non-numeric entries.\n",
            "Converting column: http.request.version\n",
            "  -> Introduced 7196 NaN values by coercing non-numeric entries.\n",
            "Converting column: tcp.options\n",
            "  -> Introduced 79280 NaN values by coercing non-numeric entries.\n",
            "Converting column: tcp.payload\n",
            "  -> Introduced 34608 NaN values by coercing non-numeric entries.\n",
            "Converting column: tcp.srcport\n",
            "  -> Introduced 367 NaN values by coercing non-numeric entries.\n",
            "Converting column: dns.qry.name.len\n",
            "  -> Introduced 29 NaN values by coercing non-numeric entries.\n",
            "Converting column: mqtt.conack.flags\n",
            "  -> Introduced 1289 NaN values by coercing non-numeric entries.\n",
            "Converting column: mqtt.msg\n",
            "  -> Introduced 1246 NaN values by coercing non-numeric entries.\n",
            "Converting column: mqtt.protoname\n",
            "  -> Introduced 1250 NaN values by coercing non-numeric entries.\n",
            "Converting column: mqtt.topic\n",
            "  -> Introduced 1246 NaN values by coercing non-numeric entries.\n",
            "\n",
            "--- Data types after conversion attempt: ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 157800 entries, 0 to 157799\n",
            "Data columns (total 63 columns):\n",
            " #   Column                     Non-Null Count   Dtype  \n",
            "---  ------                     --------------   -----  \n",
            " 0   frame.time                 1214 non-null    float64\n",
            " 1   ip.src_host                7991 non-null    float64\n",
            " 2   ip.dst_host                21428 non-null   float64\n",
            " 3   arp.dst.proto_ipv4         154824 non-null  float64\n",
            " 4   arp.opcode                 157800 non-null  float64\n",
            " 5   arp.hw.size                157800 non-null  float64\n",
            " 6   arp.src.proto_ipv4         156226 non-null  float64\n",
            " 7   icmp.checksum              157800 non-null  float64\n",
            " 8   icmp.seq_le                157800 non-null  float64\n",
            " 9   icmp.transmit_timestamp    157800 non-null  float64\n",
            " 10  icmp.unused                157800 non-null  float64\n",
            " 11  http.file_data             148329 non-null  float64\n",
            " 12  http.content_length        157800 non-null  float64\n",
            " 13  http.request.uri.query     155049 non-null  float64\n",
            " 14  http.request.method        150604 non-null  float64\n",
            " 15  http.referer               157510 non-null  float64\n",
            " 16  http.request.full_uri      150626 non-null  float64\n",
            " 17  http.request.version       150604 non-null  float64\n",
            " 18  http.response              157800 non-null  float64\n",
            " 19  http.tls_port              157800 non-null  float64\n",
            " 20  tcp.ack                    157800 non-null  float64\n",
            " 21  tcp.ack_raw                157800 non-null  float64\n",
            " 22  tcp.checksum               157800 non-null  float64\n",
            " 23  tcp.connection.fin         157800 non-null  float64\n",
            " 24  tcp.connection.rst         157800 non-null  float64\n",
            " 25  tcp.connection.syn         157800 non-null  float64\n",
            " 26  tcp.connection.synack      157800 non-null  float64\n",
            " 27  tcp.dstport                157800 non-null  float64\n",
            " 28  tcp.flags                  157800 non-null  float64\n",
            " 29  tcp.flags.ack              157800 non-null  float64\n",
            " 30  tcp.len                    157800 non-null  float64\n",
            " 31  tcp.options                78520 non-null   float64\n",
            " 32  tcp.payload                123192 non-null  float64\n",
            " 33  tcp.seq                    157800 non-null  float64\n",
            " 34  tcp.srcport                157433 non-null  float64\n",
            " 35  udp.port                   157800 non-null  float64\n",
            " 36  udp.stream                 157800 non-null  float64\n",
            " 37  udp.time_delta             157800 non-null  float64\n",
            " 38  dns.qry.name               157800 non-null  float64\n",
            " 39  dns.qry.name.len           157771 non-null  float64\n",
            " 40  dns.qry.qu                 157800 non-null  float64\n",
            " 41  dns.qry.type               157800 non-null  float64\n",
            " 42  dns.retransmission         157800 non-null  float64\n",
            " 43  dns.retransmit_request     157800 non-null  float64\n",
            " 44  dns.retransmit_request_in  157800 non-null  float64\n",
            " 45  mqtt.conack.flags          156511 non-null  float64\n",
            " 46  mqtt.conflag.cleansess     157800 non-null  float64\n",
            " 47  mqtt.conflags              157800 non-null  float64\n",
            " 48  mqtt.hdrflags              157800 non-null  float64\n",
            " 49  mqtt.len                   157800 non-null  float64\n",
            " 50  mqtt.msg_decoded_as        157800 non-null  float64\n",
            " 51  mqtt.msg                   156554 non-null  float64\n",
            " 52  mqtt.msgtype               157800 non-null  float64\n",
            " 53  mqtt.proto_len             157800 non-null  float64\n",
            " 54  mqtt.protoname             156550 non-null  float64\n",
            " 55  mqtt.topic                 156554 non-null  float64\n",
            " 56  mqtt.topic_len             157800 non-null  float64\n",
            " 57  mqtt.ver                   157800 non-null  float64\n",
            " 58  mbtcp.len                  157800 non-null  float64\n",
            " 59  mbtcp.trans_id             157800 non-null  float64\n",
            " 60  mbtcp.unit_id              157800 non-null  float64\n",
            " 61  Attack_label               157800 non-null  int64  \n",
            " 62  Attack_type                157800 non-null  object \n",
            "dtypes: float64(61), int64(1), object(1)\n",
            "memory usage: 75.8+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step V.B: Handle Infinity Values\n",
        "\n",
        "import numpy as np # Import numpy for inf representation\n",
        "\n",
        "# Ensure the DataFrame 'df' exists and has been processed by Step V.A\n",
        "\n",
        "if df is not None:\n",
        "  print(\"--- V.B: Checking for and replacing infinite values ---\")\n",
        "\n",
        "  # Count infinite values before replacement\n",
        "  infinite_values_count = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()\n",
        "  print(f\"Found {infinite_values_count} infinite values (inf or -inf) before replacement.\")\n",
        "\n",
        "  if infinite_values_count > 0:\n",
        "    # Replace infinite values with NaN\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    # Verify replacement (optional)\n",
        "    infinite_values_count_after = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()\n",
        "    print(f\"Replaced infinite values with NaN. Found {infinite_values_count_after} infinite values after replacement.\")\n",
        "  else:\n",
        "    print(\"No infinite values found to replace.\")\n",
        "\n",
        "  # Display NaN counts again to see if any new NaNs were added from infinities\n",
        "  print(\"\\n--- Current NaN counts per column (including any from replaced infinities): ---\")\n",
        "  nan_counts = df.isnull().sum()\n",
        "  print(nan_counts[nan_counts > 0])\n",
        "  if nan_counts.sum() == 0:\n",
        "      print(\"No missing (NaN) values found.\")\n",
        "\n",
        "else:\n",
        "  print(\"Skipping Infinity Handling: DataFrame 'df' is not loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnFk1BuKwG37",
        "outputId": "e5a3402d-527b-4180-a524-994365c7f3ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- V.B: Checking for and replacing infinite values ---\n",
            "Found 0 infinite values (inf or -inf) before replacement.\n",
            "No infinite values found to replace.\n",
            "\n",
            "--- Current NaN counts per column (including any from replaced infinities): ---\n",
            "Series([], dtype: int64)\n",
            "No missing (NaN) values found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step V.C: Handle Missing Values (NaN Imputation using Mean)\n",
        "\n",
        "# Ensure the DataFrame 'df' exists and has been processed by Steps V.A & V.B\n",
        "\n",
        "if df is not None:\n",
        "  print(\"--- V.C: Imputing NaN values using the mean ---\")\n",
        "\n",
        "  # Identify numeric columns with NaN values\n",
        "  cols_with_nan = df.isnull().sum()\n",
        "  cols_to_impute = cols_with_nan[cols_with_nan > 0].index.tolist()\n",
        "\n",
        "  # Exclude non-numeric columns explicitly if necessary (Attack_type is object, so select_dtypes handles it)\n",
        "  # numeric_cols_to_impute = df[cols_to_impute].select_dtypes(include=np.number).columns\n",
        "\n",
        "  print(\"Columns identified for mean imputation:\")\n",
        "  imputed_cols_list = []\n",
        "\n",
        "  for col in cols_to_impute:\n",
        "      # Check if column is numeric before imputing\n",
        "      if pd.api.types.is_numeric_dtype(df[col]):\n",
        "          col_mean = df[col].mean()\n",
        "          df[col].fillna(col_mean, inplace=True)\n",
        "          imputed_cols_list.append(col)\n",
        "          # print(f\"  - Imputed NaNs in '{col}' with mean: {col_mean:.4f}\") # Optional: print mean used\n",
        "      else:\n",
        "          print(f\"  - Skipped non-numeric column: '{col}'\")\n",
        "\n",
        "  if imputed_cols_list:\n",
        "      print(f\"\\nImputed NaNs in the following {len(imputed_cols_list)} numeric columns using their respective means:\")\n",
        "      print(imputed_cols_list)\n",
        "  else:\n",
        "      print(\"\\nNo numeric columns required imputation.\")\n",
        "\n",
        "\n",
        "  # Verify that NaNs have been handled in numeric columns\n",
        "  print(\"\\n--- NaN counts per column after imputation: ---\")\n",
        "  nan_counts_after = df.isnull().sum()\n",
        "  print(nan_counts_after[nan_counts_after > 0])\n",
        "  if nan_counts_after.sum() == 0:\n",
        "      print(\"No missing (NaN) values remaining in the dataset.\")\n",
        "  elif df[nan_counts_after > 0].select_dtypes(include=np.number).shape[1] == 0 :\n",
        "       print(\"No missing (NaN) values remaining in numeric columns.\")\n",
        "       print(\"Remaining NaNs are likely in non-numeric columns (e.g., object types).\")\n",
        "\n",
        "  print(\"\\n--- Data types after imputation: ---\")\n",
        "  df.info() # Check if any dtypes changed (shouldn't have)\n",
        "\n",
        "else:\n",
        "  print(\"Skipping NaN Imputation: DataFrame 'df' is not loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXnGc0p0wQVY",
        "outputId": "cb3d7ca3-43ce-471f-bf9b-fe6bb8c7f697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- V.C: Imputing NaN values using the mean ---\n",
            "Columns identified for mean imputation:\n",
            "\n",
            "No numeric columns required imputation.\n",
            "\n",
            "--- NaN counts per column after imputation: ---\n",
            "Series([], dtype: int64)\n",
            "No missing (NaN) values remaining in the dataset.\n",
            "\n",
            "--- Data types after imputation: ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 157800 entries, 0 to 157799\n",
            "Data columns (total 63 columns):\n",
            " #   Column                     Non-Null Count   Dtype  \n",
            "---  ------                     --------------   -----  \n",
            " 0   frame.time                 157800 non-null  object \n",
            " 1   ip.src_host                157800 non-null  object \n",
            " 2   ip.dst_host                157800 non-null  object \n",
            " 3   arp.dst.proto_ipv4         157800 non-null  object \n",
            " 4   arp.opcode                 157800 non-null  float64\n",
            " 5   arp.hw.size                157800 non-null  float64\n",
            " 6   arp.src.proto_ipv4         157800 non-null  object \n",
            " 7   icmp.checksum              157800 non-null  float64\n",
            " 8   icmp.seq_le                157800 non-null  float64\n",
            " 9   icmp.transmit_timestamp    157800 non-null  float64\n",
            " 10  icmp.unused                157800 non-null  float64\n",
            " 11  http.file_data             157800 non-null  object \n",
            " 12  http.content_length        157800 non-null  float64\n",
            " 13  http.request.uri.query     157800 non-null  object \n",
            " 14  http.request.method        157800 non-null  object \n",
            " 15  http.referer               157800 non-null  object \n",
            " 16  http.request.full_uri      157800 non-null  object \n",
            " 17  http.request.version       157800 non-null  object \n",
            " 18  http.response              157800 non-null  float64\n",
            " 19  http.tls_port              157800 non-null  float64\n",
            " 20  tcp.ack                    157800 non-null  float64\n",
            " 21  tcp.ack_raw                157800 non-null  float64\n",
            " 22  tcp.checksum               157800 non-null  float64\n",
            " 23  tcp.connection.fin         157800 non-null  float64\n",
            " 24  tcp.connection.rst         157800 non-null  float64\n",
            " 25  tcp.connection.syn         157800 non-null  float64\n",
            " 26  tcp.connection.synack      157800 non-null  float64\n",
            " 27  tcp.dstport                157800 non-null  float64\n",
            " 28  tcp.flags                  157800 non-null  float64\n",
            " 29  tcp.flags.ack              157800 non-null  float64\n",
            " 30  tcp.len                    157800 non-null  float64\n",
            " 31  tcp.options                157800 non-null  object \n",
            " 32  tcp.payload                157800 non-null  object \n",
            " 33  tcp.seq                    157800 non-null  float64\n",
            " 34  tcp.srcport                157800 non-null  object \n",
            " 35  udp.port                   157800 non-null  float64\n",
            " 36  udp.stream                 157800 non-null  float64\n",
            " 37  udp.time_delta             157800 non-null  float64\n",
            " 38  dns.qry.name               157800 non-null  float64\n",
            " 39  dns.qry.name.len           157800 non-null  object \n",
            " 40  dns.qry.qu                 157800 non-null  float64\n",
            " 41  dns.qry.type               157800 non-null  float64\n",
            " 42  dns.retransmission         157800 non-null  float64\n",
            " 43  dns.retransmit_request     157800 non-null  float64\n",
            " 44  dns.retransmit_request_in  157800 non-null  float64\n",
            " 45  mqtt.conack.flags          157800 non-null  object \n",
            " 46  mqtt.conflag.cleansess     157800 non-null  float64\n",
            " 47  mqtt.conflags              157800 non-null  float64\n",
            " 48  mqtt.hdrflags              157800 non-null  float64\n",
            " 49  mqtt.len                   157800 non-null  float64\n",
            " 50  mqtt.msg_decoded_as        157800 non-null  float64\n",
            " 51  mqtt.msg                   157800 non-null  object \n",
            " 52  mqtt.msgtype               157800 non-null  float64\n",
            " 53  mqtt.proto_len             157800 non-null  float64\n",
            " 54  mqtt.protoname             157800 non-null  object \n",
            " 55  mqtt.topic                 157800 non-null  object \n",
            " 56  mqtt.topic_len             157800 non-null  float64\n",
            " 57  mqtt.ver                   157800 non-null  float64\n",
            " 58  mbtcp.len                  157800 non-null  float64\n",
            " 59  mbtcp.trans_id             157800 non-null  float64\n",
            " 60  mbtcp.unit_id              157800 non-null  float64\n",
            " 61  Attack_label               157800 non-null  int64  \n",
            " 62  Attack_type                157800 non-null  object \n",
            "dtypes: float64(42), int64(1), object(20)\n",
            "memory usage: 75.8+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-apply COMPLETE Step V: Data Cleaning (on DataFrame 'df')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Ensure DataFrame 'df' is loaded and available ---\n",
        "if 'df' in locals() and df is not None:\n",
        "    print(\"--- Applying Step V.A: Converting relevant 'object' columns to numeric ---\")\n",
        "    # Columns identified as 'object' that likely should be numeric\n",
        "    cols_to_convert = [\n",
        "        'frame.time', 'ip.src_host', 'ip.dst_host', 'arp.dst.proto_ipv4',\n",
        "        'arp.src.proto_ipv4', 'http.file_data', 'http.request.uri.query',\n",
        "        'http.request.method', 'http.referer', 'http.request.full_uri',\n",
        "        'http.request.version', 'tcp.options', 'tcp.payload', 'tcp.srcport',\n",
        "        'dns.qry.name.len', 'mqtt.conack.flags', 'mqtt.msg',\n",
        "        'mqtt.protoname', 'mqtt.topic'\n",
        "    ]\n",
        "    nan_counts_before = df.isnull().sum()\n",
        "    for col in cols_to_convert:\n",
        "        if col in df.columns and df[col].dtype == 'object':\n",
        "            print(f\"Converting column: {col}\")\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            new_nan_count = df[col].isnull().sum()\n",
        "            introduced_nans = new_nan_count - nan_counts_before.get(col, 0)\n",
        "            if introduced_nans > 0:\n",
        "                print(f\"  -> Introduced {introduced_nans} NaN values.\")\n",
        "            else:\n",
        "                print(f\"  -> Conversion complete.\")\n",
        "        elif col not in df.columns:\n",
        "            print(f\"Skipping column {col}: Not found.\")\n",
        "\n",
        "    print(\"\\n--- Applying Step V.B: Checking for and replacing infinite values ---\")\n",
        "    infinite_values_count = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()\n",
        "    if infinite_values_count > 0:\n",
        "        print(f\"Found {infinite_values_count} infinite values. Replacing with NaN.\")\n",
        "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    else:\n",
        "        print(\"No infinite values found.\")\n",
        "\n",
        "    print(\"\\n--- Applying Step V.C: Imputing NaN values using the mean ---\")\n",
        "    cols_with_nan = df.isnull().sum()\n",
        "    cols_to_impute = cols_with_nan[cols_with_nan > 0].index.tolist()\n",
        "    imputed_cols_list = []\n",
        "    for col in cols_to_impute:\n",
        "        if pd.api.types.is_numeric_dtype(df[col]):\n",
        "            col_mean = df[col].mean()\n",
        "            df[col].fillna(col_mean, inplace=True)\n",
        "            imputed_cols_list.append(col)\n",
        "        else:\n",
        "             print(f\"  - Skipped non-numeric column during imputation: '{col}'\") # Should only be Attack_type if present\n",
        "    if imputed_cols_list:\n",
        "         print(f\"Imputed NaNs in {len(imputed_cols_list)} numeric columns using mean.\")\n",
        "    else:\n",
        "         print(\"No numeric columns required imputation.\")\n",
        "\n",
        "    print(\"\\n--- Verification after re-applying Step V ---\")\n",
        "    print(\"Checking final NaN counts in 'df':\")\n",
        "    final_nans = df.isnull().sum().sum()\n",
        "    if final_nans == 0:\n",
        "        print(\"  Success: No NaN values remaining in 'df'.\")\n",
        "    else:\n",
        "        print(f\"  Warning: {final_nans} NaN values still remain in 'df'.\")\n",
        "        print(df.isnull().sum()[df.isnull().sum() > 0])\n",
        "\n",
        "    print(\"\\nChecking final data types in 'df':\")\n",
        "    df.info()\n",
        "\n",
        "else:\n",
        "    print(\"Error: DataFrame 'df' not found. Please load the data using Step III code first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBddzpmiwaDV",
        "outputId": "99e7dae2-bfdc-420b-c60e-42399d6eb872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Applying Step V.A: Converting relevant 'object' columns to numeric ---\n",
            "Converting column: frame.time\n",
            "  -> Introduced 156586 NaN values.\n",
            "Converting column: ip.src_host\n",
            "  -> Introduced 149809 NaN values.\n",
            "Converting column: ip.dst_host\n",
            "  -> Introduced 136372 NaN values.\n",
            "Converting column: arp.dst.proto_ipv4\n",
            "  -> Introduced 2976 NaN values.\n",
            "Converting column: arp.src.proto_ipv4\n",
            "  -> Introduced 1574 NaN values.\n",
            "Converting column: http.file_data\n",
            "  -> Introduced 9471 NaN values.\n",
            "Converting column: http.request.uri.query\n",
            "  -> Introduced 2751 NaN values.\n",
            "Converting column: http.request.method\n",
            "  -> Introduced 7196 NaN values.\n",
            "Converting column: http.referer\n",
            "  -> Introduced 290 NaN values.\n",
            "Converting column: http.request.full_uri\n",
            "  -> Introduced 7174 NaN values.\n",
            "Converting column: http.request.version\n",
            "  -> Introduced 7196 NaN values.\n",
            "Converting column: tcp.options\n",
            "  -> Introduced 79280 NaN values.\n",
            "Converting column: tcp.payload\n",
            "  -> Introduced 34608 NaN values.\n",
            "Converting column: tcp.srcport\n",
            "  -> Introduced 367 NaN values.\n",
            "Converting column: dns.qry.name.len\n",
            "  -> Introduced 29 NaN values.\n",
            "Converting column: mqtt.conack.flags\n",
            "  -> Introduced 1289 NaN values.\n",
            "Converting column: mqtt.msg\n",
            "  -> Introduced 1246 NaN values.\n",
            "Converting column: mqtt.protoname\n",
            "  -> Introduced 1250 NaN values.\n",
            "Converting column: mqtt.topic\n",
            "  -> Introduced 1246 NaN values.\n",
            "\n",
            "--- Applying Step V.B: Checking for and replacing infinite values ---\n",
            "No infinite values found.\n",
            "\n",
            "--- Applying Step V.C: Imputing NaN values using the mean ---\n",
            "Imputed NaNs in 19 numeric columns using mean.\n",
            "\n",
            "--- Verification after re-applying Step V ---\n",
            "Checking final NaN counts in 'df':\n",
            "  Success: No NaN values remaining in 'df'.\n",
            "\n",
            "Checking final data types in 'df':\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 157800 entries, 0 to 157799\n",
            "Data columns (total 63 columns):\n",
            " #   Column                     Non-Null Count   Dtype  \n",
            "---  ------                     --------------   -----  \n",
            " 0   frame.time                 157800 non-null  float64\n",
            " 1   ip.src_host                157800 non-null  float64\n",
            " 2   ip.dst_host                157800 non-null  float64\n",
            " 3   arp.dst.proto_ipv4         157800 non-null  float64\n",
            " 4   arp.opcode                 157800 non-null  float64\n",
            " 5   arp.hw.size                157800 non-null  float64\n",
            " 6   arp.src.proto_ipv4         157800 non-null  float64\n",
            " 7   icmp.checksum              157800 non-null  float64\n",
            " 8   icmp.seq_le                157800 non-null  float64\n",
            " 9   icmp.transmit_timestamp    157800 non-null  float64\n",
            " 10  icmp.unused                157800 non-null  float64\n",
            " 11  http.file_data             157800 non-null  float64\n",
            " 12  http.content_length        157800 non-null  float64\n",
            " 13  http.request.uri.query     157800 non-null  float64\n",
            " 14  http.request.method        157800 non-null  float64\n",
            " 15  http.referer               157800 non-null  float64\n",
            " 16  http.request.full_uri      157800 non-null  float64\n",
            " 17  http.request.version       157800 non-null  float64\n",
            " 18  http.response              157800 non-null  float64\n",
            " 19  http.tls_port              157800 non-null  float64\n",
            " 20  tcp.ack                    157800 non-null  float64\n",
            " 21  tcp.ack_raw                157800 non-null  float64\n",
            " 22  tcp.checksum               157800 non-null  float64\n",
            " 23  tcp.connection.fin         157800 non-null  float64\n",
            " 24  tcp.connection.rst         157800 non-null  float64\n",
            " 25  tcp.connection.syn         157800 non-null  float64\n",
            " 26  tcp.connection.synack      157800 non-null  float64\n",
            " 27  tcp.dstport                157800 non-null  float64\n",
            " 28  tcp.flags                  157800 non-null  float64\n",
            " 29  tcp.flags.ack              157800 non-null  float64\n",
            " 30  tcp.len                    157800 non-null  float64\n",
            " 31  tcp.options                157800 non-null  float64\n",
            " 32  tcp.payload                157800 non-null  float64\n",
            " 33  tcp.seq                    157800 non-null  float64\n",
            " 34  tcp.srcport                157800 non-null  float64\n",
            " 35  udp.port                   157800 non-null  float64\n",
            " 36  udp.stream                 157800 non-null  float64\n",
            " 37  udp.time_delta             157800 non-null  float64\n",
            " 38  dns.qry.name               157800 non-null  float64\n",
            " 39  dns.qry.name.len           157800 non-null  float64\n",
            " 40  dns.qry.qu                 157800 non-null  float64\n",
            " 41  dns.qry.type               157800 non-null  float64\n",
            " 42  dns.retransmission         157800 non-null  float64\n",
            " 43  dns.retransmit_request     157800 non-null  float64\n",
            " 44  dns.retransmit_request_in  157800 non-null  float64\n",
            " 45  mqtt.conack.flags          157800 non-null  float64\n",
            " 46  mqtt.conflag.cleansess     157800 non-null  float64\n",
            " 47  mqtt.conflags              157800 non-null  float64\n",
            " 48  mqtt.hdrflags              157800 non-null  float64\n",
            " 49  mqtt.len                   157800 non-null  float64\n",
            " 50  mqtt.msg_decoded_as        157800 non-null  float64\n",
            " 51  mqtt.msg                   157800 non-null  float64\n",
            " 52  mqtt.msgtype               157800 non-null  float64\n",
            " 53  mqtt.proto_len             157800 non-null  float64\n",
            " 54  mqtt.protoname             157800 non-null  float64\n",
            " 55  mqtt.topic                 157800 non-null  float64\n",
            " 56  mqtt.topic_len             157800 non-null  float64\n",
            " 57  mqtt.ver                   157800 non-null  float64\n",
            " 58  mbtcp.len                  157800 non-null  float64\n",
            " 59  mbtcp.trans_id             157800 non-null  float64\n",
            " 60  mbtcp.unit_id              157800 non-null  float64\n",
            " 61  Attack_label               157800 non-null  int64  \n",
            " 62  Attack_type                157800 non-null  object \n",
            "dtypes: float64(61), int64(1), object(1)\n",
            "memory usage: 75.8+ MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-8cb901c530f6>:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(col_mean, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step VI: Separate Features (X) and Target Variables (y)\n",
        "\n",
        "# Ensure the DataFrame 'df' exists and is cleaned\n",
        "\n",
        "if df is not None:\n",
        "    print(\"--- VI: Separating Features (X) and Target Variables (y) ---\")\n",
        "\n",
        "    try:\n",
        "        # Define target column names\n",
        "        target_label_col = 'Attack_label' # Binary label\n",
        "        target_type_col = 'Attack_type'  # Categorical type\n",
        "\n",
        "        # Create the features DataFrame (X) by dropping the target columns\n",
        "        X = df.drop(columns=[target_label_col, target_type_col])\n",
        "\n",
        "        # Create the target Series (y)\n",
        "        y_label = df[target_label_col] # Binary target\n",
        "        y_type = df[target_type_col]   # Categorical target\n",
        "\n",
        "        print(\"Features (X) and Target variables (y_label, y_type) created successfully.\")\n",
        "        print(f\"\\nShape of Features (X): {X.shape}\")\n",
        "        print(f\"Shape of Target (y_label): {y_label.shape}\")\n",
        "        print(f\"Shape of Target (y_type): {y_type.shape}\")\n",
        "\n",
        "        print(\"\\nFirst 5 rows of Features (X):\")\n",
        "        print(X.head())\n",
        "\n",
        "        print(\"\\nFirst 5 rows of Target (y_label):\")\n",
        "        print(y_label.head())\n",
        "\n",
        "        print(\"\\nFirst 5 rows of Target (y_type):\")\n",
        "        print(y_type.head())\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"Error separating features and target: Column not found - {e}\")\n",
        "        print(\"Please ensure 'Attack_label' and 'Attack_type' columns exist in the DataFrame.\")\n",
        "        X = None\n",
        "        y_label = None\n",
        "        y_type = None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during feature/target separation: {e}\")\n",
        "        X = None\n",
        "        y_label = None\n",
        "        y_type = None\n",
        "else:\n",
        "    print(\"Skipping Feature/Target Separation: DataFrame 'df' is not loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7yiV1IMwky_",
        "outputId": "e194f0e7-3561-44f7-dfb6-0359043c05c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- VI: Separating Features (X) and Target Variables (y) ---\n",
            "Features (X) and Target variables (y_label, y_type) created successfully.\n",
            "\n",
            "Shape of Features (X): (157800, 61)\n",
            "Shape of Target (y_label): (157800,)\n",
            "Shape of Target (y_type): (157800,)\n",
            "\n",
            "First 5 rows of Features (X):\n",
            "   frame.time  ip.src_host  ip.dst_host  arp.dst.proto_ipv4  arp.opcode  \\\n",
            "0         6.0          0.0          0.0                 0.0         0.0   \n",
            "1         6.0          0.0          0.0                 0.0         0.0   \n",
            "2         6.0          0.0          0.0                 0.0         0.0   \n",
            "3         6.0          0.0          0.0                 0.0         0.0   \n",
            "4         6.0          0.0          0.0                 0.0         0.0   \n",
            "\n",
            "   arp.hw.size  arp.src.proto_ipv4  icmp.checksum  icmp.seq_le  \\\n",
            "0          0.0                 0.0            0.0          0.0   \n",
            "1          0.0                 0.0            0.0          0.0   \n",
            "2          0.0                 0.0            0.0          0.0   \n",
            "3          0.0                 0.0            0.0          0.0   \n",
            "4          0.0                 0.0            0.0          0.0   \n",
            "\n",
            "   icmp.transmit_timestamp  ...  mqtt.msg  mqtt.msgtype  mqtt.proto_len  \\\n",
            "0                      0.0  ...       0.0           0.0             0.0   \n",
            "1                      0.0  ...       0.0           0.0             0.0   \n",
            "2                      0.0  ...       0.0           0.0             0.0   \n",
            "3                      0.0  ...       0.0           0.0             0.0   \n",
            "4                      0.0  ...       0.0           0.0             0.0   \n",
            "\n",
            "   mqtt.protoname  mqtt.topic  mqtt.topic_len  mqtt.ver  mbtcp.len  \\\n",
            "0             0.0         0.0             0.0       0.0        0.0   \n",
            "1             0.0         0.0             0.0       0.0        0.0   \n",
            "2             0.0         0.0             0.0       0.0        0.0   \n",
            "3             0.0         0.0             0.0       0.0        0.0   \n",
            "4             0.0         0.0             0.0       0.0        0.0   \n",
            "\n",
            "   mbtcp.trans_id  mbtcp.unit_id  \n",
            "0             0.0            0.0  \n",
            "1             0.0            0.0  \n",
            "2             0.0            0.0  \n",
            "3             0.0            0.0  \n",
            "4             0.0            0.0  \n",
            "\n",
            "[5 rows x 61 columns]\n",
            "\n",
            "First 5 rows of Target (y_label):\n",
            "0    1\n",
            "1    1\n",
            "2    1\n",
            "3    1\n",
            "4    1\n",
            "Name: Attack_label, dtype: int64\n",
            "\n",
            "First 5 rows of Target (y_type):\n",
            "0    MITM\n",
            "1    MITM\n",
            "2    MITM\n",
            "3    MITM\n",
            "4    MITM\n",
            "Name: Attack_type, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step VII: Encode Categorical Target Variable (y_type)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np # Import numpy for mapping display\n",
        "\n",
        "# Ensure y_type exists from Step VI\n",
        "\n",
        "if 'y_type' in locals() and y_type is not None:\n",
        "    print(\"--- VII: Encoding Categorical Target Variable (y_type) using LabelEncoder ---\")\n",
        "\n",
        "    try:\n",
        "        # Initialize the LabelEncoder\n",
        "        label_encoder = LabelEncoder()\n",
        "\n",
        "        # Fit the encoder on the target variable and transform it\n",
        "        y_type_encoded = label_encoder.fit_transform(y_type)\n",
        "\n",
        "        print(\"Categorical target 'y_type' encoded successfully.\")\n",
        "\n",
        "        # Display the mapping from original labels to encoded numbers\n",
        "        print(\"\\nMapping (Original Label -> Encoded Number):\")\n",
        "        # Use numpy unique on original y_type and corresponding encoded values\n",
        "        original_labels = label_encoder.classes_\n",
        "        encoded_labels = label_encoder.transform(original_labels)\n",
        "        for label, encoded in zip(original_labels, encoded_labels):\n",
        "             print(f\"  '{label}' -> {encoded}\")\n",
        "\n",
        "\n",
        "        print(\"\\nFirst 10 original 'y_type' values:\")\n",
        "        print(y_type.head(10))\n",
        "\n",
        "        print(\"\\nFirst 10 encoded 'y_type_encoded' values:\")\n",
        "        print(y_type_encoded[:10])\n",
        "\n",
        "        print(f\"\\nShape of encoded target (y_type_encoded): {y_type_encoded.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Label Encoding: {e}\")\n",
        "        y_type_encoded = None\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Label Encoding: Target variable 'y_type' is not defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNMTcUT4wptV",
        "outputId": "53a608e5-7753-45d0-c578-dcee769a76aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- VII: Encoding Categorical Target Variable (y_type) using LabelEncoder ---\n",
            "Categorical target 'y_type' encoded successfully.\n",
            "\n",
            "Mapping (Original Label -> Encoded Number):\n",
            "  'Backdoor' -> 0\n",
            "  'DDoS_HTTP' -> 1\n",
            "  'DDoS_ICMP' -> 2\n",
            "  'DDoS_TCP' -> 3\n",
            "  'DDoS_UDP' -> 4\n",
            "  'Fingerprinting' -> 5\n",
            "  'MITM' -> 6\n",
            "  'Normal' -> 7\n",
            "  'Password' -> 8\n",
            "  'Port_Scanning' -> 9\n",
            "  'Ransomware' -> 10\n",
            "  'SQL_injection' -> 11\n",
            "  'Uploading' -> 12\n",
            "  'Vulnerability_scanner' -> 13\n",
            "  'XSS' -> 14\n",
            "\n",
            "First 10 original 'y_type' values:\n",
            "0    MITM\n",
            "1    MITM\n",
            "2    MITM\n",
            "3    MITM\n",
            "4    MITM\n",
            "5    MITM\n",
            "6    MITM\n",
            "7    MITM\n",
            "8    MITM\n",
            "9    MITM\n",
            "Name: Attack_type, dtype: object\n",
            "\n",
            "First 10 encoded 'y_type_encoded' values:\n",
            "[6 6 6 6 6 6 6 6 6 6]\n",
            "\n",
            "Shape of encoded target (y_type_encoded): (157800,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step VIII: Split Data into Training and Testing Sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Ensure X, y_label, and y_type_encoded exist from previous steps\n",
        "\n",
        "if 'X' in locals() and X is not None and \\\n",
        "   'y_label' in locals() and y_label is not None and \\\n",
        "   'y_type_encoded' in locals() and y_type_encoded is not None:\n",
        "\n",
        "    print(\"--- VIII: Splitting data into Training (80%) and Testing (20%) sets ---\")\n",
        "\n",
        "    try:\n",
        "        # Define test size (e.g., 0.20 for 20%)\n",
        "        test_set_size = 0.20\n",
        "        # Use a fixed random_state for reproducibility\n",
        "        random_seed = 42\n",
        "\n",
        "        # Split the data, stratifying on the binary label (y_label)\n",
        "        X_train, X_test, y_label_train, y_label_test, y_type_train, y_type_test = train_test_split(\n",
        "            X,\n",
        "            y_label,\n",
        "            y_type_encoded,\n",
        "            test_size=test_set_size,\n",
        "            random_state=random_seed,\n",
        "            stratify=y_label # Stratify based on the binary attack label\n",
        "        )\n",
        "\n",
        "        print(\"Data splitting completed successfully.\")\n",
        "        print(\"\\nShapes of the resulting sets:\")\n",
        "        print(f\"  X_train shape:      {X_train.shape}\")\n",
        "        print(f\"  X_test shape:       {X_test.shape}\")\n",
        "        print(f\"  y_label_train shape: {y_label_train.shape}\")\n",
        "        print(f\"  y_label_test shape:  {y_label_test.shape}\")\n",
        "        print(f\"  y_type_train shape:  {y_type_train.shape}\")\n",
        "        print(f\"  y_type_test shape:   {y_type_test.shape}\")\n",
        "\n",
        "        # Optional: Verify stratification by checking class distribution (e.g., for y_label)\n",
        "        print(\"\\nVerifying stratification (distribution of Attack_label in train/test):\")\n",
        "        print(f\"  y_label Original distribution:\\n{y_label.value_counts(normalize=True)}\")\n",
        "        print(f\"  y_label_train distribution:\\n{y_label_train.value_counts(normalize=True)}\")\n",
        "        print(f\"  y_label_test distribution:\\n{y_label_test.value_counts(normalize=True)}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during data splitting: {e}\")\n",
        "        # Reset variables in case of error\n",
        "        X_train, X_test, y_label_train, y_label_test, y_type_train, y_type_test = [None]*6\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Data Splitting: Features (X) or Target variables (y_label, y_type_encoded) are not defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXzjz1cvw1yD",
        "outputId": "c2428b45-2dcf-4f5e-dc0b-018840f91406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- VIII: Splitting data into Training (80%) and Testing (20%) sets ---\n",
            "Data splitting completed successfully.\n",
            "\n",
            "Shapes of the resulting sets:\n",
            "  X_train shape:      (126240, 61)\n",
            "  X_test shape:       (31560, 61)\n",
            "  y_label_train shape: (126240,)\n",
            "  y_label_test shape:  (31560,)\n",
            "  y_type_train shape:  (126240,)\n",
            "  y_type_test shape:   (31560,)\n",
            "\n",
            "Verifying stratification (distribution of Attack_label in train/test):\n",
            "  y_label Original distribution:\n",
            "Attack_label\n",
            "1    0.846001\n",
            "0    0.153999\n",
            "Name: proportion, dtype: float64\n",
            "  y_label_train distribution:\n",
            "Attack_label\n",
            "1    0.846\n",
            "0    0.154\n",
            "Name: proportion, dtype: float64\n",
            "  y_label_test distribution:\n",
            "Attack_label\n",
            "1    0.846008\n",
            "0    0.153992\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step IX: Feature Scaling using StandardScaler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd # Import pandas to display scaled data as DataFrame\n",
        "\n",
        "# Ensure X_train and X_test exist from Step VIII\n",
        "\n",
        "if 'X_train' in locals() and X_train is not None and \\\n",
        "   'X_test' in locals() and X_test is not None:\n",
        "\n",
        "    print(\"--- IX: Scaling features using StandardScaler ---\")\n",
        "    print(\"Fitting scaler on X_train and transforming X_train & X_test...\")\n",
        "\n",
        "    try:\n",
        "        # Initialize the StandardScaler\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "        # Fit the scaler ONLY on the training data features\n",
        "        scaler.fit(X_train)\n",
        "\n",
        "        # Transform both the training and testing data features\n",
        "        # The output will be numpy arrays, retain column names for clarity if needed\n",
        "        X_train_scaled = scaler.transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        print(\"Feature scaling completed successfully.\")\n",
        "\n",
        "        # Convert scaled arrays back to DataFrames to show head() with column names\n",
        "        X_train_scaled_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
        "        # X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns) # Optional: create for X_test too\n",
        "\n",
        "        print(\"\\nShapes of the scaled feature sets:\")\n",
        "        print(f\"  X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "        print(f\"  X_test_scaled shape:  {X_test_scaled.shape}\")\n",
        "\n",
        "        print(\"\\nFirst 5 rows of scaled training data (X_train_scaled):\")\n",
        "        # Displaying the DataFrame version for readability\n",
        "        print(X_train_scaled_df.head())\n",
        "\n",
        "        # Optional: Verify mean and std dev of scaled training data (should be close to 0 and 1)\n",
        "        # print(\"\\nMean of scaled training data (first 5 columns):\")\n",
        "        # print(X_train_scaled_df.mean()[:5])\n",
        "        # print(\"\\nStandard Deviation of scaled training data (first 5 columns):\")\n",
        "        # print(X_train_scaled_df.std()[:5])\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during feature scaling: {e}\")\n",
        "        X_train_scaled = None\n",
        "        X_test_scaled = None\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Feature Scaling: Training data (X_train) or Testing data (X_test) not defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20HJ9hsoxDWM",
        "outputId": "503c2808-f08d-49cd-a5cc-6e34e4651bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- IX: Scaling features using StandardScaler ---\n",
            "Fitting scaler on X_train and transforming X_train & X_test...\n",
            "Feature scaling completed successfully.\n",
            "\n",
            "Shapes of the scaled feature sets:\n",
            "  X_train_scaled shape: (126240, 61)\n",
            "  X_test_scaled shape:  (31560, 61)\n",
            "\n",
            "First 5 rows of scaled training data (X_train_scaled):\n",
            "        frame.time  ip.src_host  ip.dst_host  arp.dst.proto_ipv4  arp.opcode  \\\n",
            "115205    0.002788          0.0          0.0                 0.0   -0.095086   \n",
            "120894    0.002788          0.0          0.0                 0.0   -0.095086   \n",
            "100324    0.002788          0.0          0.0                 0.0   -0.095086   \n",
            "144944    0.002788          0.0          0.0                 0.0   -0.095086   \n",
            "21612     0.002788          0.0          0.0                 0.0   -0.095086   \n",
            "\n",
            "        arp.hw.size  arp.src.proto_ipv4  icmp.checksum  icmp.seq_le  \\\n",
            "115205    -0.100729                 0.0      -0.274174    -0.284575   \n",
            "120894    -0.100729                 0.0      -0.274174    -0.284575   \n",
            "100324    -0.100729                 0.0      -0.274174    -0.284575   \n",
            "144944    -0.100729                 0.0       5.463902     3.405754   \n",
            "21612     -0.100729                 0.0      -0.274174    -0.284575   \n",
            "\n",
            "        icmp.transmit_timestamp  ...  mqtt.msg  mqtt.msgtype  mqtt.proto_len  \\\n",
            "115205                -0.023889  ...       0.0     -0.124078       -0.088452   \n",
            "120894                -0.023889  ...       0.0     -0.124078       -0.088452   \n",
            "100324                -0.023889  ...       0.0     -0.124078       -0.088452   \n",
            "144944                -0.023889  ...       0.0     -0.124078       -0.088452   \n",
            "21612                 -0.023889  ...       0.0     -0.124078       -0.088452   \n",
            "\n",
            "        mqtt.protoname  mqtt.topic  mqtt.topic_len  mqtt.ver  mbtcp.len  \\\n",
            "115205             0.0         0.0       -0.089537 -0.088452        0.0   \n",
            "120894             0.0         0.0       -0.089537 -0.088452        0.0   \n",
            "100324             0.0         0.0       -0.089537 -0.088452        0.0   \n",
            "144944             0.0         0.0       -0.089537 -0.088452        0.0   \n",
            "21612              0.0         0.0       -0.089537 -0.088452        0.0   \n",
            "\n",
            "        mbtcp.trans_id  mbtcp.unit_id  \n",
            "115205             0.0            0.0  \n",
            "120894             0.0            0.0  \n",
            "100324             0.0            0.0  \n",
            "144944             0.0            0.0  \n",
            "21612              0.0            0.0  \n",
            "\n",
            "[5 rows x 61 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1120: RuntimeWarning: overflow encountered in square\n",
            "  temp **= 2\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: overflow encountered in square\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in subtract\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py:81: RuntimeWarning: overflow encountered in square\n",
            "  upper_bound = n_samples * eps * var + (n_samples * mean * eps) ** 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnostic Step: Identify Non-Numeric Columns in X_train\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure X_train exists from Step VIII\n",
        "\n",
        "if 'X_train' in locals() and X_train is not None and isinstance(X_train, pd.DataFrame):\n",
        "    print(\"--- Diagnosing non-numeric data in X_train columns ---\")\n",
        "    non_numeric_cols = []\n",
        "    for col in X_train.columns:\n",
        "        try:\n",
        "            # Attempt to convert the column to numeric.\n",
        "            # If it fails for any value, it will raise an error.\n",
        "            pd.to_numeric(X_train[col])\n",
        "        except (ValueError, TypeError):\n",
        "            # ValueError occurs for strings that can't be floats\n",
        "            # TypeError might occur for other non-numeric types\n",
        "            non_numeric_cols.append(col)\n",
        "            print(f\"  - Found non-numeric data in column: {col}\")\n",
        "            # Optional: Show a sample of non-numeric values\n",
        "            # try:\n",
        "            #     is_numeric = pd.to_numeric(X_train[col], errors='coerce').isna()\n",
        "            #     print(f\"    Sample non-numeric values:\\n{X_train[col][is_numeric].unique()[:5]}\")\n",
        "            # except Exception:\n",
        "            #     print(\"    Could not retrieve non-numeric samples.\")\n",
        "\n",
        "\n",
        "    if not non_numeric_cols:\n",
        "        print(\"\\nDiagnosis complete: No columns found with strictly non-numeric data that prevents basic conversion.\")\n",
        "        print(\"The error might be more subtle (e.g., mixed types not caught easily, or error in scaler itself).\")\n",
        "        print(\"Let's re-check data types:\")\n",
        "        print(X_train.info())\n",
        "    else:\n",
        "        print(f\"\\nDiagnosis complete: Found {len(non_numeric_cols)} column(s) containing non-numeric data:\")\n",
        "        print(non_numeric_cols)\n",
        "        print(\"\\nPlease re-examine Step V (Data Cleaning) for these columns before attempting Step IX (Scaling) again.\")\n",
        "\n",
        "elif 'X_train' not in locals() or X_train is None:\n",
        "     print(\"Skipping Diagnosis: DataFrame 'X_train' is not defined.\")\n",
        "elif not isinstance(X_train, pd.DataFrame):\n",
        "     print(f\"Skipping Diagnosis: 'X_train' is not a Pandas DataFrame (Type: {type(X_train)}). Scaling requires DataFrame or NumPy array.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g8oeHDZwI1A",
        "outputId": "6dfbf895-5f51-4cac-cc57-14d043f74e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Diagnosing non-numeric data in X_train columns ---\n",
            "  - Found non-numeric data in column: frame.time\n",
            "  - Found non-numeric data in column: ip.src_host\n",
            "  - Found non-numeric data in column: ip.dst_host\n",
            "  - Found non-numeric data in column: arp.dst.proto_ipv4\n",
            "  - Found non-numeric data in column: arp.src.proto_ipv4\n",
            "  - Found non-numeric data in column: http.file_data\n",
            "  - Found non-numeric data in column: http.request.uri.query\n",
            "  - Found non-numeric data in column: http.request.method\n",
            "  - Found non-numeric data in column: http.referer\n",
            "  - Found non-numeric data in column: http.request.full_uri\n",
            "  - Found non-numeric data in column: http.request.version\n",
            "  - Found non-numeric data in column: tcp.options\n",
            "  - Found non-numeric data in column: tcp.payload\n",
            "  - Found non-numeric data in column: tcp.srcport\n",
            "  - Found non-numeric data in column: dns.qry.name.len\n",
            "  - Found non-numeric data in column: mqtt.conack.flags\n",
            "  - Found non-numeric data in column: mqtt.msg\n",
            "  - Found non-numeric data in column: mqtt.protoname\n",
            "  - Found non-numeric data in column: mqtt.topic\n",
            "\n",
            "Diagnosis complete: Found 19 column(s) containing non-numeric data:\n",
            "['frame.time', 'ip.src_host', 'ip.dst_host', 'arp.dst.proto_ipv4', 'arp.src.proto_ipv4', 'http.file_data', 'http.request.uri.query', 'http.request.method', 'http.referer', 'http.request.full_uri', 'http.request.version', 'tcp.options', 'tcp.payload', 'tcp.srcport', 'dns.qry.name.len', 'mqtt.conack.flags', 'mqtt.msg', 'mqtt.protoname', 'mqtt.topic']\n",
            "\n",
            "Please re-examine Step V (Data Cleaning) for these columns before attempting Step IX (Scaling) again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step X: Save Processed Data to Google Drive\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd # Needed if targets are still Pandas Series\n",
        "\n",
        "# Define the folder path in Google Drive where you want to save the files\n",
        "# You can change this path if you prefer a different location\n",
        "save_folder_drive = '/content/drive/MyDrive/Colab Notebooks/processed_data/'\n",
        "\n",
        "# Ensure all the necessary variables exist from previous steps\n",
        "required_vars = ['X_train_scaled', 'X_test_scaled', 'y_label_train', 'y_label_test', 'y_type_train', 'y_type_test']\n",
        "vars_exist = all(var in locals() and locals()[var] is not None for var in required_vars)\n",
        "\n",
        "if vars_exist:\n",
        "    print(f\"--- Saving processed data to: {save_folder_drive} ---\")\n",
        "\n",
        "    try:\n",
        "        # Create the destination directory in Google Drive if it doesn't exist\n",
        "        os.makedirs(save_folder_drive, exist_ok=True)\n",
        "        print(f\"Destination folder '{save_folder_drive}' ensured.\")\n",
        "\n",
        "        # Define file paths\n",
        "        paths = {\n",
        "            'X_train': os.path.join(save_folder_drive, 'X_train_scaled.npy'),\n",
        "            'X_test': os.path.join(save_folder_drive, 'X_test_scaled.npy'),\n",
        "            'y_label_train': os.path.join(save_folder_drive, 'y_label_train.npy'),\n",
        "            'y_label_test': os.path.join(save_folder_drive, 'y_label_test.npy'),\n",
        "            'y_type_train': os.path.join(save_folder_drive, 'y_type_train.npy'),\n",
        "            'y_type_test': os.path.join(save_folder_drive, 'y_type_test.npy')\n",
        "        }\n",
        "\n",
        "        # Data arrays map\n",
        "        data_to_save = {\n",
        "            'X_train': X_train_scaled,\n",
        "            'X_test': X_test_scaled,\n",
        "            # Convert pandas Series to numpy array before saving if needed\n",
        "            'y_label_train': y_label_train.to_numpy() if isinstance(y_label_train, pd.Series) else y_label_train,\n",
        "            'y_label_test': y_label_test.to_numpy() if isinstance(y_label_test, pd.Series) else y_label_test,\n",
        "            'y_type_train': y_type_train, # Already numpy array from LabelEncoder/split\n",
        "            'y_type_test': y_type_test   # Already numpy array from LabelEncoder/split\n",
        "        }\n",
        "\n",
        "        # Save each array\n",
        "        for name, data in data_to_save.items():\n",
        "            file_path = paths[name]\n",
        "            np.save(file_path, data)\n",
        "            print(f\"  - Saved {name} data to {file_path} (shape: {data.shape})\")\n",
        "\n",
        "        print(\"\\nAll processed data arrays saved successfully to Google Drive.\")\n",
        "\n",
        "        # Optional: Verify files exist\n",
        "        print(\"\\nVerifying saved files:\")\n",
        "        all_files_exist = True\n",
        "        for name, file_path in paths.items():\n",
        "            if os.path.exists(file_path):\n",
        "                print(f\"  - Verified: {file_path}\")\n",
        "            else:\n",
        "                print(f\"  - Verification FAILED for: {file_path}\")\n",
        "                all_files_exist = False\n",
        "        if all_files_exist:\n",
        "            print(\"All files verified successfully.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while saving data: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Save Data: One or more required data variables are not defined.\")\n",
        "    print(\"Please ensure X_train_scaled, X_test_scaled, y_label_train, y_label_test, y_type_train, y_type_test exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmbdAjRyszUB",
        "outputId": "14e32f5a-ea37-4d65-e20e-912ea3d481e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Save Data: One or more required data variables are not defined.\n",
            "Please ensure X_train_scaled, X_test_scaled, y_label_train, y_label_test, y_type_train, y_type_test exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnostic & Save Step: Verify variable existence then attempt saving\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "print(\"--- Verifying existence and type of required variables ---\")\n",
        "\n",
        "required_vars = {\n",
        "    'X_train_scaled': (np.ndarray), # Expect NumPy array after scaling\n",
        "    'X_test_scaled': (np.ndarray),  # Expect NumPy array after scaling\n",
        "    'y_label_train': (pd.Series, np.ndarray), # Could be Series or Array\n",
        "    'y_label_test': (pd.Series, np.ndarray),  # Could be Series or Array\n",
        "    'y_type_train': (np.ndarray), # Expect NumPy array after encoding/split\n",
        "    'y_type_test': (np.ndarray)   # Expect NumPy array after encoding/split\n",
        "}\n",
        "\n",
        "all_vars_found = True\n",
        "vars_data = {}\n",
        "\n",
        "for var_name, expected_type in required_vars.items():\n",
        "    if var_name in locals():\n",
        "        var_value = locals()[var_name]\n",
        "        if var_value is not None:\n",
        "            print(f\"  - Found '{var_name}' of type: {type(var_value)}\")\n",
        "            # Simple check if it's one of the expected types\n",
        "            if not isinstance(var_value, expected_type):\n",
        "                 print(f\"    - Warning: Type is not as expected ({expected_type}).\")\n",
        "            vars_data[var_name] = var_value # Store for saving\n",
        "        else:\n",
        "            print(f\"  - Found '{var_name}' but it is None.\")\n",
        "            all_vars_found = False\n",
        "    else:\n",
        "        print(f\"  - Did NOT find '{var_name}'.\")\n",
        "        all_vars_found = False\n",
        "\n",
        "if not all_vars_found:\n",
        "    print(\"\\nError: One or more required variables were not found or are None. Cannot proceed with saving.\")\n",
        "else:\n",
        "    print(\"\\nVerification successful: All required variables appear to exist.\")\n",
        "    print(\"--- Attempting to save data (Step X) ---\")\n",
        "\n",
        "    # Define the folder path in Google Drive\n",
        "    save_folder_drive = '/content/drive/MyDrive/Colab Notebooks/processed_data/'\n",
        "\n",
        "    try:\n",
        "        # Create the destination directory\n",
        "        os.makedirs(save_folder_drive, exist_ok=True)\n",
        "        print(f\"Destination folder '{save_folder_drive}' ensured.\")\n",
        "\n",
        "        # Define file paths\n",
        "        paths = {\n",
        "            'X_train': os.path.join(save_folder_drive, 'X_train_scaled.npy'),\n",
        "            'X_test': os.path.join(save_folder_drive, 'X_test_scaled.npy'),\n",
        "            'y_label_train': os.path.join(save_folder_drive, 'y_label_train.npy'),\n",
        "            'y_label_test': os.path.join(save_folder_drive, 'y_label_test.npy'),\n",
        "            'y_type_train': os.path.join(save_folder_drive, 'y_type_train.npy'),\n",
        "            'y_type_test': os.path.join(save_folder_drive, 'y_type_test.npy')\n",
        "        }\n",
        "\n",
        "        # Data arrays map (using verified vars_data)\n",
        "        data_to_save = {\n",
        "            'X_train': vars_data['X_train_scaled'],\n",
        "            'X_test': vars_data['X_test_scaled'],\n",
        "            'y_label_train': vars_data['y_label_train'].to_numpy() if isinstance(vars_data['y_label_train'], pd.Series) else vars_data['y_label_train'],\n",
        "            'y_label_test': vars_data['y_label_test'].to_numpy() if isinstance(vars_data['y_label_test'], pd.Series) else vars_data['y_label_test'],\n",
        "            'y_type_train': vars_data['y_type_train'],\n",
        "            'y_type_test': vars_data['y_type_test']\n",
        "        }\n",
        "\n",
        "        # Save each array\n",
        "        for name, data in data_to_save.items():\n",
        "            file_path = paths[name]\n",
        "            np.save(file_path, data)\n",
        "            print(f\"  - Saved {name} data to {file_path} (shape: {data.shape})\")\n",
        "\n",
        "        print(\"\\nAll processed data arrays saved successfully to Google Drive.\")\n",
        "\n",
        "        # Optional: Verify files exist\n",
        "        print(\"\\nVerifying saved files:\")\n",
        "        all_files_exist_verify = True\n",
        "        for name, file_path in paths.items():\n",
        "            if os.path.exists(file_path):\n",
        "                print(f\"  - Verified: {file_path}\")\n",
        "            else:\n",
        "                print(f\"  - Verification FAILED for: {file_path}\")\n",
        "                all_files_exist_verify = False\n",
        "        if all_files_exist_verify:\n",
        "            print(\"All files verified successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred while saving data: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxHOTPqHxBsp",
        "outputId": "5289aade-2bca-4b50-8f9b-37ba3d8223a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Verifying existence and type of required variables ---\n",
            "  - Found 'X_train_scaled' of type: <class 'numpy.ndarray'>\n",
            "  - Found 'X_test_scaled' of type: <class 'numpy.ndarray'>\n",
            "  - Found 'y_label_train' of type: <class 'pandas.core.series.Series'>\n",
            "  - Found 'y_label_test' of type: <class 'pandas.core.series.Series'>\n",
            "  - Found 'y_type_train' of type: <class 'numpy.ndarray'>\n",
            "  - Found 'y_type_test' of type: <class 'numpy.ndarray'>\n",
            "\n",
            "Verification successful: All required variables appear to exist.\n",
            "--- Attempting to save data (Step X) ---\n",
            "Destination folder '/content/drive/MyDrive/Colab Notebooks/processed_data/' ensured.\n",
            "  - Saved X_train data to /content/drive/MyDrive/Colab Notebooks/processed_data/X_train_scaled.npy (shape: (126240, 61))\n",
            "  - Saved X_test data to /content/drive/MyDrive/Colab Notebooks/processed_data/X_test_scaled.npy (shape: (31560, 61))\n",
            "  - Saved y_label_train data to /content/drive/MyDrive/Colab Notebooks/processed_data/y_label_train.npy (shape: (126240,))\n",
            "  - Saved y_label_test data to /content/drive/MyDrive/Colab Notebooks/processed_data/y_label_test.npy (shape: (31560,))\n",
            "  - Saved y_type_train data to /content/drive/MyDrive/Colab Notebooks/processed_data/y_type_train.npy (shape: (126240,))\n",
            "  - Saved y_type_test data to /content/drive/MyDrive/Colab Notebooks/processed_data/y_type_test.npy (shape: (31560,))\n",
            "\n",
            "All processed data arrays saved successfully to Google Drive.\n",
            "\n",
            "Verifying saved files:\n",
            "  - Verified: /content/drive/MyDrive/Colab Notebooks/processed_data/X_train_scaled.npy\n",
            "  - Verified: /content/drive/MyDrive/Colab Notebooks/processed_data/X_test_scaled.npy\n",
            "  - Verified: /content/drive/MyDrive/Colab Notebooks/processed_data/y_label_train.npy\n",
            "  - Verified: /content/drive/MyDrive/Colab Notebooks/processed_data/y_label_test.npy\n",
            "  - Verified: /content/drive/MyDrive/Colab Notebooks/processed_data/y_type_train.npy\n",
            "  - Verified: /content/drive/MyDrive/Colab Notebooks/processed_data/y_type_test.npy\n",
            "All files verified successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ioohIxWo35wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis of the Edge-IIoTset Dataset"
      ],
      "metadata": {
        "id": "Ri481ykrqdPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Block 1: Imports and Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set plot style\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "print(\"Libraries imported and plot style set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ4ehciCqiqg",
        "outputId": "6f0f01ef-0e43-4115-e53a-3ff6f9c05451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported and plot style set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Block 2: Data Loading (Corrected - MODIFY FILE PATH)\n",
        "import pandas as pd # Re-import pandas just in case the session was interrupted\n",
        "\n",
        "# ====> MODIFY THIS LINE to the correct path to your CSV FILE <====\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/processed_data/ML-EdgeIIoT-dataset.csv' # Example if in that Drive folder\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Display the first 5 rows\n",
        "    print(\"First 5 rows of the dataset:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Display the shape of the dataset\n",
        "    print(f\"\\nDataset dimensions (rows, columns): {df.shape}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {file_path} was not found.\")\n",
        "    print(\"Please ensure the path points directly to the CSV file and it exists.\")\n",
        "    raise\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during file loading: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "JxF9L-chqn9d",
        "outputId": "027be74f-4e50-4156-dbe7-74644a2f764a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file /content/drive/MyDrive/Colab Notebooks/processed_data/ML-EdgeIIoT-dataset.csv was not found.\n",
            "Please ensure the path points directly to the CSV file and it exists.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/processed_data/ML-EdgeIIoT-dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9ce6f1ef2c65>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Display the first 5 rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/processed_data/ML-EdgeIIoT-dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "# Updated file_path based on your output:\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    # Display the first 5 rows\n",
        "    print(\"First 5 rows of the dataset:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Display the shape of the dataset\n",
        "    print(f\"\\nDataset dimensions (rows, columns): {df.shape}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {file_path} was not found. Please check the file path.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    # exit() # You might want to comment this out in a notebook\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "LJhDLAGVujIb",
        "outputId": "48a9c41a-9ce7-4250-c7ec-15f1f21f25cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-2ce6ee85919b>:6: DtypeWarning: Columns (3,6,11,13,14,15,16,17,31,32,34,39,45,51,54,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the dataset:\n",
            "  frame.time    ip.src_host ip.dst_host arp.dst.proto_ipv4  arp.opcode  \\\n",
            "0        6.0  192.168.0.152         0.0                0.0         0.0   \n",
            "1        6.0  192.168.0.101         0.0                0.0         0.0   \n",
            "2        6.0  192.168.0.152         0.0                0.0         0.0   \n",
            "3        6.0  192.168.0.101         0.0                0.0         0.0   \n",
            "4        6.0  192.168.0.152         0.0                0.0         0.0   \n",
            "\n",
            "   arp.hw.size arp.src.proto_ipv4  icmp.checksum  icmp.seq_le  \\\n",
            "0          0.0                0.0            0.0          0.0   \n",
            "1          0.0                0.0            0.0          0.0   \n",
            "2          0.0                0.0            0.0          0.0   \n",
            "3          0.0                0.0            0.0          0.0   \n",
            "4          0.0                0.0            0.0          0.0   \n",
            "\n",
            "   icmp.transmit_timestamp  ...  mqtt.proto_len mqtt.protoname  mqtt.topic  \\\n",
            "0                      0.0  ...             0.0            0.0         0.0   \n",
            "1                      0.0  ...             0.0            0.0         0.0   \n",
            "2                      0.0  ...             0.0            0.0         0.0   \n",
            "3                      0.0  ...             0.0            0.0         0.0   \n",
            "4                      0.0  ...             0.0            0.0         0.0   \n",
            "\n",
            "  mqtt.topic_len mqtt.ver mbtcp.len mbtcp.trans_id mbtcp.unit_id  \\\n",
            "0            0.0      0.0       0.0            0.0           0.0   \n",
            "1            0.0      0.0       0.0            0.0           0.0   \n",
            "2            0.0      0.0       0.0            0.0           0.0   \n",
            "3            0.0      0.0       0.0            0.0           0.0   \n",
            "4            0.0      0.0       0.0            0.0           0.0   \n",
            "\n",
            "   Attack_label  Attack_type  \n",
            "0             1         MITM  \n",
            "1             1         MITM  \n",
            "2             1         MITM  \n",
            "3             1         MITM  \n",
            "4             1         MITM  \n",
            "\n",
            "[5 rows x 63 columns]\n",
            "\n",
            "Dataset dimensions (rows, columns): (157800, 63)\n"
          ]
        }
      ]
    }
  ]
}